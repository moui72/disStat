---
title: "Overview of PP-attachment ambiguities"
author: "Tyler J. Peckenpaugh"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: yes
    toc_depth: 2
    latex_engine: xelatex
fontsize: 12pt
mainfont: Georgia
---
# Outline of the dissertation so far

What follows in this section is a brief overview of what is so far included in the dissertation.

## Introduction

  * What a garden path is, and how (the garden-path/Construal theory of) parsing works.[^q]
  * The PP-attachment ambiguity this paper is concerned with
  * The intuitive observation that interrogative versions of these PP-attachment garden paths are less difficult to parse or recover from.
  
[^q]: I think there's a fair chance this needs expanding. What do you see as being the necessary scope? Should I go through the whole development of the Frazier line of parsing theory?

## Background

  * Prosody can effect parsing
    * Kjelgaard and Speer (1999), 
  * Prosody in silent reading
    * Fodor’s (2002) Implicit Prosody Hypothesis[^n]
  * The prosody of questions vs. declaratives
    * Hedberg, Sosa, and Görgülü (2017)
  * The details of how the parse of PP-attachment ambiguities leads to a garden path.
  * Predictions
  
[^n]: Is this necessary? I supposed it's not...

## Methodology

  * Recruitment
  * Location
  * Equipment and software
  * Procedure
  * Materials
  * Groups of participants and versions of experiment
  * IRT measurement
  * Prosodic judgements
  
## Results

  * Data treatment
  * Prosodic judgements
  * IRT
  * Delay comparison
  * Demographic data and self-reported reading habits

## Discussion

  * Are the hypothesis supported?
  * Behavioral correlate?
  * Explaining the intuition
  * Confounds
  * Areas for further study

## Conclusion
  * Behavioral correlate of the intuition still might exist in IRT, but it remains to be fully supported (more
data). Other possibilities exist (eye-tracking, ERP).
  * An explanation for the intuition still in the air, but the data seem to lean towards a non-prosodic
account. Prosody-controlled embedded question work is the best next step.
  * Other interesting findings: 
    * Interrogativity has a robust impact on IRT. 
    * Garden path condition has a robust impact on prosodic pattern.

## Appendices

  A. Experimental Items
  
  B. Filler items
  
  C. Instructions to participants
  
  D. Instructions to research assistant on providing prosodic judgements

# Specific requests for feedback

This section outlines some specific questions I have.

## Predictions

These are the hypothesis I wrote before running the study, but they are not all really answered by the data I got. Is it dishonest to omit or revise them for this paper?

*Hypothesis 1*
High attachment of PP2 is prosodically marked by a prosodic break between PP1 and and PP2.

*Hypothesis 2*
A first reading of a GP sentence will exhibit less natural prosody (more hesitation at and after the disambiguating region) than:
  
  * A first reading of a non-GP sentence.
  * A second reading of a GP sentence.

*Hypothesis 3*
A first reading of a garden-path sentence will more often be produced with prosodic structure that represents an implausible or ungrammatical parse of the string (low attachment of PP2), whereas a previewed reading sentence will more often be pronounced with the prosodic structure that represents the intended parse (high attachment of PP2).

*Hypothesis 4*
A first reading of a declarative GP sentence will exhibit less natural prosody (more hesitation at and after the disambiguating region) and be more likely to be produced with  prosodic structure that represents an implausible or ungrammatical parse of the string than a cold reading of an interrogative GP sentence.

## Reliability of prosodic judgements

```{r results_setup,echo=F,warning=F,message=F}
knitr::opts_chunk$set(
  echo=FALSE, 
  warning=FALSE, 
  message=FALSE, 
  fig.height = 3, 
  fig.pos="H"
)
options(tinytex.verbose = TRUE)
bgcolor <- "#ffffff"
par(bg = bgcolor)

kable <- function(data,...) {
  knitr::kable(data, booktabs = TRUE,...)
}

library(readr)
library(huxtable)
library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)
library(ggthemes)
library(jsonlite)
library(kableExtra)
library(psych)
library(lme4)

### global plot themeing
theme_set(
  theme_tufte() + 
    theme(
      plot.background = element_rect(fill = bgcolor,color=bgcolor),
      panel.border = element_blank()
    )
)

```
```{r prosody_setup}
mdata_raw <- read_csv("../csvs/got_prosody.csv")
### remove items where STRONG == NA (these are the items Sita didn't respond to)
mdata <- mdata_raw[!is.na(mdata_raw$STRONG),]

### code for pairs of SUBJ-ITEM
mdata$pair<-paste(mdata$SID,mdata$IID,sep="-")
n1 <- nrow(mdata)
### filter out rows missing its pair
mdata<-mdata %>% group_by(pair) %>% filter(n()>1)
n2<- nrow(mdata)
missingPairs <- n1-n2

mdata$V<-mdata$V == "YES"
mdata$OBJ<-mdata$OBJ == "YES"
mdata$PP1<-mdata$PP1 == "YES"
mdata$STRUG<-mdata$STRUG == "YES"
mdata$condition <- with(mdata,
  paste(
    ifelse(Condition_Q,"Q","D"),
    ifelse(Condition_GP,"+GP","-GP")
  )
)
mdata$STRONG <- as.factor(mdata$STRONG)
mdata$WEAK <- as.factor(mdata$WEAK)
mdata$Reading <- as.factor(mdata$Reading)
mdata$prosody <- as.factor(mdata$prosody)
mdata$two_level_prosody <- as.factor(mdata$two_level_prosody)
mdata$IID <- as.factor(mdata$IID)
mdata$UID <- as.factor(mdata$UID)
mdata$SID <- as.factor(mdata$SID)
mdata$Participant <- mdata$SID
mdata$STRUG_START <- as.factor(mdata$STRUG_START)
mdata$pdom <- mdata$two_level_prosody %in% c("PP1", "PP1 > OBJ")
mdata$odom <- mdata$two_level_prosody %in% c("OBJ", "OBJ > PP1")
mdata$simple2lvl <- ifelse(
  mdata$OBJ & mdata$PP1, 
  "BOTH", ifelse(
    mdata$OBJ,
    "OBJ",
    ifelse(
      mdata$PP1,
      "PP1",
      "NEITHER"
    )
  )
)
mdata$simple2lvl <- as.factor(mdata$simple2lvl)
mdata<-subset(mdata,SID!=8)
mdata <- droplevels(mdata)
mdata <- subset(mdata,!is.na(OBJ)|!is.na(PP1))

```
```{r irt_setup, cache=T}
itemsDf <- read_csv(
  "../csvs/src/items.csv", 
  col_types = cols(
    OBJ = col_skip(), 
    `P>NP1` = col_skip(), 
    `P>NP2+GP` = col_skip(), 
    `P>NP2-GP` = col_skip(), 
    target = col_character()
  )
)
itemsDf$Item<-itemsDf$target
itemsDf$target <- NULL
### load files
use_old <- FALSE
old<-"../csvs/irt-rvad-4_11.csv"
new<-"../csvs/irt-merged.csv"
raw_file <- read_csv(ifelse(use_old,old,new), 
  col_types = cols(
    Item = col_factor(levels = seq(1,48)), 
    Participant = col_character()
  )
)

irt_data <- raw_file

### P 8 is messed up
irt_data<-subset(irt_data,Participant != 8)

### demographics
pmeta <- read_csv("../csvs/p-meta.csv")
pmeta$Participant <- pmeta$ID
irt_data<-merge(pmeta,irt_data,by="Participant")
irt_data$wkd <- ifelse(irt_data$DOTW == "MONDAY", 1,
                ifelse(irt_data$DOTW == "TUESDAY", 2, 
                ifelse(irt_data$DOTW == "WEDNESDAY", 3, 
                ifelse(irt_data$DOTW == "THURSDAY", 4, 
                5))))



  
irt_data$cond <- paste(
  ifelse(irt_data$Condition_Q,"Q","D"),
  ifelse(
    irt_data$isFiller,
    ifelse(irt_data$Condition_GP,"+PP","-PP"),
    ifelse(irt_data$Condition_GP,"+GP","-GP")
  )
)

irt_data$Q <- ifelse(
  irt_data$Condition_Q,
  "Q",
  "D"
)
irt_data$GP <- ifelse(
  irt_data$Condition_GP,
  "+GP",
  "-GP"
)

irt_data_all <- irt_data ### data before trimming

### exclude 250 < IRTs > 25000
irt_min <- 250
irt_max <- 25000

### note how many IRTs are excluded for implausibility
irt_lo <- subset(irt_data, irt < irt_min)
irt_hi <- subset(irt_data, irt > irt_max)

### subset to trimmed items
irt_data <- subset(
  irt_data, 
  irt > irt_min & irt < irt_max
)

### with fillers
irt_data_allitems <- irt_data
irt_fillers <- subset(irt_data,isFiller)

### experimental only
irt_data <- subset(irt_data,!isFiller)
irt_data <- (merge(itemsDf, irt_data, by = 'Item'))

### make Participant and Item factors with specified ref levels 
irt_data$Participant<-relevel(as.factor(irt_data$Participant),ref=5)
irt_data$Item<-relevel(as.factor(irt_data$Item),ref=1)

### winsorize top/bottom 2.5% of data (for normal distrbution, this is +/- 2sd)
### see: https://sciencing.com/relationship-between-standard-deviations-percentiles-8768703.html
win_trim=0.025
irt_data <- irt_data %>% group_by(Participant) %>% mutate(win_irt = winsor(irt, trim=win_trim))
irt_data$wirt<-irt_data$win_irt

### log 10 transform
irt_data$wirt.log10 <- log10(irt_data$win_irt)


### meta 
json <- read_file("../meta-4_11.json")
meta<-fromJSON(json)
handset <- read.csv("../csvs/_SITA_SET_VALUES.csv")
meta[meta$file %in% handset$Filename,]$agg <- NA
meta[meta$file %in% handset$Filename,]$hpf <- NA
hscount <- sum(handset$Filename %in% meta$file)

```
```{r normalize sets}
irt_data$ID <- paste0(irt_data$Participant,"-",irt_data$Item)
mdata$ID <- paste0(mdata$SID,"-",mdata$IID)

irt_data <- subset(irt_data, ID %in% mdata$ID)
mdata <- subset(mdata,ID %in% irt_data$ID)
```
```{r describe}
### counts
irt_data$Participant <- as.factor(irt_data$Participant)
irt_data$Participant <- relevel(irt_data$Participant,ref=9)
recs<-nrow(irt_data)
subjs<-length(levels(irt_data$Participant))
items<-length(levels(irt_data$Item))
expectN <- subjs * items

### utterance lengths
if(!use_old){
  ### only available in newer files
  uls <- append(irt_data$`R1 UL`,irt_data$`R2 UL`)
  uls_desc <- describe(uls)
  uls_r1_desc <- describe(irt_data$`R1 UL`)
  uls_r2_desc <- describe(irt_data$`R2 UL`)
}

### note properties
irt_props_all <- describe(irt_data$irt)
irt_props <- describe(irt_data$irt)
wirt_props <- describe(irt_data$win_irt)

### sd by condition table
sdByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=sd
)

### means by condition table
meansByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=mean
)
names(meansByConditionLong)[3] <- "mean"
### concise means by condition table
meansByCondition <- meansByConditionLong %>% spread(Q,mean) 
names(meansByCondition)[1] <- "Condition"
### add sd and se to meansByConditionLong
meansByConditionLong$sd<-sdByConditionLong$x
meansByConditionLong$se<-sdByConditionLong$x/sqrt(irt_props$n)


### difference in condition means
diffs<-list(Condition="Increase", `D`=diff(meansByCondition$`D`), `Q`= diff(meansByCondition$`Q`))
meansByCondition<-rbind(meansByCondition, diffs)
cdiff <- meansByCondition$`D`[3]-meansByCondition$`Q`[3]

### participant means by condition
pmeansByCondition <- aggregate(
  irt_data$irt,
  by=list(
    "Condition"=irt_data$cond,
    "Participant"=irt_data$Participant
  ),
  FUN=mean
) %>% spread(Condition,x) 
pmeansByCondition$pattern <- (
  pmeansByCondition$`Q +GP` - pmeansByCondition$`Q -GP` <
  pmeansByCondition$`D +GP` - pmeansByCondition$`D -GP`
) 

### simple P means
spm <- aggregate(
  irt_data$irt,
  by=list(
    "Participant"=irt_data$Participant
  ),
  FUN=mean
) 

 
prosDescrip <- describe(mdata)
tot<-as.data.frame(xtabs( ~ Condition_GP + Condition_Q, data=mdata))
gtot <- tot

orderMap <- read_csv(
  "../csvs/src/order-p-map.csv",
  col_types = cols(
    ID = col_character(),
    Order = col_character(),
    LIST = col_character()
  )
)

orderMap$Participant <- orderMap$ID
orderMap$ID <- NULL

mdata<-merge(mdata,orderMap,by="Participant")

versiontab <- mdata %>% 
  filter(!duplicated(Participant)) %>%
  with(table("Version"=LIST,Order)) %>%
  addmargins() 

rownames(versiontab)[1:4]<-paste("Version", rownames(versiontab))
```

A second trained linguist repeated the task over 128 recordings selected from 8 participants (two from each group, one per ordering). Even number experimental items were used from 4 participants, and odd numbered from the other 4. There were 8 recordings missing from the 128 selected, so the reliability task resulted in judgements over 120 recordings. The first informat also blindly re-rated those 120, with the recording name obscured and instructions not to revisit her original ratings. Reliability scores (percent of recordings agreed upon) are reported in table \ref{tab:validity}.

```{r validitySetup}
library(irr)
library(readr)
all <- read_csv2("../drafts/export/prosody_validity.csv")
# columns to iterate
vals = c("V","OBJ","PP1","STRONG","WEAK","STRUG","STRUG_START","QI")
# raters to compare to main
suffixes=c("-dr","-sp")

# comps will hold string rep of Agr%, Kappa, (z) *pval*
comps <- matrix(nrow=6,ncol=length(vals))
colnames(comps)<-vals

# aggs will hold just agree%
aggs <- matrix(nrow=2,ncol=length(vals),dimnames = list(suffixes))
colnames(aggs)<-vals

# for each col...
for(val in vals) {
  # for each rater...
  sufn = 0
  for(suf in suffixes) {
    row = 3 * sufn + 1
    sufn = sufn + 1
    # d is 120 rows, 2 col, where col[1] = main, col[2] = suf rater
    d <- cbind(all[[paste0(val)]],all[[paste0(val,suf)]])
    a <-kappa2(d)
    ag <- agree(d)
    aggs[suf,val] <- paste0(ag$value,"%")
    
    comps[row,val] <- sprintf(
      "%0.1f%%",
      ag$value
    )
    comps[row+1,val] <- sprintf(
      "K = %0.2f%s",
      a$value,
      ifelse(a$p.value < 0.001,"***",
             ifelse(a$p.value < 0.01,"**",
                    ifelse(a$p.value < 0.05,"*",
                      ifelse(a$p.value < 0.1," .",
                             "")))))
    comps[row+2,val] <- sprintf(
      "(z = %0.2f)",
      a$statistic
    )
    
    # comps[suf,val] <- sprintf(
    #   "%0.1f%s %0.2f%s <br> (z = %0.2f)",
    #   ag$value,
    #   "% <br> K = ",
    #   a$value,
    #   ifelse(a$p.value < 0.001,"***",
    #          ifelse(a$p.value < 0.01,"**",
    #                 ifelse(a$p.value < 0.05,"*",
    #                   ifelse(a$p.value < 0.1," .",
    #                          "")))),
    #   a$statistic
    # )
  }
}

comps <- comps %>% as.data.frame()
labels <- matrix(c(rep("Interrater",3),rep("Intrarater",3)))
comps <- cbind(labels,comps)
```

```{r validity}
kable(
  comps,
  caption="Inter and intrarater agreement with Cohen's Kappa", 
  digits=2,
  align="c",
  escape=knitr::is_latex_output(),
  col.names=c(" ","V","OBJ","PP1","STRONGEST","WEAKEST","STRUGGLED","START REGION","FINAL RISE")
) %>%
  kable_styling(latex_options = c("hold_position","scale_down")) %>%
  add_header_above(c(" " = 1, "Breaks" = 3, "Break strength" = 2, "Struggle" = 2, " " = 1)) %>%
  column_spec(1, bold = T) %>%
  row_spec(c(1:2,4:5), hline_after = F) %>%
  collapse_rows(columns = 1, latex_hline = "major")  %>%
  footnote("*** p < 0.001; ** p < 0.01; * p < 0.05, . p < 0.1") 
```

The lower intrarater agreement for relative break strength was likely a result of a methodological issue: it was possible to report the same pattern, e.g. a pattern where a PP1 break is stronger than an OBJ break, by either giving the response "PP1" for strongest break, and "OBJ" for weakest break; or, "PP1" for strongest and "NONE" for weakest; or, "NONE" for strongest and "OBJ" for weakest. While the instructions to the rater requested a particular method (avoid using "NONE" in these instances across the board), it's likely that inconsistencies occurred for these cases.

The same inconsistencies would have hurt interrater agreement for strongest/weakest also. A further contributing issue for interrater agreement of those two judgements stems from the poor agreement on the presence of the verb break. When the raters don't agree about the presence of a break, that disagreement is magnified for the judgement of the relative strength of breaks.

> So, what am I to do about this spotty reliability?

Martin Chodorow on Cohen's &Kappa;:

> Kappa measures the agreement over and above chance agreement. Consider a case where we have 2 raters, A and B, and two classification categories (Yes, No). 
If each rater is saying "Yes" 90% of the time and "No" 10% of the time, then we would expect them to agree on any given judgment 82% of the time, even if
their judgments were independent. The reason for this is that if each source is generating Yes with probability of .90, then the probability that the Yes judgments
will coincide by chance is .90 x .90 = .81, and if they are generating No with probability of .10, then the probability that those No judgments will coincide is .10 x .10 = .01 
(so, .81 +.01 = .82). Let's call the probability of this kind of chance agreement Pc and the actual observed proportion of agreement Pa. The kappa formula
is K = (Pa - Pc)/(1 - Pc). If, for my hypothetical example, the observed agreement were .91, then K = (.91 - .82)/(1 - .82) = .09/.18 = .50. When one of the two 
classification categories has a high occurrence rate for both raters, it is difficult to get a very high kappa value. The table below shows the interpretations of kappa
that are commonly used by researchers.

```
Κ Interpretation
      < 0 Poor agreement
0.00-0.20 Slight agreement
0.21-0.40 Fair agreement
0.41-0.60 Moderate agreement
0.61-0.80 Substantial agreement
0.81-1.00 Almost perfect agreement
```
\clearpage

## PP1 and object breaks and their relative prominence

Table \ref{tab:pros2lvlPre} presents data that incorporates the rater's judgement of the relative prominence of the breaks. The &gt; symbol indicates that the rater found the break on the left of that symbol to be stronger, or more prominent, than the break on the left. When no symbol is shown between the two breaks, the rather found them to be of equal prominence.

```{r pros2lvlPre}

p2lv<-prop.table(xtabs(~two_level_prosody+condition,data=subset(mdata,Reading==2)),margin = 2) * 100

kable(p2lv, digits = 1, caption="Reading 1 prosodic pattern type by condition") %>% kable_styling(latex_options = c("hold_position"))

```

Another way of looking at these same data is to think of a recording as being PP1-dominant (i.e., PP1 is the only or the strongest break), OBJ-dominate, or neither. This categorization is useful because it creates binary outcomes that can be subjected to logistic regression analyses. The frequency of these patterns is reported in table \ref{tab:domtab}.

> Should "dominance" supplant or supplement the reporting of the simpler "PP1, OBJ, or both" reporting?

```{r domsetup}
pdom <- xtabs(~condition+pdom,data=mdata)%>% prop.table(margin=1)
pdom.r1 <- xtabs(~condition+pdom,data=subset(mdata,Reading==1))%>% prop.table(margin=1)
pdom.r2 <- xtabs(~condition+pdom,data=subset(mdata,Reading==2))%>% prop.table(margin=1)

odom <- xtabs(~condition + odom ,data=mdata) %>% prop.table(margin=1)
odom.r1 <- xtabs(~condition + odom,data=subset(mdata,Reading==1))%>% prop.table(margin=1)
odom.r2 <- xtabs(~condition + odom,data=subset(mdata,Reading==2))%>% prop.table(margin=1)
```

```{r domtab}
domtab<-cbind(odom.r2,pdom.r2)* 100
kable(
  domtab, 
  caption="Break dominance by condition, Reading 2 only",
  col.names=c(rep(c("Not dominant", "Dominant"), 2)),
  align="c",
  digits=1
) %>%
  add_header_above(c(" "=1, "OBJ" = 2, "PP1" = 2)) %>%
  kable_styling(latex_options = c("hold_position"))
```

```{r r2prosmod,cache=T}
r2data<-subset(mdata,Reading==2)
pp1Mod<- glmer(PP1~Condition_Q*Condition_GP+(1|IID)+(1|SID),data=r2data,family=binomial)
objMod<- glmer(OBJ~Condition_Q*Condition_GP+(1|IID)+(1|SID),data=r2data,family=binomial)
pdomMod<- glmer(pdom~Condition_Q*Condition_GP+(1|IID)+(1|SID),data=r2data,family=binomial)
odomMod<- glmer(odom~Condition_Q*Condition_GP+(1|IID)+(1|SID),data=r2data,family=binomial)
```

```{r prosmodels}
huxreg(  
  list("OBJ"=objMod,"PP1 Dominance"=pdomMod,"OBJ Dominance"=odomMod),
  coefs = c(
    "Intercept"="(Intercept)",
    "GP"="Condition_GPTRUE",
    "Interrogativity"="Condition_QTRUE",
    "GP:Q Interaction"="Condition_QTRUE:Condition_GPTRUE"
  ),
  statistics = c(
    N="nobs",
    "logLik",
    "AIC"
  )
) %>%
  set_caption("Logistic regression models of prosody, Reading 2 only") %>%
  set_label("lmPros") %>%
  set_tabular_environment("tabular")
```

> This is perhaps a question for Martin, but: what should be made of the significance of interrogativity and GP:Q interaction?

\clearpage

## Reading habits and demographics

Some demographic information reported here is obtained from data in the QC recruitment system and other data are from a short hand-written survey adminstered after the experimental procedure. The survey questions were:
  
  1. How often do you read magazines or newspapers for fun (i.e. not for school)?
  
    At least once a day	
    At least once a week	
    At least once a month	
    Less than once a month
  
  2. How often do you read fiction or non-fiction books for fun (i.e. not for school)?
  
    At least once a day	
    At least once a week	
    At least once a month	
    Less than once a month
  
  3. Do you speak any languages other than English natively and/or regularly?
  
    Yes	
    No			
    If yes, what language(s)?
  
  4. When you read for fun, do you most often read in English?
  
    Yes, I typically read in English		
    No, I typically read in another language	
    Neither is true
  
  5. How hard was it to read the sentences the way you wanted to?
  
    Not at all hard	
    Somewhat hard	
    Very hard	
    Impossible
  
  6. What do you think this study was about?
  
  7. Do you have any suggestions for how your experience might have been improved?
  
  8. Any other suggestions or comments?
  

For questions (1), (2), and (5), the responses were coded on a 1-4 scale during data entry, with the topmost answer being 4 and the bottom being 1. Question (3) was coded as 1 for "No" and 0 for any other answer. Table \ref{tab:dem} shows the responses that are appropriate for data analysis. 

```{r demset}
### demographics
pmeta <- read_csv("../csvs/p-meta.csv")

pmeta<-subset(pmeta,ID != 8)
```
```{r ddesc}
field <- colnames(pmeta)
description <- c("Participant", "Gender", "Survey question 1", "Survey question 2", "Survey question 3", "Survey question 5", "Semester the data were collected", "The day of the way data were collected", "The time of day the data were collected (24hr)", "The date the data were collected")
ddesc<-cbind(field,description)
ddesc %>% kable(caption="Survey response to data input map") %>%
  kable_styling(latex_options=c("hold_position"))
```
```{r dem}
pmeta %>% kable(caption="Survey responses") %>%
  kable_styling(latex_options = c("hold_position","scale_down"))
```

What if any of this do you think would be interesting to analyze? My thought is to see if reading habit has an impact on IRT and/or break dominance. I looked briefly at some of this, and it's interesting to note that `semester` seemed to have an effect on IRT.