```{r setup, echo=F, warning=F, message=F}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 3,
	message = FALSE,
	warning = FALSE
)                                              

library(readr)
library(jsonlite)
library(dplyr)
library(knitr)
library(kableExtra)
library(pander)

kable<-function (data,...){
  if(is_html_output() | is_latex_output()) {
    return (knitr::kable(data,...))
  } else {
    return (pander(data))
  }
}

```

# Methodology {#method}

\setlength\parindent{24pt}\setlength{\parskip}{0.0pt plus 1.0pt}

This chapter describes the methodology employed for the reported study reported in Chapter \@ref(res). The protocol outlined is referred to as the *Double Reading Procedure* and was first implemented by @fodor2019center. Under this protocol, participants are asked to read aloud a visually presented sentence twice, once without taking any time to preview sentence content (Reading 1), and then again after unlimited preview (Reading 2).

@fodor2019center aimed to investigate the extent to which preview impacted the prosodic phrasing of center-embedded sentences, as well as whether readers would find the doubly center-embedded sentences more comprehensible after preview (or, comprehensible at all, as the doubly center-embedded sentences often were not on first attempt) under the assumption that in order to pronounce a sentence with the optimal prosody, it is necessary for the speaker to understand the sentence. In the prosody literature up to this point, preview has largely been ignored as a factor in reading aloud tasks. @fodor2019center found that preview did indeed impact the prosodic grouping that readers used, suggesting that comprehension improved on the second reading.

While the questions addressed in the current study are different from those of @fodor2019center, it is still concerned with the prosody that is produced, as well as the degree of difficulty the reader experiences in parsing a sentence in order to read it aloud. This experimental paradigm eliminates the uncertainty of not knowing whether a given pronunciation represents a naive or considered attempt to read a sentence aloud.

## Materials {#mat}

In total there were 16 experimental items each constructed in 4 versions, and 32 filler items each constructed in two versions. The design decisions are discussed in detail in this section.

### Experimental items {#exps}

The basic experimental items were created in a 2 x 2 design with one factor being Speech Act (declarative/D vs. interrogative/Q) and the other being PP2 Status, i.e., PP2 was either a prepositional phrase which must be a modifier (Mod) of the preceding phrase, or else one which must be an argument of the verb (Arg). A full list of experimental items is available in Appendix \@ref(appExp).

```{r sentences}
sentences <-cbind( 
  c(
    "D Mod",
    "D Arg",
    "Q Mod",
    "Q Arg"
  ), 
  c(
    "He had decided to stick the large check in the envelope from her church.",
    "He had decided to stick the large check in the envelope into her wallet.",
    "Had he decided to stick the large check in the envelope from her church?",
    "Had he decided to stick the large check in the envelope into her wallet?"
  )
)
colnames(sentences) <- c("Version","Sentence")

kable(
  sentences, 
  booktab=T, 
  caption="Illustrative experimental item, constructed in four versions."
) %>% kable_styling(latex_options = c("HOLD_position"))

```

As previously mentioned, the current study was motivated by an observation first reported in @qp2. The experimental stimuli used in the current study were developed from the materials from that earlier study with several adjustments made to accommodate the objectives of the current study. The sequence of parts for each of the basic items was always the same, shown in (@itemdiag). Note that the material starting with the infinitival verb, e.g., *stick* until the end of sentence will be referred to as the construction throughout this and later chapters, as labeled in the example below.

\singlespacing

  (@itemdiag) 
\begin{tabular}{lllcllp{1cm}p{1cm}}
  \multicolumn{3}{c}{\small Introductory material} & & \multicolumn{4}{c}{\small Construction} \\
  \cmidrule(r){1-3} \cmidrule(r){5-8}
  Subject & Auxiliary & Matrix verb & & Infinitival verb & Object & PP1 & PP2 \\
  \multicolumn{2}{p{3.25cm}}{
    \footnotesize \raggedright Order shown  for D versions \\ (reversed in Q)
  } & & & & & \footnotesize always am\-big\-uous & \footnotesize Disam\-big\-uation of PP1 \\
\end{tabular}

\doublespacing

All four versions of any given quadruple used the same introductory material, the only difference arising through the necessary inversion of auxiliary and matrix subject, as required by the Speech Act factor. Across quadruples, subjects alternated between *she* and *he*, with half using one and half using the other; the auxiliary was always *had*. The matrix verb did not vary within a quadruple, but did vary between quadruples; for any given quadruple, the matrix verb was one of four verbs of mental state (*decide*, *intend*, *plan*, or *want*). The rationale for the use of these mental state verbs is discussed later in this section.

A given quadruple had one of four construction verbs: *cram*, *put*, *set*, or *stick*. The construction verb form was always infinitival. Each construction verb appeared in four different quadruples, and was paired once with each matrix verb, to create 16 unique pairings. Thus for matrix verb *decide*, for example, pairing with each of four construction verbs afforded *decided to cram*, *decided to put*, *decided to set*, and *decided to stick*; pairing each matrix verb with construction verb *cram* gave, *decided to cram*, *intended to cram*, *wanted to cram*, and *planned to cram*. 

The word order and content of the construction was the same across all versions of a quadruple, with the exception of the content of PP2 which varied across the PP2 Status factor: The Arg versions of a quadruple had a PP2 which was headed by *into* or *onto*, while the Mod versions had a PP2 which was headed by *of* or *from.* 

PP1 was the same across versions of a given quadruple, e.g., *stick the large check in the envelope* (see Table \@ref(tab:sentences)'s illustrative example). That is, PP1 was identical (and temporarily ambiguous) in every version of a given quadruple, being interpretable as either the goal argument of the construction verb or as a modifier of the object NP. However, in Arg versions of a quadruple, an argument interpretation of PP1 cannot be sustained when PP2 is encountered. In those cases, PP2 must fill the goal argument slot and PP1 must be a modifier. The working assumptions about parsing discussed earlier, i.e., that the parser will initially postulate PP1 to be the goal argument due to the primary status of arguments, predicts that (all and only) Arg versions of a quadruple will require reanalysis.  Between quadruples, the preposition that headed PP1 varied, but was always one which was compatible with it being a goal argument or a modifier of the object: *in* in 8 cases, *on* in 7 cases, and in one case, *under.* 

A benefit of using a complex verb cluster (auxiliary + matrix participle + infinitive; see (@itemdiag) above) rather than a single verb[^aux] was that the differences between declarative and interrogative versions of a quadruple were isolated  to the left extremity of the introductory material of the sentence: i.e., only the position of the subject and the auxiliary were affected, meaning that the construction itself and several words prior to it were completely untouched by the Speech Act manipulation. The construction is in a sense buffered from changes triggered by Speech Act, and is only affected by the PP2 Status manipulation.

[^aux]: Note that the use of an auxiliary also eliminates length differences across D vs. Q versions of a quadruple: if an auxiliary verb were not present, interrogative versions of a basic item would have an extra word, the result of so-called *do*-support, that would not appear in the declaratives (e.g., *he crammed* … vs. *did he cram* …?)

The purpose of including introductory matrix verbs (e.g., *intended*) was to reduce the oddity of the polar interrogative versions of each quadruple. It seems odd to ask, “Did Mary put the jelly beans in the window onto a fancy dish?” because, when it is clear that the speaker already knows so much about the situation, it becomes difficult to imagine a pragmatically plausible context where such a question would be asked. Such sentences might well be described as “prosecutorial”.[^dcbp] Arguably, this problem is somewhat mitigated by the addition of a verb like *intended*: rather than asking about facts that we already seem to know, we are instead asking about an actor’s mental state with regard to those facts. Even if we know the facts of the situation, we do not necessarily know, for instance, whether it was the result of a decision, some third party’s action, or mere happenstance. Another adjustment made in order to make the polar interrogative versions of each quadruple more pragmatically acceptable limited the amount of detail in the experimental sentences, so that fewer adjectives and adverbs were included compared to the items employed in @qp2, and subjects were always third person nominative pronouns (*he* or *she*).

Importantly, the construction verb was always one which demanded a goal argument. Where some of the verbs used in the items employed by @qp2 only optionally took a goal, the current study used only verbs which require a goal argument. Verbs that optionally take a goal (e.g., *hide*) might result in a parse where PP1 is not immediately incorporated as the goal argument, which would mean that PP2 would not necessarily force reanalysis. Consider the contrasting sub-categorization of the verbs *hide* and *cram* shown in (@optg) and (@oblg):

  (@optg) __Optional goal__ (*hide*) \linebreak\nopagebreak
  The gangsters had hidden the shotguns in a U-Haul truck. \linebreak\nopagebreak
  $\checkmark$ The gangsters had hidden the shotguns.
  
  (@oblg) __Obligatory goal__ (*put*) \linebreak\nopagebreak
  The gangsters had put the shotguns in a U-Haul truck. \linebreak\nopagebreak
  $*$ The gangsters had put the shotguns.

A verb like *hide*, as in (@optg), can take a goal, but can also be used without one. A verb like *put*, on the other hand, as in (@oblg), really must have a goal even if not fully detailed (e.g. *put it somewhere*). The use of verbs that require a goal argument in the current study maximized the likelihood of a robust garden path effect in the Arg versions, when PP2 triggered reanalysis. The four construction verbs used in this study were: *cram*, *put*, *set*, and *stick*. While certain constructions containing these verbs do exist where no goal is needed (e.g., *the student had needed to cram all night*; *the narrator had set the scene*; *the disgruntled worker decided to stick with the program*; and so on), all experimental items used in the current study would be rendered ungrammatical by the omission of the goal argument. The verbs of the experimental materials are used with their directed motion sense, which must always have a goal, and the uses where the goal is not necessary represent a different meaning of the verb.

Another important consideration was ensuring that the Arg versions had a PP2 which definitively disambiguated the attachment site of PP1 from the goal argument role to being a modifier of the object; i.e., that reanalysis was forced. 

  (@on) She had decided to put the child [~PP1~ on the rocking horse] [~PP2~ on the see-saw].
  (@onto) She had decided to put the child [~PP1~ on the rocking horse] [~PP2~ onto the see-saw].

In (@on), PP2 is implausible as a modifier of *rocking horse*, but not strictly impossible syntactically, and the sentence is grammatical with PP2 modifying it. On the other hand, the use of *onto* in (@onto) completely disallows the modifier interpretation of PP2 at the syntactic level: a PP headed by *onto* cannot grammatically modify the preceding NP. 

Where @qp2 relied on plausibility to force reanalysis, the current study uses syntactic disambiguation, such that the Arg versions always have a PP2 headed by *into* or *onto* which cannot head a PP2 that modifies the NP of PP1. This avoids any inconsistency in the interpretation that might result from discrepancies between individuals’ real world knowledge or beliefs. For the Mod items, the head preposition of PP2 was always either *from* or *of*, which are compatible with the parse that has already been developed, where PP1 is the goal argument and PP2 is modifying the NP within PP1. 

It is worth noting that some linguists (e.g., @den2006relators) believe *of* is not always a preposition in the same sense as *from*, *on*, or *in*, and so on, in that it appears in some cases to be serving a strictly functional role, without real lexical content, e.g., *slice of pie*. Importantly, it is also only 2 characters long, whereas *into*, *onto*, and *from* (the other possible heads of PP2) are all 4 characters. This is revisited and its possible impact is explored in the results section (Section \@ref(pp2h)).

To sum up, the experimental items were designed to have limited detail, with either *he* or *she* as the matrix subject. A complex verb cluster, e.g., *had decided to cram* was used to facilitate subject-auxiliary inversion without *do*-support in the interrogatives and thereby limit the difference between items, as well as provide a verb of mental state (*decide*, *intend*, *plan*, or *want*) to support more pragmatically plausible questions. PP1 was always interpretable as either the goal argument or a modifier of the object. PP2 differed across the PP2 Status factor, but not across the Speech Act factor. In the two Arg versions of a quadruple, PP2 was headed by *into* or *onto* and was intended to force reanalysis, under the assumption that PP1 had been incorporated into the parse as an argument, since a PP headed by *into* or *onto* must be interpreted as the goal argument, which is the position that PP1 would have presumably been occupying in the ongoing parse. For the two Mod versions of a quadruple, PP2 was headed by *from* or *of* and therefore was not expected to require reanalysis, as *from*- and *of*-headed PPs can attach as modifiers of a preceding NP (in this case, the NP within PP1), allowing PP1 to remain in the goal argument slot.

[^dcbp]: Thank you to Dr. Dianne Bradley for making this observation, and for the very clever “prosecutorial” descriptor.

### Fillers

There were 32 filler items that ranged in complexity, e.g., some contained embedded finite or non-finite clauses, some contained reduced relative clauses or full relative clauses, and some were simple matrix clauses. Of these 32, 16 were constructed to end in a sequence of two PPs, to mirror the experimental items (+PP), while the other half contained no final PPs (-PP). The +PP fillers were unrelated to the -PP fillers. All fillers were designed in two versions: declarative (D) and interrogative (Q). For the +PP fillers, the attachment sites of PP1 and PP2 varied between argument and modifier attachment. Examples of +PP and -PP filler pairs are shown in table \@ref(tab:fsentences). A full list of fillers is available in Appendix \@ref(appFill).

```{r fsentences}
sentences <-cbind( 
  c(
    "D +PP",
    "Q +PP" ,
    "D -PP",
    "Q -PP"
  ), 
  c(
    "He had forgotten to try the famous pastry in the restaurant of the fancy hotel.",
    "Had he forgotten to try the famous pastry in the restaurant of the fancy hotel?",
    "She had forgotten to report that the clerk was ignoring her request.",
    "Had she forgotten to report that the clerk was ignoring her request?"
  )
)
colnames(sentences) <- c("Version","Sentence")
t<-kable(
  sentences, 
  booktab=T, 
  caption="Illustrative filler items, constructed in two versions.",
  linesep=c("","\\addlinespace")
) 
if(is_html_output() || is_latex_output()){
  t %>%
    row_spec(2, hline_after = T) %>%
    kable_styling(latex_options = c("hold_position"))
}else{
  t
}
```

All filler items had the same sort of introductory material as the experimental items (*he/she* + *had* + mental state past participle verb). The past participle was either one of the four mental state verbs used for the experimental items (*decide*, *intend*, *plan*, *want*), or one of four additional verbs of mental state (*forgot*, *mean*, *need*, *remember*), with each of the 8 past participles being used twice in +PP fillers and twice in -PP fillers, for a total of 4 times each. This meant that a participant saw 8 instances each of *decide*, *intend*, *plan*, and *want*, (i.e., four times in experimental items, 4 times in filler items) but only 4 instances of the filler-only mental state verbs. Fillers used both mental state verbs from the experimental items as well as other similar verbs in order to prevent the experimental items being identifiable through their sentence-initial content, and to avoid extreme amounts of repetition for any given lexical item.

### Length

Sentence length was tightly controlled across items. For experimental quadruples, all sentences were between 66 and 75 characters long, and between 13 and 15 words long. The length within a quadruple never varied across the D vs. Q factor. Across the PP2 Status factor, given that the content of PP2 differed within a given quadruple, there was a maximum length difference of one character. Two quadruples varied in word length across  PP2 Status by one word. Across all quadruples, Arg and Mod sentence sub-types were equally likely to be longer (word- and character-wise).

Control over filler pair length was slightly less stringent. They ranged from 63 to 79 characters and 12 to 14 words. Length was never different within a filler pair, since only the Speech Act factor was implemented in the construction of fillers.

## Participant recruitment {#parti}

All participants in the current study were undergraduate students enrolled at CUNY's Queens College in Psychology 101[^irb] who participated for course credit. Self-reported age ranged from 18 to 25 years. Participants were recruited via a software system designed for university participant pools. Students saw a recruitment notice on the system website (see Appendix \@ref(rec)), and were able to schedule their own appointment time within a range of hours offered.

The 35 participants recruited reported themselves to be native speakers of American English, for whom English was the primary language. One participant was disqualified after producing a Caribbean English pronunciation pattern; one further participant was excluded post-hoc due to an extremely disfluent reading cadence. A final participant was excluded due to a system crash that resulted in more than half of the recordings being unavailable. All excluded participants were nonetheless awarded class credit for participating.

[^irb]: IRB approval number: 2018-0072

## Location

All data were collected in a private room with only the experimenter and participant present. While every effort was undertaken to ensure a quiet environment, intrusive noise from passersby or neighboring rooms were sometimes unavoidable. This resulted in occasional unusable or partially unusable recordings (see Section \@ref(data)).

## Equipment and software

The experiment was presented on a laptop running Windows 10; stickers were affixed to the keyboard labeling relevant keys: the left shift key was labeled *START*, right shift was labeled *NEXT*, and the touch-pad was labeled *DONE*.

The presentation of items and instruction[^instr] was implemented using the Open Sesame software [@os2012] which provides a graphical user interface, scripting language, and interpretation of Python code. The system was capable of 10-20 millisecond accuracy, with the display's 60Hz refresh rate being the limiting factor. Key input had a latency of about 10ms.

Recordings were captured via a Blue Yeti USB microphone positioned near the participant's left hand and angled to point at the space in front of the participant's mouth. The angle was adjusted for each participant's height. Audio was recorded at 44.1kHz single-channel quality.

[^instr]: Instructions were also provided verbally and in hard-copy, see Appendix \@ref(instr) for the latter.

## Versions of the experiment

The experiment was presented in 4 basic versions, with split-half ordering  (where the first 24 of the items presented to one group was the second 24 presented to the other) for a total of 8 groups. Each version contained 7 practice items, 3 of which were overt practice and 4, covert practice; these preceded the item list proper, made up of 16 experimental and 32 filler items. No version contained more than one variant of a given experimental quadruple, or a given filler pair, and each version contained one member of every experimental quadruple and filler pair. Each participant saw the same number of each sub-type of experimental quadruple: 4 D Mod, 4 D Arg, 4 Q Mod and 4 Q Arg. The experimental items were presented in a fixed pseudo-random order, interspersed with 1 to 3 fillers. Ignoring fillers, the same type of a different quadruple never occurred in sequence (e.g., after encountering a D Arg, the next experimental item was never another D Arg).

## Procedure

Participants were given a verbal overview of the experimental procedure and were then asked to read a one-page procedural summary (see Appendix \@ref(instr)) before signing a consent form. 

The instructions made clear that the task consisted only of reading each sentence twice in succession, under different timing conditions. For Reading 1, participants were asked to imagine they were reading an urgent update from a teleprompter, with emphasis placed on minimal delay; for Reading 2, the scenario shifted to providing a voice over to a documentary, and instructions made clear that the participant should understand the meaning of the sentence before beginning Reading 2. With those formalities complete, participants sat at the computer and were again walked through instructions before the first practice item was presented. Participants consulted with the experimenter between completing the overt practice items and beginning the main portion study.

Participants used keyboard button presses to navigate the experimental presentation. Each such key-press terminated the current screen, and initiated display of the screen that was programmed to follow, as shown in Figure \@ref(fig:screens). The succession of 4 screens constituting the presentation of any item was participant-paced, as was the progress from item to item. Between items, the display defaulted to a fixation screen showing a line of ten pluses aligned with the left edge of the to-be-revealed sentence. This was designed to direct the participant's attention to the beginning of the sentence, and thus minimize unintended look-ahead (the issue of potential look-ahead is discussed at greater length in Section \@ref(look-ahead) below). The sentences to be read aloud were uniformly presented without sentence-internal line breaks.

\begin{figure}[!hb]
  \centering
  \setlength{\fboxsep}{2.5ex}
  \myframebox{Sentence Display $\shortrightarrow$ \textbf{Reading 2}} \enspace DONE terminates \\[-2ex]
  \hskip 2em \myframebox{Inter-Reading Screen}\enspace START terminates \\[-2ex]
  \hskip 4em\myframebox{Sentence Display $\shortrightarrow$ \textbf{Reading 1}}\enspace NEXT terminates \\[-2ex]
  \hskip 6em \myframebox{Fixation Screen}\enspace START terminates
  \caption{Diagram of 4-screen sequence presented for each item, showing the key presses triggering movement between successive screens.}
  \label{fig:screens}
\end{figure}

The first *START* key-press that terminated the fixation screen and initiated the first display of a given item also began the first of the two recordings collected for each item. Recording continued through the presentation of the inter-item instruction slide and was terminated by the press of *START* that dismissed the inter-item instruction slide and simultaneously initiated the second display of the item.

The second display of an item was displayed in black font on a pale blue background. All other screens were displayed in black font on a pale green background (i.e., fixation screen, first display of an item, and inter-item instruction screen, as well as the initial instructions).

The inter-item instruction slide which was displayed after the first display of an item was terminated contained the following text:

\begin{quotation}
Your first reading is complete.\\
Press START to begin the second reading.
\end{quotation}

The sequence of required key presses (i.e., START-NEXT-START-DONE) and the changing background color prevented accidental double-presses of any given button from having unintended side effects, and helped the participant track where in the protocol a given screen was located. It took some time for the participants to adapt to the procedure, but generally the necessary habits were acquired before the first item of the experiment proper was presented.

### On look-ahead {#look-ahead}

An advantage of the Double Reading Procedure is that it allows for certain assumptions to be made about Reading 2 for the purposes of analysis that otherwise would be unclear to a researcher: The procedure does everything possible to ensure that Reading 2 represents a considered reading of the sentence. The participant not only has necessarily read the sentence (and heard it read) in producing Reading 1, but has had ample time to examine the sentence. This means Reading 2 can plausibly be thought to represent a considered syntactic and prosodic structure, at least more so than an entirely naive reading, and should not reflect any processing issues; a parse should have already been developed during Reading 1, or during subsequent study of the sentence prior to Reading 2.

The nature of Reading 1 is less clear. Because participants were free to set the delay between display of any sentence and the onset of phonation, it is possible that Reading 1 is not always delivered without preview. The properties of the observed Reading 1 (R1) delays are discussed at length in Section \@ref(r1del), but for now it suffices to say that only very limited preview is possible during a delay that typically falls in the 0.2 to 2.7s range. As an example of common reading rates, @ashby2012eye reported faster readers as averaging 328 words per minute (wpm), and slower readers 228 wpm, in silent reading. That study found that reading rate was slower for reading aloud, and that an experimental manipulation of the availability of parafoveal information (i.e., 1-word vs. 3-word windows) was less impactful for that reading mode. Given that the experimental items range from 13 to 15 words, most R1 delays would not allow even a very fast reader to read the entire sentence, and the median R1 delay (1s) would allow reading very few words. In fact, the time window is even shorter, because in addition to fixating the text, the participant is also performing several other cognitive activities (e.g., visual processing, lexical access, issuing motor commands, and so on).For most recordings, therefore, the utterance of Reading 1 should contain within it any behavioral reflex of whatever online parsing difficulty the reader has encountered.

In order to clearly understand the results of this double reading study, it is important to understand the mechanics of reading. Specifically, we would want to know at what point during the reading of a temporarily ambiguous sentence the participant will become aware of the existence of a disambiguating PP2, since this is when it will be realized that the initial parse may crash. The work of several decades on the time course of silent reading is thoroughly summarized in @rayner2012psychology. They describe reading as consisting of a series of fixations, during each of which foveal vision takes in a small region of the visual field, and saccades where the eyes move ahead ballistically (i.e., on a planned trajectory that cannot be interrupted). As a consequence of the ballistic property of saccadic movement and the additional finding that landing sites (fixations) are not random, it can be inferred that at least some look-ahead is available, i.e., a reader must know something about what is coming in order to plan a suitable landing site. The primary predictor of fixation point seems to be a word's length in characters, meaning that word boundary information (represented orthographically by spaces in languages like English) must be available at the periphery of attention, i.e., within the perceptual span. Some details on the perceptual span, or the information that can be accessed by the eyes at any given time, is discussed in brief here, with special attention to its relevance for the study at hand.

@rayner2012psychology discuss a number of studies that explore the size and properties of the perceptual span, the most fruitful of those studies being based on a gaze-contingent moving-window technique. In this technique, text is presented on a video monitor while the reader's eye movements are being monitered via eye-tracking equipment. A computer constantly samples the position of the reader’s eyes and updates the display accordingly. The mutilation of text outside a window of clear text creates a so-called moving window around the reader’s point of fixation. By manipulating the size of this window, it has been found that reading speed is maximized when information is available about 15 characters to either side of the fixation site  (it turns out this is actually asymmetric, and the window need only extend about 4 characters in the direction of what has already been read for optimal reading time to proceed, i.e., to the left for English readers). 

In a study by @spanmcr, in order to determine what information was available at the periphery of the perceptual span, the amount of information outside a window of clear text known to be smaller than the ideal (e.g., 21 characters, 10 to either side) was manipulated. When all other characters and spaces were replaced with *X*, essentially destroying all information outside the window, reading was slower than when character spaces were maintained but all other information was obscured. Improvements in reading speed also occurred when the original characters outside the moving window were replaced with characters that had similar shape (i.e., the same pattern of ascenders and descenders) as the character they replaced, with and without spaces. Using these techniques and manipulating the size of the window, McConkie and Rayner were able to determine that it is only word boundary information that is available at the extreme edge of the perceptual span; character shape (ascenders and descenders) is available about 10 characters out from the fixation point, and character identity is available more or less only for the fixated word.

The relevant question for the study at hand is as follows: how much of the sentence will the reader have seen and processed when a given word is being spoken? A typical item from the current study is displayed in (@dtc), with the words most likely to be fixated underlined, numbered by presumed fixation sequence, and labeled. The number of characters (including spaces) intervening before the start of the disambiguating region (the left edge of PP2) is displayed below each label. These counts are calculated from the initial character of the fixated word to the initial character of the disambiguating region; the actual fixation site is likely to be closer to the center of the word, meaning the distance would be shortened by a few (1-4) characters, depending on the length of the fixated word.

  (@dtc)
\begingroup
  \setlength{\tabcolsep}{1pt}
  \begin{tabular}{lllll|l}
    He had & 
    \underline{intended} to & 
    \underline{stick} the & 
    \underline{letter} in the & 
    \underline{mailbox} \  & onto the \underline{proper stack} \\
    & \footnotesize 1-INITIAL & \footnotesize 2-VERB & \footnotesize 3-OBJ & \footnotesize 4-PP1 & \footnotesize DISAMBIGUATION-PP2 \\
    & 45 & 32 & 22 & 7 & 0
\end{tabular}
\endgroup
  
Table \@ref(tab:dtcs) presents these distances averaged across all experimental items. Note that these values do not vary across condition, because the initial fixation is located after both the subject and auxiliary verb, and the counts end before PP2. The only changes across versions are subject-auxiliary inversion and the content of PP2 which fall outside the string of material over which these counts were calculated. 

```{r dtcs}
csv <- '
Median,46,34.5,25.5,7.5
Maximum,50,38,27,9
Minimum,45,32,21,5
'

tab <- read_csv(csv, col_names = F)

t<-tab %>% kable(
  col.names = c(" ","1-INITIAL","2-VERB","3-OBJ","4-PP1"),
  align="rccc", 
  caption="Distance in characters from fixation to disambiguation of the current study's experimental.",
  booktab = T
) 
if(is_html_output() || is_latex_output()){
  t %>% 
    kable_styling(latex_options = "hold_position")
}else{
  t
}
```

From the presumed initial fixation point, the distance to disambiguation ranges from 45 to 50 characters, with a median of 46 characters. Recall that word boundary information is available only 15 to 18 characters to the right of fixation, thus the disambiguating region is far out of view until several fixations in.

When does the reader become aware of the existence of PP2? When fixated on the direct object head noun, the range of distance is 21 to 27 characters, with a median of 25.5: PP2's content is still outside of view, even in the case of the smallest distance, and adjusting it to be a few characters smaller to account for the fact that fixation is likely to occur closer to the center of a word rather than on its first character. At most, the presence of the first few characters of PP2’s preposition may be available, but certainly not the character space after it. The distance from the presumed fourth fixation point (the head noun within PP1) to the disambiguating region, ranges from 5 to 9 characters, with a median between 7 and 8 characters. Thus, we can say with some certainty that the reader of a sentence such as (@dtc) will be aware that another phrase, one which starts with a 4-character word, remains to be incorporated into the parse sometime after processing of the direct object, and before processing of PP1.

There is yet another factor to consider, given that the readers in this study were reading aloud rather than silently: the so-called eye-voice span (EVS). According to @evs, when reading aloud the voice is typically behind the eyes by some 10-20 characters (M = 16.2 characters, SD = 5.2 characters). Adjusting Table \@ref(tab:dtcs) by subtracting 16 from each cell, we can approximate the position of the voice when the disambiguating region comes within the perceptual span. These values are shown in Table \@ref(tab:evsdtcr).


```{r evsdtcr}
csv <- '
Median,30,18.5,9.5,-8.5
Maximum,34,22,11,-7
Minimum,29,16,5,-11
'

tab <- read_csv(csv, col_names = F)

t<-tab %>% kable(
  col.names = c(" ","1-INITIAL","2-CONSTRUCTION VERB","3-OBJ","4-PP1"),
  align="rccc", 
  caption="EVS-adjusted character distance to disambiguation in experimental items.",
  booktab=T
)

if(is_html_output() || is_latex_output()){
  t %>% 
    kable_styling(latex_options = "hold_position")
}else{
  t
}
```

It is likely, then, that an oral reader's voice would actually still be on the object when the eyes' fixation begins to provide information of some kind about the existence of PP2, and will still be pronouncing PP1 when the eyes are first fixated on PP2. This raises a question about any prosodic breaks produced after the object, because it is difficult to distinguish between an intentional prosodic break at that point, and one arising from the reader using a natural position for a break to hesitate due to the garden path effect of discovering the disambiguating PP2. This property of oral reading calls into question the status of OBJ breaks to be reported in Chapter \@ref(res) with regard to whether they are linguistically motivated prosodic boundaries, or represent hesitation due to the processing difficulty experienced when PP2 is first observed by the reader.

## Measurements of utterance timing {#method-irt}

The elicitation protocol described earlier asked participants to read each sentence twice, once with no preview at all (Reading 1), and then again without any time pressure (Reading 2). Reading 1 (R1) delay is the elapsed time after a sentence is first displayed and when the participant begins speaking. Reading 2 (R2) delay is the same measure, now taken from the start of the second recording, which begins after the key press terminating the inter-item instruction slide. Inter-reading time (IRT) is a measure of the time elapsing between when a participant stops speaking after R1 and when speaking resumes for R2. IRT includes but is not synonymous with R2 delay, because IRT also includes the elapsed time after the participant stops speaking and the end of the first recording. Recall that the experiment was self-paced, and the participant might spend time studying the sentence after completing the first reading but before advancing to the next screen. For this reason, IRT is necessarily calculated across both recordings.

```{r metasetup}

## meta 
json <- read_file("../meta-4_11.json")
meta<-fromJSON(json)
handset <- read.csv("../csvs/_SITA_SET_VALUES.csv")
meta[meta$file %in% handset$Filename,]$agg <- NA
meta[meta$file %in% handset$Filename,]$hpf <- NA
meta<-cbind("P"=read.table(text=meta$file, sep=".")[[1]],meta)
hscount <- sum(handset$Filename %in% meta$file)
meta<-subset(meta,P != 8)
```

Utterance timing measurements made use of Voice Activity Detection (VAD) software, which reports whether a given interval in a sound file contains speech-like sound. It is worth making clear that while VAD was employed, most of the measurements of interest actually focused on the inverse, i.e., the spans of time in a recording that did not contain speech-like noise. For each recording, the time elapsing from the beginning of the recording to phonation onset and offset was found using VAD; then, R1 delay, R2 delay, and IRT were calculated as a function of each recording's length and the VAD-reported onset and offset of phonation.

The specific software used included a Python script developed by the author and Google's WebRTC VAD. The recordings were 44.1kHz WAV files down-sampled to 8kHz via SOX[^upsamp]. Google's VAD system uses Gaussian Mixture Models to make probabilistic decisions as to whether a given audio frame should be considered speech or noise (see Falk & Chan -@gmm1 for a complete description). Google's implementation takes one parameter called aggressiveness: a 4-tier setting for the level of confidence necessary to label a given interval speech. The implementation codes this setting on a 0-3 scale, where 0 is the most lenient (most likely to label a frame as speech) and 3 is the most stringent (most likely to label a frame as noise).

[^upsamp]: Google's VAD API only accepts WAV files with sample rates that are a multiple of 8kHz. It ultimately down-samples all files to 8kHz, regardless of the input sampling rate.

The recordings varied in the volume of a speaker's voice and the amount of background noise present. An algorithm was constructed to allow for the most stringent (highest VAD aggressiveness) measurement of the least modified signal that gave plausible measurements. Specifically, each file was measured using the highest possible aggressiveness for the VAD algorithm and no modification of the recording. If the timings detected were not plausible, the timings were re-measured with the same aggressiveness threshold, but after the recording had undergone a 200Hz high-pass filter[^lag] (HPF). If that still failed, a 400Hz HPF was used. After a further failure, the VAD aggressiveness was lowered, with each HPF value tried again (0, 200Hz, 400Hz); and that process was itself repeated until the lowest aggressiveness threshold was tried among the four possible settings. The majority of measurements were collected using the highest aggressiveness (85.4%), with more than half requiring no HPF (59.6%) and most of the remaining recordings requiring a 200Hz HPF (40.1%).

A plausible set of measurements was required to meet the following criteria:

  A. *Utterance length:* An utterance length between 2s and 10s, where utterance length is the longest contiguous span in the recording that VAD reports as phonation, with breaks in phonation of less than 1s not breaking contiguity; note here that @goldman1961-pa found that a large majority (82.5 to 87%) of pauses in fluent speech are less than 1s. If we assume a speech rate of 3 to 7 syllables per second [@jacewicz2010-sr] we would expect utterances between 2.5s and 7.3s, given that the length of stimulus sentences ranged from 18-22 syllables. Conservative thresholds higher than the expected were used, to allow for possibly slow readings of the admittedly difficult items tested in the current study.

  B. *Minimum leading silence:* A leading silence ("delay") of more than 120ms. Even a very fast simple reaction time should not permit a delay shorter than 120ms, so a shorter delay likely means an inaccurate set of measurements has been returned.

  C. *Maximum edge silence:* Maximum trailing and leading silence lengths of less than 95% of the file's length total was also used, in order to filter out recordings that did not represent a valid trial. Very long silences less than this very conservative threshold that impact the IRT are dealt with in the data clean-up rather than via phonation detection, as described in the results section of this paper (see Section \@ref(data)).

With 32 participants reading 48 items (experimental plus filler), twice each, there are an expected number of `r 32*48*2` recordings; due to system crashes at the time of data collection, `r 32*48*2 - nrow(meta)` recordings are missing. Of the `r nrow(meta)` recordings subjected to this treatment, `r nrow(meta[meta$success & !is.na(meta$agg),])` resulted in plausible timings. A review of those that did not result in plausible timings found 9 recordings that were too noisy for computer analysis, but still usable, and those timings were recorded by hand.

[^lag]: The exact algorithm is available on [github](https://gist.github.com/moui72/4ebc4eb8f69eb9fdb1cab160ce299675) (URL: [bit.ly/2uMrcrG](https://bit.ly/2uMrcrG))

To verify the accuracy of the computer measurement, timings were collected by hand for `r 120*2` recordings. There was a significant positive correlation between hand-measured and computer-measured timings (r(118)=0.87, p < .001), with a median difference of 0.4s (SD = 1.5)[^error].

[^error]: Hand measurement was done to the nearest half second, so a fair amount of error is to be expected.

## Prosodic judgments {#sita}

A linguistically trained informant, naive to the research being conducted, listened to the 978 recordings of experimental items (note that `r 1024-978` recordings were missing or omitted, as just mentioned) and reported the presence or absence of prosodic boundaries in specified regions of each sentence, and additionally gave several other judgments, e.g., the relative strengths of reported breaks, whether the reader struggled, and whether the reader used a sentence-final rising tone. Analyses of some of these judgments, e.g., the presence or absence of the OBJ and PP1 break, are reported in Chapter \@ref(res), while others proved uninformative and are not discussed further, e.g., the presence or absence of the V break. The informant familiarized herself with a speaker's speech patterns before rating any recordings by listening to 6 filler item recordings from that speaker. She was given a diagram of the sentences as in (@bpos), as well as full plain-text lists of all items.

  (@bpos)
  \begingroup
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccccccc}
      & & \footnotesize V Break & & \footnotesize OBJ Break & & \footnotesize PP1 Break & \\
      She had & wanted to set & \% & the textbooks & \% & on the top shelf & \% & into the file box. \\
      \cmidrule(r){2-2} \cmidrule(r){4-4} \cmidrule(r){6-6} \cmidrule(r){8-8} 
      & \footnotesize V Region & & \footnotesize OBJ Region & & \footnotesize PP1 Region & & PP2 Region \\
    \end{tabular}
  \endgroup


The informant was asked to report on whether she heard a prosodic boundary directly following the region labeled **OBJ**, and directly following the region labeled **PP1**. Note that throughout this dissertation, the word "break" is often used as shorthand when referring to prosodic boundaries. Importantly, it is not ised tp refer to other discontinuities such as hesitations or filler words like "um". Admittedly, however, there may have been times when the coder mislabled a hesitation as a boundary. This matter is returned to in Section \@ref(hesi). Instruction regarding how to define prosodic boundary was provided to the coder by way of the following prose:

>Please work with the assumption that “prosodic boundary” in what follows is any subset of the following features, clustered in such a way as to trigger your intuition that a new prosodic element (of any size) is beginning: pitch change, volume change, segmental lengthening, or pause.

The decision to use loose intuitive judgments of prosodic phenomena, rather than an automatic computer-based classification system or an established standard like ToBI (Tones and Break Indices, @Beckman1997-eu), is based on (a) the fact that prosodic phenomena are difficult to phonetically describe (cf. @choi2005finding), and computer performance is typically worse than that of human informants, and (b) the fact that the level of detail offered by more standard systems such as ToBI was simply not necessary for the analyses pursued here, and would have greatly increased the level of expertise needed from the informant.

Detailed instructions on the order in which items should be coded, both within speaker and across speakers, were also provided. The result was that she never listened to both readings of a sentence in sequence; she never listened to two Reading 1 versions of different sentences in sequence; and she never listened to the sentences in the same order for a different participant. The instructions given to the informant and the set of judgments requested can be found in full in Appendix \@ref(RA).

The prosodic coding procedure was designed to downplay any patterns in the data, e.g., systematic differences between Readings 1 and 2. It also mitigated any ordering effects that might occur in the data or as a result of the informant's process, whether conscious or unconcious. The familiarization process via filler items allowed the informant to judge the existence of breaks relative to the typical cadence and fluency of a given speaker, prior to exposure to any of the experimental items for that speaker.

### Reliability {#rel}

A second linguistically trained informant, also naive to the purpose of the research, repeated the task over 120 recordings selected from 8 participants (two from each group, one per ordering). Even numbered experimental items were used from 4 participants, and odd numbered from the other 4. There were 8 recordings missing from the 128 selected, so the reliability assessment was based on judgments over 120 recordings. The first informant also blindly re-rated those 120, with the recording name obscured and instructions not to revisit her original ratings. Reliability scores (percent of recordings agreed upon) and Cohen's Kappa (K) are reported in Table \@ref(tab:validity). Cohen’s Kappa adjusts the measure of agreement so that it takes into account the level of agreement that would be expected to occur by chance alone based on the judges' rates of using the category labels. 

```{r validitySetup}
library(irr)
library(readr)
all <- read_csv2("export/prosody_validity.csv")
# columns to iterate
vals = c("V","OBJ","PP1","STRONG","WEAK","STRUG","STRUG_START","QI")
# raters to compare to main
suffixes=c("-dr","-sp")

# comps will hold string rep of Agr%, Kappa, (z) *pval*
comps <- matrix(nrow=6,ncol=length(vals))
colnames(comps)<-vals

# aggs will hold just agree%
aggs <- matrix(nrow=2,ncol=length(vals),dimnames = list(suffixes))
colnames(aggs)<-vals

# for each col...
for(val in vals) {
  # for each rater...
  sufn = 0
  for(suf in suffixes) {
    row = 3 * sufn + 1
    sufn = sufn + 1
    # d is 120 rows, 2 col, where col[1] = main, col[2] = suf rater
    d <- cbind(all[[paste0(val)]],all[[paste0(val,suf)]])
    a <-kappa2(d)
    ag <- agree(d)
    aggs[suf,val] <- paste0(ag$value,"%")
    
    comps[row,val] <- sprintf(
      "%0.1f%%",
      ag$value
    )
    comps[row+1,val] <- sprintf(
      "K = %0.2f%s",
      a$value,
      ifelse(a$p.value < 0.001,"***",
             ifelse(a$p.value < 0.01,"**",
                    ifelse(a$p.value < 0.05,"*",
                      ifelse(a$p.value < 0.1," ^",
                             "")))))
    comps[row+2,val] <- sprintf(
      "(z = %0.2f)",
      a$statistic
    )
    
    # comps[suf,val] <- sprintf(
    #   "%0.1f%s %0.2f%s <br> (z = %0.2f)",
    #   ag$value,
    #   "% <br> K = ",
    #   a$value,
    #   ifelse(a$p.value < 0.001,"***",
    #          ifelse(a$p.value < 0.01,"**",
    #                 ifelse(a$p.value < 0.05,"*",
    #                   ifelse(a$p.value < 0.1," .",
    #                          "")))),
    #   a$statistic
    # )
  }
}

comps <- comps %>% as.data.frame()
labels <- matrix(c(rep("Inter-rater",3),rep("Intra-rater",3)))
comps <- cbind(labels,comps)
```

```{r validity}
kable(
  comps[,c(1,3:5)],
  caption="Percent agreement between informants (inter-rater) and first and second coding by the primary informant (intra-rater).", 
  booktab=T, 
  align="c",
  col.names=c(" ","OBJ","PP1","Break strength")
) %>%
  column_spec(1, bold = T) %>%
  row_spec(c(1:2,4:5), hline_after = F) %>%
  collapse_rows(columns = 1, latex_hline = "major")  %>%
  footnote("*** p < 0.001; ** p < 0.01; * p < 0.05, ^ p < 0.1") %>%
  kable_styling(latex_options = c("hold_position"))

```

The lower intra-rater agreement for relative break strength was likely impacted by the method of reporting: because the informant was actually asked to provide judgments over three break locations (the third, V, is omitted throughout this report because it was extremely rare, occurring in just over 8% of recordings). As such, disagreement on that break and the fact that break strength is actually a compilation of two judgments (weakest and strongest break) amplified the noise to some extent. Overall, inter-rater agreement is somewhat less than expected: @agreements offer the classification of Kappa values shown in Table \@ref(tab:agreements).

```{r agreements}
csv <- "
Kappa (K) value,Implications for reliability
0.00 < K $\\leq$ 0.40,Unreliable distinction
0.40 < K $\\leq$ 0.60,Questionably reliable distinction with only moderate agreement
0.60 < K $\\leq$ 0.80,Reliable distinction with substantial agreement
0.80 < K $\\leq$ 1.00,Highly reliable distinction
"
tab <- read_csv(csv, col_names = T)

tab %>% kable(
  escape=F,
  align="rl", 
  caption="Stratification of kappa values and implications for reliability",
  booktab=T
) %>%
  kable_styling(latex_options = c("hold_position")) 

```

It is unfortunate that inter-rater reliability is so poor, and that intra-rater reliability is only moderate. Future work would likely benefit from employing a system like RaP (Breen et al. 2007), though this would require a high level of expertise from unbiased coders.