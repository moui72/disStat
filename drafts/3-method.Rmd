```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 3,
	message = FALSE,
	warning = FALSE
)
library(huxtable)
library(readr)
library(jsonlite)
library(dplyr)
library(knitr)
library(kableExtra)
br <- function() {
  if(knitr::is_html_output()){
    return("<br/>")
  } else {
    return("/linebreak")
  }
}
```
# Methodology 
This section outlines the methodology employed for the reported study. The methodology outlined is referred to as the *Double Reading Procedure* and was first implemented by @fodor2019center. Participants are asked to read sentences twice, once without any previewed (Reading 1), and then again after unlimited preview (Reading 2).

## A note on the *Double Reading Procedure*

An advantage that the *Double Reading Procedure* has is that it allows for certain assumptions to be made about Reading 2 that otherwise would be unclear: Reading 2 represents a considered reading of the sentence. Not only has the reader had ample time to examine the sentence, they have necessarily read it, and they have heard themselves read it. We can say this with certainly, because they have necessarily already produce a recording of Reading 1. This means Reading 2 represents a considered prosodic structure, and it should not represent any processing issues, because a parse should have already been obtained during Reading 1.

The nature of Reading 1 is less clear. Because there is variability in the delay between the display of the sentence and the onset of phonation, it's possible that Reading 1 is not entirely non-previewed. The properties of these Reading 1 delays are discussed at length in \ref{sec:r1del}, but for now it suffices to say that the amount of preview that is possible during a delay that typically falls in the 0.6s to 1.6s range is very limited. As an example of common reading rates, @ashby2012eye reported faster readers as averaging 328 words per minute (wpm), and slower readers 228wpm in silent reading with 3 word windows allowing for parafoveal processing. That study found that reading time is slower for reading aloud, and that the availability of parafoveal information (i.e. the difference between 1 word and 3 word windows) is less impactful for that reading mode. For reading aloud, faster readers were slowed to an average of 213wpm, and slower readers to an average of 183wpm with 3 word windows.

We can say about Reading 1, at the very least, that a full parse of the sentence is not possible during the time before phonation begins, even delays in the 1.6s range. The utterance of Reading 1 should contain within it any behavioral reflex of whatever parsing difficult the reader has.

## Participants recruitment

The participants in this study were all undergraduate students at Queens College, CUNY, enrolled in Psychology 101. These students were required by the university to participant in studies for course credit. Self reported age ranged from 18 to 25 years. Participants were recruited using the Queens College Sona system, which is software designed for university participant pools. Students saw a notice on the Sona website (see Appendix \ref{sec:rec}) and were able to schedule their own appointment time within the hours offered (Monday - Thursday, 10am to 4pm).

All participants were self identified native and primary speakers of American English. One participant was disqualified post-hoc after producing a Caribbean English pronunciation pattern; one other participant was excluded post-hoc due to an extremely disfluent reading cadence. Both excluded participants were still awarded class credit for participating

## Location

All data were collected in a private room with only the experimenter and participant present. While every effort was undertaken to ensure a quiet environment, intrusive noise from passersby or neighboring rooms were sometimes unavoidable. This resulted in some unusable or partially unusable recordings (discussed in section \@ref(results-irt) of the results chapter).

## Equipment and software

The experiment was presented on a laptop running Windows 10 with stickers on the keyboard labeling relevant keys. The left shift key was labeled *START*, right shift was labeled *NEXT*, and the touch-pad was labeled *DONE*.

The presentation of items and instruction[^instr] was done using the Open Sesame software [@os2012] which provides a graphical user interface, scripting language, and interpretation of Python code, for precise control of experimental presentation.

Recording was done using a Blue Yeti USB microphone position near the participant's left hand and angled to point at the space in front of the participant's mouth. The angle was adjusted for each participant's height. It recording 44.1kHz single-channel audio.

[^instr]: Instructions were also provided verbally and via printout. Details on the instructions are available in the appendices \@ref(sec:instr).

## Procedure
Participants were given a verbal overview of the experimental procedure and then asked to read a one page printout of the procedure before signing a consent form. After signing a consent form, participants sat at the computer and were once again walked through instructions before the first practice item was presented.

Participants sat at a computer and used keyboard button presses to navigate the experimental presentation. They received thorough instructions and completed three practice items, the consulted with the experimenter, before beginning the main portion of the study. The study also contained 4 pseudo-practice items that were not included in any analyses, in order to allow some time for the participant to settle into the procedure before any results were recorded.

Each experimental item was preceded by a screen showing a line of ten Xes with its left edge aligned with the left edge of the to-be-revealed sentence. This was intended to fix the participant's attention on the start of the sentence, and hopefully avoid unintended look-ahead. The issue of potential look-ahead is discussed at greater length in section \ref{sec:look-ahead}

This screen remained visible indefinitely, until the participant pressed *START*.

Immediately after *START* was pressed, recording of the first reading began and the sentence appeared on the screen on a blue background. The recording continued and the screened remained visible until the participant pressed *NEXT*. After pressing *NEXT*, a screen appeared with instructions telling the participant that they were between readings, and needed to press *START* to reveal the sentence again and prepare for their second reading. Immediately after *START* was pressed, the first recording ended and the second recording began, and the sentence reappeared, this time on a green background.

![Procedure diagram](dissertation-procedure.png)

The shifting of required key presses and the changing background color were intended to aid the participant in remember where they were in the process, and to prevent accidental double-presses of any given button from having unintended side effects. It took some time for the participants to adapt to the somewhat complicated procedure, but generally were well adjusted by the time the first item was presented.

### On look-ahead {#look-ahead}

As reported by @rayner2012psychology, the perceptual span during reading is about 31 characters, or about 15 characters to either side, with an asymmetry favoring the direction in which reading occurs (i.e. wider to the right for English readers, wider to the left for Arabic readers) and a lack of importance of parafoveal information beyond the start of the fixated word in the direction of what has already been read (i.e. to the left in English). Rayner et al. show that at the extreme edges of this perceptual span, it appears to be only the presence and absence spaces that is shown to be available. Character shape (but not identity; e.g. whether a letter has ascenders or descenders) is available 10 characters to either side,

With a fixation aligned with the left edge of a stimuli, focal attention is likely centered on the third or fourth character. The resulting potential for look ahead can be diagrammed as follows, assuming initial fixation on *had*,  using a notation where a character whose identity can likely be extracted is reproduced, a character whose shape but not identity can be extracted is represented by an x, b, or j (depending on the shape of the original character, i.e. if the original character has an ascender, I've represented it with a b, if it has a descender I have represented it with a j, and if it has neither I've represented it with an x). Characters entirely outside of the perceptual span are represented with an x. Spaces are inserted only as far out as they could be identified, and otherwise filled with an x.

  (@la1) XXXXXXXXXX *(fixation slide)* `r br()`
         He had intendxb bx nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn `r br()`
         He had intended to cram the paperwork in the drawer of his filing cabinet.
         
Critically, the disambiguation PP2 region (in @la1, *of his filing cabinet*) is not within the perceptual span until, at the earliest, one is in the object region (*the paperwork*). It is important to note as well, though, that when PP1, the phrase with ambiguous attachment site, is the fixation point, the existence of PP2 at least, is within the perceptual span. If we assume fixation on *the*, the available information should be something something like (@la2).

  (@la2) nnnnnnnnnnnnnnnnnnnnnnn nnn nnjxxxxxb in the drawer xb bxnnnnnnnnnnnnnnnnn `r br()`
         He had intended to cram the paperwork in the drawer of his filing cabinet.

There is, therefore, the potential that look-ahead while reading PP1 could trigger the garden path effect of PP2 early, because it can be deduced that a short and then longer word are coming up on the periphery. The reader is likely to be aware as early as this point that something may be wrong with the parse that attaches PP1 as an argument of the verb, as that parse has limited available real-estate for further phrases. 

Another wrinkly involves the so-called eye-voice span (EVS). According to @evs, when reading aloud, the voice is typically behind the eyes by 10-20 characters, varying with various properties of the text and different parts of the reading cycle. Thus, it's difficult to know exactly where in the recording a reflex of the garden path would occur, but it is likely to be before the actually phonation of the disambiguating region. Most likely seems to be a hesitation in or around the previous phrase, i.e. PP1. This to some degree confounds the analysis of a pause after that phrase: it could be an intentional prosodic break, or it could be added processing time as a result of the reader discovering the garden path. This issue is revisited during discussion of the results.

## Materials

In total there were 16 experimental items in 4 versions each, and 32 fillers in two versions each. The design decisions are discussed in detail in this section.
<!--
  NOTE TO SELF 
  Ensure that BACKGROUND covers: 
    + "garden path" model of parsing
    + "expected parse" and "reanalysis"
    + "goal argument" -> **define argument vs. modifier**
    +  *Minimal Attachment* & *Argument Preference*
    + that arguments *must* be attached immediately (cf. construal)

-->
### Experimental items

The goal in creating the experimental items was to have four versions of each item: a declarative garden path (D+GP), a declarative non-garden path (D-GP), an interrogative garden path (Q+GP) and an interrogative non-garden path (Q-GP). The experimental stimuli are based on earlier pilot study exploring this same phenomenon [@qp2], but with several issues with that study's experimental items corrected.  The structure of the items are, schematically, something like (@schema).

  (@schema) Schematic description of experimental items
<pre>
            She       had planned to put                  the jelly beans in the window onto a fancy dish.
            [Subject] [Verb-cluster: AUX + Matrix + Main] [Object]        [PP1]         [PP2]
</pre>
  
The reason for using a complex verb cluster rather than a single verb has two parts. The auxiliary is there in order to eliminate differences across items: an interrogative version, +GP or -GP, is always identical to its declarative counterpart in every way, except for inversion of the order of the subject and auxiliary verb, and the replacement of a final period with a final question mark. If an auxiliary verb were not present, then the interrogatives would have an extra word, so-called *do*-support, that does not appear in the declarative (e.g. *she put ...* vs. *did she put ...?*). The auxiliary is always *had* and the past participle is always one of the following four: *decided*, *intended*, *wanted*, and *planned*. The purpose of the past participle matrix verbs is to partially alleviate the oddity of the yes/no questions. It seems quite odd to ask, "did she put the jelly beans in the window onto a fancy dish?" because, when it’s clear that the speaker already knows so much about the situation, it becomes difficult to imagine a pragmatically plausible context where the question would be asked. Such sentences might be described as “prosecutorial[^dcbp].” Arguably, this is somewhat mitigated by the addition of a verb like *decided*: rather than asking about facts that we already seem to know, we are instead asking about the actor's intention with regard to something that may or may not have taken place. Even if we know the facts of the situation, we do not necessarily know for instance whether it was the result of a decision, some third party's action, or merely happenstance.

[^dcbp]: Thank you to Dianne Bradley for making this observation, and for the very clever “prosecutorial” descriptor.

Importantly, the main verb needs to be one which obligatorily takes a PP goal argument. One of these issues with the items from [@qp2] was that the some of the verbs used only optionally took a goal[^goalmod]. Verbs that only optionally take a goal are likely to result in a parse where PP1 is not immediately incorporated as the goal argument, but rather be incorporated as a modifier of the object, which would mean that PP2 would not force reanalysis. Consider the contrasting sub-categorization of the verbs in (@weak-@strong):

[^goalmod]: Because "optional argument" is arguably a contradictory term, I refrain from referring to goals that are not obligatory as "arguments"; I will instead call them goal modifiers of the verb, or just goals.

  (@weak) Weak PP-argument sub-categorization (*hide*) 
  
  a. The gangsters had hidden the shotguns in a U-Haul truck in an old outhouse. 
  b. \checkmark The gangsters had hidden the shotguns.
            
  (@strong) Strong PP-argument sub-categorization (*cram*) 
  
  a. Justin had crammed the old newspapers under the couch in a small wastebasket.
  b. \* Justin had crammed the old newspapers.

A verb like *hide*, as in (@weak), can take a goal, but is also able to do without one. A verb like *cram*, on the other hand, as in (@strong), really must have a goal. In designing the items for this study, only verbs that require a goal argument were used in order to maximize the likelihood of a robust garden path effect in the +GP versions, when PP2 triggers reanalysis. The four verbs used in this study were: *cram*, *put*, *stick* and *set*. 



```{r sentences}
sentences <-cbind( 
  c(
    "D-GP",
    "Q-GP" ,
    "D+GP",
    "Q+GP"
  ), 
  c(
    "He had intended to cram the paperwork in the drawer of his filing cabinet.",
    "Had he intended to cram the paperwork in the drawer of his filing cabinet?",
    "He had intended to cram the paperwork in the drawer into his boss's desk.",
    "Had he intended to cram the paperwork in the drawer into his boss's desk?"
  )
)
colnames(sentences) <- c("Version","Sentence")
kable(sentences, booktabs=T, caption="Experimental item in four versions")
```

Another important consideration was ensuring that the +GP versions had a PP2 which fully disambiguated the attachment site of PP1 such that reanalysis is forced. In (@ambig), PP2 is implausible as a modifier of *rocking horse*, but strictly impossible, and the sentence is certainly grammatical with low attachment of PP2. On the other hand, the use of *onto* in (@disam) completely disallows the low attachment of PP2 at the syntactic level: a PP headed by *onto* simply cannot modify the preceding NP.

  (@ambig) She had decided to put the child [~PP1~ on the rocking horse] [~PP2~ **on** the see-saw].
  (@disam) She had decided to put the child [~PP1~ on the rocking horse] [~PP2~ **onto** the see-saw].
  
Where [@qp2] relied on plausibility to force reanalysis, this study uses syntactic disambiguation, such that the +GP versions always have a PP2 headed by *into* or *onto* This avoids any discrepency between individuals' belief of what is plausible from impacting the parse obtained.

### Fillers

There were 32 filler items that ranged in complexity. Of these 32, 16 were designed to end in a sequence of two PPs, to mirror the experimental items (+PP). The other half contained no final PPs (-PP). All fillers were designed in two versions: declarative (D) and interrogative (Q). The +PP fillers were not related to the -PP fillers.

  (@fillerdpp) He had forgotten to try the famous pastry in the restaurant of the fancy hotel.
  (@fillerqpp) Had he forgotten to try the famous pastry in the restaurant of the fancy hotel?
  
  (@fillerd) She had forgotten to report that the clerk was ignoring her request.
  (@fillerq) Had she forgotten to report that the clerk was ignoring her request?

A full list of fillers is available in appendix \ref{sec:fillers}. Fillers were not limited to the four main verbs of the experimental item, but did always follow the same pattern of *had* followed by a past participle. The past participle was always one of the four used for experimental item (*intended*, *decided*, *planned*, and *wanted*), or one of these additional four: *forgotten*, *remembered*, *meant*, or *needed*, with each of the 8 past participles being used twice in the +PP fillers and twice in the -PP fillers, for a total of 4 times each.

### Length

Length was tightly controlled across items. For experimental items, all sentences were between 66 and 75 characters long, and between 13 and 15 words long. The length never varied across the &plusmn;Q condition. Across the &plusmn;GP condition there was a maximum length difference of one character. Two items varied in word length across &plusmn; by one word. For all length variations (word and character), an equal number were longer in the +GP condition as in the -GP condition. They ranged from 18 to 22 syllables.

Control over filler length was slightly less stringent. They ranged from 63 to 79 characters and 12 to 14 words. 

## Experimental groups and order of items

No participant saw more than one version of each sentence, and each participant saw one version of every sentence. Each participant saw the same number of each type of sentence: 4 garden path declaratives, 4 garden path interrogatives, 4 non-garden path declaratives and 4 non-garden path interrogatives. The experimental item were presented in pseudo-random order, interspersed with 1 to 3 fillers and with two experimental items never occurring in the same version twice in a row (e.g. after declarative garden path, the next experimental item was never also a declarative garden path).

The four versions of the experimental sentences resulted in four groups. A split-half ordering (where the first 24 of the items presented to one group was the second 24 presented to the other) resulted in 2 versions for each group, and so there were 8 groups in total.

## Inter-reading time (IRT) measurement

Subjects were asked to read each sentence twice, once with no preview at all (reading 1, a cold reading), and then again after unlimited preview (reading 2, a previewed reading). Inter-reading time (IRT) is a measure of the amount of time between when a subject stops speaking after a cold reading and when they begin speaking for a previewed reading.

```{r metasetup}

## meta 
json <- read_file("../meta-4_11.json")
meta<-fromJSON(json)
handset <- read.csv("../csvs/_SITA_SET_VALUES.csv")
meta[meta$file %in% handset$Filename,]$agg <- NA
meta[meta$file %in% handset$Filename,]$hpf <- NA
meta<-cbind("P"=read.table(text=meta$file, sep=".")[[1]],meta)
hscount <- sum(handset$Filename %in% meta$file)
meta<-subset(meta,P != 8)
```

IRT was measured using a Python script and Google's WebRTC Voice Activity Detection (VAD) over 44.1kHz WAV files down-sampled to 8kHz via SOX[^upsamp]. This VAD system uses Gaussian Mixture Models to make probabilistic decisions on whether a given audio frame is speech or noise (see @gmm1 for a complete explanation). Google's implementation takes one parameter, which they call aggressiveness: a 4-tier setting for the level of confidence necessary to call a given frame speech. I call this "rejection rate", where a higher rejection rate means that the model requires a high level of confidence before assuming a frame is speech, i.e. it is more likely to label something noise than speech. The implementation codes this setting as 0-3, where 0 is the most lenient (most likely to label a frame as speech) and 3 is the most stringent (most likely to label a frame as noise).

[^upsamp]: Google's VAD API only accepts WAV files with sample rates that are a multiple of 8kHz. It ultimately down-samples all files to 8kHz, regardless of the input rate.

The recordings vary in the volume of the speaker's voice and the amount of background noise present. An algorithm was constructed to allow for the most stringent measurement of the least modified data that gave plausible measurements. Specifically, each file was measured using the highest possible rejection rate for the VAD algorithm and no modification of the file. If the timings detected were not plausible, the timings were re-measured with the same rejection rate, but after the recording had undergone a 200Hz high-pass filter[^alg] (HPF). If that still failed, a 400Hz HPF was used. After a further failure, the rejection rate for the VAD was lowered, and the whole thing was tried again (0, 200Hz, 400Hz); and that process was itself repeated until the lowest possible rejection rate was tried of the four possible settings. 

A plausible set of measurements had to meet the following criteria:

*Utterance length:* An utterance length between 2s and 10s, where utterance timing is the longest contiguous span in the recording that VAD reports as phonation, with breaks in phonation of less than 1s not breaking contiguity (@goldman1961-pa found that a large majority (82.5 to 87%) of pauses in fluent speech are less than 1s). Stimuli range from 18-22 syllables in length. If we assume a speech rate of 3 to 7 syllables per second [@jacewicz2010-sr] we would expect utterances between 2.5s and 7.3s. Conservative thresholds higher and lower than the expected were used, especially on the higher end, to allow for any difficulties processing or fluency that might have lead to longer reading times.

*Minimum leading silence:* A leading silence ("delay") of more than 120ms. Even a very fast human reaction time should not permit a delay shorter than 120ms, so a shorter delay likely means something went wrong with the procedure and the participant is already speaking before the sentence is displayed.

*Maximum edge silence:* A maximum trailing and leading silence length of less than 95% of the file's length was also used, in order to filter out recordings that do not represent a valid trial. Very long silences less than this very conservative threshold that impact the IRT are dealt with in the data clean up rather than via phonation detection, as described in the results section of this paper (section \ref{sec:irtDis}).

With 32 participants reading 48 items (experimental and filler) twice each, there are an expected number of `r 32*48*2` recordings; due to technical issues at the time of data collection, `r 32*48*2 - nrow(meta)` recordings are missing. Of the `r nrow(meta)` recordings subjected to this treatment, `r nrow(meta[meta$success & !is.na(meta$agg),])` resulted in plausible timings. For those that were successful, the breakdown of HPF and rejection rate used is reported in Table \@ref(tab:metable). A further `r hscount` were set by hand, resulting in a total of `r nrow(meta[meta$success,])` recordings.

[^alg]: The exact algorithm is available on [github](https://gist.github.com/moui72/4ebc4eb8f69eb9fdb1cab160ce299675) (URL: [bit.ly/2uMrcrG](https://bit.ly/2uMrcrG))

```{r metable}
metaTable<-xtabs(~agg+hpf, meta[meta$success,])
row.names(metaTable)<-c("Lowest VAD rejection rate","...","...","Highest VAD rejection rate")
kable(
  metaTable, 
  col.names = c("No HPF", "HPF at 200Hz", "HPF at 400Hz"),
  caption="VAD rejection rate and high pass filter (HPF) values",
  booktabs=T
) %>% kable_styling(latex_options = "hold_position")
```

## Timing measurement reliability
```
NOTE TO DI:

Hand measurement is not yet completed.
```
Timings were collected by hand for `r 384*2` recordings (`r round(384*2 / nrow(meta) * 100,1)`%). Human measured timings were within 100ms of the VAD-measured timings in `??`% of cases.


## Prosodic judgments

A trained linguist informant naive to the research being conducted listened to recordings and reported the presence or absence of breaks in certain regions of the sentence, as well as several other judgments. She was instructed to familiarize herself with a speaker's speech patterns before rating any recordings by listening to 6 filler item recordings from that speaker. She was given a diagram of the sentences as in table \ref{tab:reg}, as well as full plain-text lists of all items.

```{r reg}
Sentence = c(
  "He", 
  "had", 
  "meant", 
  "to stick ||$_{V}$", 
  "the pencil case ||$_{OBJ}$", 
  "in the cabinet ||$_{PP1}$", 
  "into his book bag."
)
Syntax = c(
  "NP$_{SUBJ}$",
  "AUX", "V$_1$",
  "V$_2$", 
  "NP$_{OBJ}$",
  "PP$_1$", 
  "PP$_2$"
)

region = c("SUBJ", " ", "V", " ", "OBJ", "PP1", "PP2")
tab = rbind(Sentence,Syntax)
colnames(tab) <- region
kable(tab, booktab=T, caption="Sentence region labels", escape=F,row.names = F) %>% 
  kable_styling(latex_options = c("scale_down","hold_position"))

```

She was asked to report on whether or not she heard a prosodic boundary directly after the region labeled **V**, directly after the region labeled **OBJ**, and directly after the region labeled **PP1**. The following definition of prosodic break was provided:

>Please work with the assumption that “prosodic boundary” in what follows is any subset of the following features, clustered in such a way as to trigger your intuition that a new prosodic element (of any size) is beginning: pitch change, volume change, segmental lengthening, or pause.

The judgments requested also included whether or not the speaker struggled, where that struggle began, whether or not the speaker used question intonation, and which break(s) were stronger or more prominent than which other break(s). 

Detailed instructions on the order in which items should be listened to, both within speaker and across speakers, were also provided. The result was that she never listened to both readings of a sentence in sequence; she never listened to two reading 1 versions of different sentences in sequence; and she never listened to the sentences in the same order for a given participant as she did for the previous one.

Details on the instructions given and the judgments collected can be found in the appendices (\ref{sec:RA}).

### Reliability

A second trained linguist repeated the task over 128 recordings selected from 8 participants (two from each group, one per ordering). Even number experimental items were used from 4 participants, and odd numbered from the other 4. There were 8 recordings missing from the 128 selected, so the reliability task resulted in judgments over 120 recordings. The first informant also blindly re-rated those 120, with the recording name obscured and instructions not to revisit her original ratings. Reliability scores (percent of recordings agreed upon) are reported in table \ref{tab:validity}.

```{r validitySetup}
library(irr)
library(readr)
all <- read_csv2("export/prosody_validity.csv")
# columns to iterate
vals = c("V","OBJ","PP1","STRONG","WEAK","STRUG","STRUG_START","QI")
# raters to compare to main
suffixes=c("-dr","-sp")

# comps will hold string rep of Agr%, Kappa, (z) *pval*
comps <- matrix(nrow=6,ncol=length(vals))
colnames(comps)<-vals

# aggs will hold just agree%
aggs <- matrix(nrow=2,ncol=length(vals),dimnames = list(suffixes))
colnames(aggs)<-vals

# for each col...
for(val in vals) {
  # for each rater...
  sufn = 0
  for(suf in suffixes) {
    row = 3 * sufn + 1
    sufn = sufn + 1
    # d is 120 rows, 2 col, where col[1] = main, col[2] = suf rater
    d <- cbind(all[[paste0(val)]],all[[paste0(val,suf)]])
    a <-kappa2(d)
    ag <- agree(d)
    aggs[suf,val] <- paste0(ag$value,"%")
    
    comps[row,val] <- sprintf(
      "%0.1f%%",
      ag$value
    )
    comps[row+1,val] <- sprintf(
      "K = %0.2f%s",
      a$value,
      ifelse(a$p.value < 0.001,"***",
             ifelse(a$p.value < 0.01,"**",
                    ifelse(a$p.value < 0.05,"*",
                      ifelse(a$p.value < 0.1," .",
                             "")))))
    comps[row+2,val] <- sprintf(
      "(z = %0.2f)",
      a$statistic
    )
    
    # comps[suf,val] <- sprintf(
    #   "%0.1f%s %0.2f%s <br> (z = %0.2f)",
    #   ag$value,
    #   "% <br> K = ",
    #   a$value,
    #   ifelse(a$p.value < 0.001,"***",
    #          ifelse(a$p.value < 0.01,"**",
    #                 ifelse(a$p.value < 0.05,"*",
    #                   ifelse(a$p.value < 0.1," .",
    #                          "")))),
    #   a$statistic
    # )
  }
}

comps <- comps %>% as.data.frame()
labels <- matrix(c(rep("Inter-rater",3),rep("Intra-rater",3)))
comps <- cbind(labels,comps)
```

```{r validity}
kable(
  comps,
  caption="Inter and intra-rater agreement with Cohen's Kappa", 
  booktab=T, 
  digits=2,
  align="c",
  escape=knitr::is_latex_output(),
  col.names=c(" ","V","OBJ","PP1","STRONGEST","WEAKEST","STRUGGLED","START REGION","FINAL RISE")
) %>%
  kable_styling(latex_options = c("hold_position","scale_down")) %>%
  add_header_above(c(" " = 1, "Breaks" = 3, "Break strength" = 2, "Struggle" = 2, " " = 1)) %>%
  column_spec(1, bold = T) %>%
  row_spec(c(1:2,4:5), hline_after = F) %>%
  collapse_rows(columns = 1, latex_hline = "major")  %>%
  footnote("*** p < 0.001; ** p < 0.01; * p < 0.05, . p < 0.1") 
```

The lower intra-rater agreement for relative break strength was likely a result of a methodological issue: it was possible to report the same pattern, e.g. a pattern where a PP1 break is stronger than an OBJ break, by either giving the response "PP1" for strongest break, and "OBJ" for weakest break; or, "PP1" for strongest and "NONE" for weakest; or, "NONE" for strongest and "OBJ" for weakest. While the instructions to the rater requested full verbosity, it's likely that inconsistencies occurred for these cases.

The same inconsistencies would have hurt inter-rater agreement for strongest/weakest also. A further contributing issue for inter-rater agreement of those two judgments stems from the poor agreement on the presence of the verb break. When the raters do not agree about the presence of a break, that disagreement is magnified for the judgement of the relative strength of breaks.

