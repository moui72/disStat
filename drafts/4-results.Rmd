---
title: "Results and analyses"
author: "Tyler Peckenpaugh"
date: "April 7, 2019"
bibliography: refs.bib
tables: true
output: 
  bookdown::tufte_html2: 
    toc: true
  bookdown::tufte_handout:
    toc: true
---
<!-- Results -->
<!-- This is the fourth content sectionî¿¿ -->
<!-- Previous section: methodology -->
<!-- Following section: discussion -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.height = 3)
bgcolor <- if (knitr::is_html_output()) "#fffff8" else NULL
par(bg = bgcolor)

library(readr)
library(huxtable)
library(psych)
library(dplyr)
library(tidyr)
library(knitr)
library(lme4)
library(kableExtra)
library(ggplot2)
library(ggthemes)
library(jsonlite)
```

This section reports various analyses of the recordings obtained.

# Prosody {#results-prod}

```{r prosody-setup}
mdata <- read_csv("../csvs/got_prosody.csv")
mdata$V<-mdata$V == "YES"
mdata$OBJ<-mdata$OBJ == "YES"
mdata$PP1<-mdata$PP1 == "YES"
mdata$STRUG<-mdata$STRUG == "YES"
mdata$condition <- with(mdata,
  paste(
    ifelse(Condition_Q,"+Q","-Q"),
    ifelse(Condition_GP,"+GP","-GP")
  )
)
mdata$STRONG <- as.factor(mdata$STRONG)
mdata$WEAK <- as.factor(mdata$WEAK)
mdata$Reading <- as.factor(mdata$Reading)
mdata$prosody <- as.factor(mdata$prosody)
mdata$two_level_prosody <- as.factor(mdata$two_level_prosody)
mdata$IID <- as.factor(mdata$IID)
mdata$UID <- as.factor(mdata$UID)
mdata$SID <- as.factor(mdata$SID)
mdata$STRUG_START <- as.factor(mdata$STRUG_START)
mdata$pdom <- mdata$two_level_prosody %in% c("PP1", "PP1 > OBJ")
mdata$odom <- mdata$two_level_prosody %in% c("OBJ", "OBJ > PP1")
mdata<-subset(mdata,SID!=8)
```

## Breaks after PP1 and direct object

Across all experimental items, there was a break after the direct object in `r round(nrow(mdata[mdata$OBJ,])/nrow(subset(mdata,!is.na(OBJ)))*100,1)`% of recordings and a break after PP1 in `r round(nrow(mdata[mdata$PP1,])/nrow(subset(mdata,!is.na(PP1)))*100,1)`%.

```{r obj}
objtab<-subset(mdata, !is.na(OBJ)) %>%
  group_by(OBJ,Condition_GP) %>%
  summarise(n=n())

objyGP <- objtab[objtab$OBJ & objtab$Condition_GP,]$n/sum(objtab[objtab$Condition_GP,]$n) * 100
objnGP <- objtab[objtab$OBJ & !objtab$Condition_GP,]$n/sum(objtab[!objtab$Condition_GP,]$n) * 100
```
```{r pp1}
pp1tab<-subset(mdata, !is.na(PP1)) %>%
  group_by(PP1,Condition_GP) %>%
  summarise(n=n())

pp1yGP <- pp1tab[pp1tab$PP1 & pp1tab$Condition_GP,]$n/sum(pp1tab[pp1tab$Condition_GP,]$n) * 100
pp1nGP <- pp1tab[pp1tab$PP1 & !pp1tab$Condition_GP,]$n/sum(pp1tab[!pp1tab$Condition_GP,]$n) * 100
```

For garden paths, there is a PP1 break in `r round(pp1yGP,1)`% of items, while non-garden paths had a PP1 break in `r round(pp1nGP,1)`% of cases.

```{r cond_obj}
objtabTotal<-subset(mdata, !is.na(OBJ)) %>%
  group_by(Condition_Q,Condition_GP) %>%
  summarise(n=n()) %>% 
  mutate(Q=ifelse(Condition_Q,"Interrogative","Declarative"))%>%
  mutate(GP=ifelse(Condition_GP,"Garden path","Non-garden path"))%>%
  subset(select=c("Q","GP","n")) %>%
  spread(GP,n)

objtab<-subset(mdata, !is.na(OBJ)) %>%
  group_by(OBJ,Condition_Q,Condition_GP) %>%
  summarise(n=n()) %>% 
  subset(OBJ) %>%
  mutate(Q=ifelse(Condition_Q,"Interrogative","Declarative"))%>%
  mutate(GP=ifelse(Condition_GP,"Garden path","Non-garden path"))%>%
  subset(select=c("Q","GP","n")) %>%
  spread(GP,n)

colnames(objtab)[1]<-""
objtab[,c(2:3)] <- objtab[,c(2:3)]/objtabTotal[,c(2:3)]*100
kable(objtab,digits=1,caption="Percentage of recordings with an object break, by condition")
```

## &plusmn;Struggle

This section presents the counts of recordings where the reader struggled to maintain fluency. Table \@ref(tab:strug) shows the overall pattern across reading.

```{r strug}
all_strug <-table(mdata$STRUG,mdata$Reading)
colnames(all_strug)<-c("Cold reading", "Previewed reading")
row.names(all_strug)<-c("Fluent", "Struggle")
kable(all_strug, caption="Difficulty in reading by reading type")
```

Table \@ref(tab:gpstrug) shows the pattern across reading for just +GP items.

```{r gpstrug}
GP_strug <- with(subset(mdata,Condition_GP==TRUE),table(STRUG,Reading))
colnames(GP_strug)<-c("Cold reading", "Previewed reading")
row.names(GP_strug)<-c("Fluent", "Struggle")
kable(GP_strug, caption="Difficulty in reading by reading type for garden paths")
```

Table \@ref(tab:strugModel) shows a number of logistic regression models. Models with complex random slope structures failed to converge, and models with random slopes for just *Previewed Reading* were worse than those without random slopes, so the models presented have no random slopes for any predictors. None of the models differ in statistically significant ways, except that the model with no random effects is significantly worse than the others (e.g. no random effects (AIC=575) compared to full model (AIC=547), &Chi;^2(3)^=3, p < 0.001).

```{r strugModel, cache=T}
models.strug <- glmer(STRUG~Condition_Q*Condition_GP+Reading+(1|IID)+(1|SID),data=mdata,family=binomial)
models.strug.noRand <- glm(STRUG~Condition_Q+Condition_GP+Reading,data=mdata,family=binomial)
models.strug.noInt <- glmer(STRUG~Condition_Q+Condition_GP+Reading+(1|IID)+(1|SID),data=mdata,family=binomial)
models.strug.noCon <- glmer(STRUG~Reading+(1|IID)+(1|SID),data=mdata,family=binomial)
huxreg(
  list(
    "Full"=models.strug, 
    "No random effects"=models.strug.noRand,
    "No interaction"=models.strug.noInt, 
    "Reading as only pred"=models.strug.noCon
  ),
  coefs = c(
    "Intercept"="(Intercept)",
    "+GP"="Condition_GPTRUE",
    "+Q"="Condition_QTRUE",
    "+GP +Q"="Condition_QTRUE:Condition_GPTRUE",
    "Previewed reading" = "Reading2",
    "Participant"="sd_(Intercept).SID",
    "Item"="sd_(Intercept).IID"
  ),
  statistics = c(
    N="nobs",
    "logLik",
    "AIC"
  ),
  align="."
) %>% 
  set_caption("Logistic regression models of disfluency") %>% 
  set_label("strugMod")%>%
  set_position("left")

compNoRand <- anova (models.strug,models.strug.noRand)
```

Cold vs. previewed reading is a statistically significant predictor in every case.

# Inter-reading time (IRT) {#results-irt}

```{r irt-setup}
items <- read_csv(
  "../csvs/src/items.csv", 
  col_types = cols(
    OBJ = col_skip(), 
    `P>NP1` = col_skip(), 
    `P>NP2+GP` = col_skip(), 
    `P>NP2-GP` = col_skip(), 
    target = col_character()
  )
)

items$Item<-items$target

# global plot themeing
theme_set(
  theme_tufte() + 
    theme(
      plot.background = element_rect(fill = bgcolor,color=bgcolor),
      panel.border = element_blank()
    )
)

# load files
use_old <- FALSE
old<-"../csvs/irt-rvad-4_11.csv"
new<-"../csvs/irt-merged.csv"
raw_file <- read_csv(ifelse(use_old,old,new), 
  col_types = cols(
    Item = col_factor(levels = seq(1,48)), 
    Participant = col_character()
  )
)

irt_data <- raw_file
irt_data <- (merge(items, irt_data, by = 'Item'))
# P 8 is messed up
irt_data<-subset(irt_data,Participant != 8)

# counts
irt_data$Participant <- as.factor(irt_data$Participant)
irt_data$Participant <- relevel(irt_data$Participant,ref=9)
recs<-nrow(irt_data)
subjs<-length(levels(irt_data$Participant))
items<-length(levels(irt_data$Item))
expectN <- subjs * items
  
irt_data$cond <- paste(
  ifelse(irt_data$Condition_Q,"+Q","-Q"),
  ifelse(
    irt_data$isFiller,
    ifelse(irt_data$Condition_GP,"+PP","-PP"),
    ifelse(irt_data$Condition_GP,"+GP","-GP")
  )
)

irt_data$Q <- ifelse(
  irt_data$Condition_Q,
  "+Q",
  "-Q"
)
irt_data$GP <- ifelse(
  irt_data$Condition_GP,
  "+GP",
  "-GP"
)

# utterance lengths
if(!use_old){
  # only available in newer files
  uls <- append(irt_data$`R1 UL`,irt_data$`R2 UL`)
  uls_desc <- describe(uls)
  uls_r1_desc <- describe(irt_data$`R1 UL`)
  uls_r2_desc <- describe(irt_data$`R2 UL`)
}

irt_props_all <- describe(irt_data$irt)
irt_data_all <- irt_data # data before trimming

# exclude 250 < IRTs > 25000
irt_min <- 250
irt_max <- 25000

# note how many IRTs are excluded for implausibility
irt_lo <- subset(irt_data, irt < irt_min)
irt_hi <- subset(irt_data, irt > irt_max)

# subset to trimmed experimental items
irt_data <- subset(
  irt_data, 
  irt > irt_min & irt < irt_max & !isFiller
)
# make Participant and Item factors with specified ref levels 
irt_data$Participant<-relevel(irt_data$Participant,ref=5)
irt_data$Item<-relevel(as.factor(irt_data$Item),ref=1)
# winsorize top/bottom 5% of data (for normal distrbution, this is +/- 2sd)
# see: https://sciencing.com/relationship-between-standard-deviations-percentiles-8768703.html
win_trim=0.05
irt_data <- irt_data %>% group_by(Participant) %>% mutate(win_irt = winsor(irt, trim=win_trim))

# log 10 transform
irt_data$wirt.log10 <- log10(irt_data$win_irt)

# note properties
irt_props <- describe(irt_data$irt)
wirt_props <- describe(irt_data$win_irt)

# sd by condition table
sdByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=sd
)
# means by condition table
meansByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=mean
)
names(meansByConditionLong)[3] <- "mean"
# concise means by condition table
meansByCondition <- meansByConditionLong %>% spread(Q,mean) 
names(meansByCondition)[1] <- "Condition"
# add sd and se to meansByConditionLong
meansByConditionLong$sd<-sdByConditionLong$x
meansByConditionLong$se<-sdByConditionLong$x/sqrt(irt_props$n)


# difference in condition means
diffs<-list(Condition="Increase", `-Q`=diff(meansByCondition$`-Q`), `+Q`= diff(meansByCondition$`+Q`))
meansByCondition<-rbind(meansByCondition, diffs)
cdiff <- meansByCondition$`-Q`[3]-meansByCondition$`+Q`[3]

# participant means by condition
pmeansByCondition <- aggregate(
  irt_data$irt,
  by=list(
    "Condition"=irt_data$cond,
    "Participant"=irt_data$Participant
  ),
  FUN=mean
) %>% spread(Condition,x) 
pmeansByCondition$pattern <- (
  pmeansByCondition$`+Q +GP` - pmeansByCondition$`+Q -GP` <
  pmeansByCondition$`-Q +GP` - pmeansByCondition$`-Q -GP`
) 

# simple P means
spm <- aggregate(
  irt_data$irt,
  by=list(
    "Participant"=irt_data$Participant
  ),
  FUN=mean
) 

# meta 
json <- read_file("../meta-4_11.json")
meta<-fromJSON(json)
handset <- read.csv("../csvs/_SITA_SET_VALUES.csv")
meta[meta$file %in% handset$Filename,]$agg <- NA
meta[meta$file %in% handset$Filename,]$hpf <- NA
hscount <- sum(handset$Filename %in% meta$file)
```

This document examines the inter-reading time (IRT) from the study. Subjects were asked to read each sentence twice, once with no preview at all (reading 1, a cold reading), and then again after unlimited preview (reading 2, a previewed reading). Inter-reading time (IRT) is a measure of the amount of time between when a subject stops speaking after a cold reading and when they begin speaking for a previewed reading. IRT was measured over `r recs` recordings: 32 participants, 48 items = `r subjs * items` recording pairs (reading 1 and reading 2), with `r  subjs * items - recs` missing pairs. The missing data are a result of one or both recordings from a pair being unusable due to technical issues (e.g. a failure of recording equipment, or participant error).

## IRT measurement

IRT was measured using a Python script and Google's WebRTC Voice Activity Detection (VAD) over 44.1kHz WAV files downsampled to 8kHz via SOX[^upsamp]. This VAD system uses Gaussian Mixture Models to make probabilistic decisions on whether a given audio frame is speech or noise (see [@gmm1] for a complete explanation). Google's implementation takes one paramater, which they call aggressiveness: a 4-tier setting for the level of confidence necessary to call a gvien frame speech. I call this "rejection rate", where a higher rejection rate means that the model requires a high level of confidence before assuming a frame is speech, i.e. it is more likely to label something noise than speech. The implementation codes this setting as 0-3, where 0 is the most lenient (most likely to label a frame as speech) and 3 is the most stringent (most likely to label a frame as noise).

[^upsamp]: Google's VAD API only accepts WAV files with sample rates that are a multiple of 8kHz. It ultimately downsamples all files to 8kHz, regardless of the input rate.

The recordngs vary in the volume of the speaker's voice and the amount of background noise present. An algorothm was constructed to allow for the most stringent measurement of the least modified data that gave plausible measurements. Specifically, each file was measured using the highest possible rejection rate for the VAD algorithm and no modification of the file. If the timings detected were not plausible, the timings were re-measured with the same rejection rate, but after the recording had undergone a 200Hz high-pass filter[^alg] (HPF). If that still failed, a 400Hz HPF was used. After a further failure, the rejection rate for the VAD was lowered, and the whole thing was tried again (0, 200Hz, 400Hz); and that process was itself repeated until the lowest possible rejection rate was tried of the four possible settings. 

Of the `r nrow(meta)` recordings subjected to this treatment, `r nrow(meta[meta$success & !is.na(meta$agg),])` resulted in plausible timings. For those that were successfull, the breakdown of HPF and rejection rate used is reported in Table \@ref(tab:metable). A further `r hscount` were set by hand, resulting in a total of `r nrow(meta[meta$success,])` recordings.

[^alg]: The exact algorithm is available on [github](https://gist.github.com/moui72/4ebc4eb8f69eb9fdb1cab160ce299675) (URL: [bit.ly/2uMrcrG](https://bit.ly/2uMrcrG))

```{r metable, tidy=FALSE}
metaTable<-xtabs(~agg+hpf, meta[meta$success,])
row.names(metaTable)<-c("Lowest VAD rejection rate","...","...","Highest VAD rejection rate")
kable(
  metaTable, 
  col.names = c("No HPF", "HPF at 200Hz", "HPF at 400Hz"),
  caption="VAD rejection rate and HPF values",
  booktab=T
)
```

Plausible timings had to meet the following criteria: 

1. An utterance length between 2s and 10s, where utterance timing is the longest contiguous span in the recording that VAD reports as phonation, with breaks in phonation of less than 1s not breaking contiguity.

2. A leading silence ("delay") length of more than 120ms and less than 95% of the entire recording's duration.

3. A trailing silence length of less than 95% of the entire recording's duration.

Stimuli range from 18-22 syllables in length. If we assume a speeach rate of 3 to 7 syllables per second [@jacewicz2010-sr] we would expect utterances between 2.5s and 7.3s. Conservative thresholds higher and lower than the expected were used, especially on the higher end to allow for any processing or fluency. @goldman1961-pa found that a large majority (82.5 to 87%) of pauses in fluent speech are less than 1s. Even a very fast human reaction time should not permit a delay shorter than 120ms.

# Distribution of IRT

The raw IRTs including fillers and before any outliers are trimmed are distributed as shown in Figure \@ref(fig:rawIRThist). Overall mean IRT of these data (n = `r irt_props_all$n`), is `r round(irt_props_all$mean/1000,1)`s. The longest is `r round(irt_props_all$max/1000,1)`s and the shortest `r round(irt_props_all$min,0)`ms. Median IRT is `r round(irt_props_all$median/1000,1)`s.

```{r rawIRThist,fig.cap="Distribution of raw IRT"}
histSettings = geom_histogram(binwidth = 500,color="white",fill="#333333")
ggplot(irt_data_all, aes(irt)) +
  histSettings +
  xlab("Raw IRT") + 
  ylab("Frequency") + 
  ggtitle("Distribution of raw IRT",subtitle="Bin size = 500ms")
```

IRTs below `r irt_min`ms (`r nrow(irt_lo)`) and above `r round(irt_max/1000,1)`s (`r nrow(irt_hi)`) are (assumed to be implausible) omitted. Experimental data were then Winsorized by participant to bring data in the `r win_trim*100`th and `r 100-(100*win_trim)`th percentile of data to the value at those tresholds. The resulting measure is referred to as wIRT and is distribued as shown in Figure \@ref(fig:wIRT) (n = `r wirt_props$n`). Overall mean for wIRT is `r round(wirt_props$mean/1000,1)`s. The longest IRT is `r round(wirt_props$max/1000,1)`s and the shortest is `r round(wirt_props$min,0)`ms. Median wIRT is `r round(wirt_props$median/1000,1)`s.

```{r wIRT,fig.cap="Distribution of wIRT"}
ggplot(irt_data, aes(win_irt)) +
  histSettings + 
  xlab("wIRT") + ylab("Frequency") + 
  ggtitle(
    "Distribution of wIRT (ms)", 
    subtitle="Bin size = 500ms"
  )
```

For the purposes of regression analysis, a common log transformation reduces the skew in the data. This distribution is seen in Figure \@ref(fig:log10wins).

```{r log10wins,fig.cap="Common log of wIRT"}
ggplot(irt_data, aes(wirt.log10)) +
  geom_histogram(binwidth = 0.1, color="white",fill="#333333") + 
  xlab("Common log of wIRT") + ylab("Frequency") + 
  ggtitle("Distribution of Common Log of wIRT", subtitle="Bin size = 0.1")

```

# Means by condition

Table \@ref(tab:mns) shows the mean wIRT by experimental condition. The top left cell represents the mean wIRT for the declaritive controls ("-Q -GP"). The bottom row shows the increase in IRT across the garden path condition. 

```{r mns}
meansByConditionS <- meansByCondition
meansByConditionS$`-Q` <- meansByConditionS$`-Q`/1000
meansByConditionS$`+Q` <- meansByConditionS$`+Q`/1000
kable(
  meansByConditionS, 
  caption = "Means (s) by condition",
  digits = 2,
  booktab=T
)
```

The difference in the effect of &plusmn;GP across &plusmn;Q is `r round(cdiff/1000,2)`s. That is, the mean amount that IRT increased for a garden path declarative compared to a non-garden path declarative is `r round(cdiff/1000,2)`s `r ifelse(cdiff > 0, paste("more"),paste("less"))` than the amount that IRT increased for a garden path interrogative compared to a non-garden path interrogative.

```{r interactinplot,fig.cap="Mean IRT by condition",fig.height=4}

meansByConditionLong$Qn <- ifelse(
  meansByConditionLong$Q == "+Q", 
  "Interrogative", 
  "Declarative"
)

meansByConditionLong$GPn <- ifelse(
  meansByConditionLong$GP == "+GP", 
  "Garden path", 
  "Non-garden path"
)

ggplot(
  meansByConditionLong, 
  aes(mean,x=GPn,y=mean,group=Qn,linetype=Qn)
) + 
  geom_line() +
  geom_point() +
  ylim(6000,7200) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.125) +
  labs(x="", y="Mean IRT (ms)",linetype="",title="Mean IRT by condition", subtitle = "Confidence intervals represent one standard error")
```

# Regression models of IRT

The models with random slopes for participant and item did not converge, so the tables in this section show models with no random slopes. 

```{r lmeMods, cache=T}

irt.full <- lmer(
  wirt.log10 ~ Condition_Q * Condition_GP + 
    (1 | Participant) + 
    (1 | Item),
  data = irt_data,
  REML=F
)

irt.noInteraction <- lmer(
  wirt.log10 ~ Condition_Q + Condition_GP + 
    (1 | Participant) + 
    (1 | Item),
  data = irt_data,
  REML=F
)

irt.noParticipant <- lmer(
  wirt.log10 ~ Condition_Q * Condition_GP + 
    (1 | Item),
  data = irt_data,
  REML=F
)

irt.noItem <- lmer(
  wirt.log10 ~ Condition_Q * Condition_GP + 
    (1 | Participant),
  data = irt_data,
  REML=F
)

irt.noFxd <- lmer(
  wirt.log10 ~
    (1 | Participant) + 
    (1 | Item),
  data = irt_data,
  REML=F
)

irt.noRand <- lm(wirt.log10 ~ Condition_Q * Condition_GP,
                 data = irt_data)

irt.dummy <- lm(wirt.log10 ~ Condition_Q * Condition_GP,
                  data = irt_data
                )
irt.dummy.noInt <- lm(wirt.log10 ~  Condition_Q + Condition_GP + 
                  Participant + Item,
                  data = irt_data
                )
```

For the first model, fixed effects of &plusmn;GP and &plusmn;Q as well is the interaction between them were included, along with random effects of participant and item. The second model removes the interaction, but keeps both main effects.

```{r modTabLME,fig.fullwidth=T,fig.align="left",fig.cap="Models"}
huxreg(
  list(
    "Full"=irt.full, 
    "No interaction"=irt.noInteraction,
    "No random effects"=irt.noRand,
    "No fixed effects"=irt.noFxd
  ),
  coefs = c(
    "+GP"="Condition_GPTRUE",
    "+Q"="Condition_QTRUE",
    "+GP +Q"="Condition_QTRUE:Condition_GPTRUE"
  ),
  note=NULL,
  statistics = c(N="nobs","logLik","AIC")
) %>% 
  set_caption("Models") %>% 
  set_position("left") %>% 
  set_label("tab:models")
```

A model with no fixed effects and one with no random effects were also run. The estimates from these models can be seen in table \@ref(tab:models). 

## Model comparisons

Several model comparisons were made, seen in the following tables.

```{r modCom3}
comp <-anova(irt.full,irt.noFxd)
tab <- hux(comp[,c(1:2,4,6:8)],autoformat = T)
tab <- add_rows(
  tab, 
  hux("Df","AIC","logLik","&Chi;<sup>2</sup>","Df(&Chi;)","p value"),
  after=0
)
tab <- insert_column(
  tab, c("","No fixed effects model", "Full model")
)

tab %>% set_escape_contents(F,row=1,col=5:7) %>%
  set_number_format(value=2,col = c(3:5,7),row=2:3) %>%
  set_label("tab:modComp3") %>% 
  set_caption("No fixed effects vs. full model") %>%
  set_top_border(row=1,value=1,col=everywhere) %>%
  set_bottom_border(row=1,value=1,col=2:7) %>% 
  set_bottom_border(row=final(1),value=1,col=everywhere) %>% 
  set_position("left")
```

There are clearly main effects of both &plusmn;GP and &plusmn;Q: table \@ref(tab:modComp3) shows that the full model is significantly better than the one with no fixed effects. 

```{r modCom}
comp <-anova(irt.full,irt.noInteraction)
tab <- hux(comp[,c(1:2,4,6:8)],autoformat = T)
tab <- add_rows(
  tab, 
  hux("Df","AIC","logLik","&Chi;<sup>2</sup>","Df(&Chi;)","p value"),
  after=0
)
tab <- insert_column(
  tab, c("","No interaction", "Full")
)

tab %>% set_escape_contents(F,row=1,col=5:7) %>%
  set_number_format(value=2,col = c(3:5,7),row=2:3) %>%
  set_label("tab:modComp") %>% 
  set_caption("No interaction vs. full model") %>%
  set_top_border(row=1,value=1,col=everywhere) %>%
  set_bottom_border(row=1,value=1,col=2:7) %>% 
  set_bottom_border(row=final(1),value=1,col=everywhere) %>% 
  set_position("left")
```

The interaction between main effects, though, is not able to be confirmed (in fact, table \@ref(tab:modComp) shows the non-interaction to be better, but not to a stastically significance degree).

```{r modCom2}
comp <-anova(irt.full,irt.noRand)
tab <- hux(comp[,c(1:2,4,6:8)],autoformat = T)
tab <- add_rows(
  tab, 
  hux("Df","AIC","logLik","&Chi;<sup>2</sup>","Df(&Chi;)","p value"),
  after=0
)
tab <- insert_column(
  tab, c("","No random effects", "Full")
)

tab %>% set_escape_contents(F,row=1,col=5:7) %>%
  set_number_format(value=2,col = c(3:5,7),row=2:3) %>%
  set_label("tab:modComp2") %>% 
  set_caption("No random effects vs. full model") %>%
  set_top_border(row=1,value=1,col=everywhere) %>%
  set_bottom_border(row=1,value=1,col=2:7) %>% 
  set_bottom_border(row=final(1),value=1,col=everywhere) %>% 
  set_position("left")
```

Table \@ref(tab:modComp2) shows that the random effects of participant and item improve the model in a stastically signifcant way.

# Delay comparison for cold vs. previewed readings

A comparison of the delay for cold readings compared with that of previewed readings can lend insight into the extent to which subjects followed task instructions.

"Delay" here is the amount of time after the start of a recording until the beginning of phonation of the target sentence. Cold readings are also called "reading 1", while previewed readings are the same as "reading 2". Implausible delays of >15s are excluded in the data shown here.

```{r delayComparison, echo=F}
raw_rs_file <- read_csv("../csvs/merged.csv")
raw_rs <- subset(raw_rs_file,!isFiller & Leading < 15000)
raw_rs$reading <- raw_rs$Reading
raw_rs$Reading <- ifelse(raw_rs$reading == 1, "Cold", "Previewed")
ggplot(raw_rs, aes(Leading, fill = Reading)) +
  geom_histogram(binwidth = 400,position="dodge",color="black") +
  scale_fill_manual(values=c("black","white")) +
  ggtitle("Distribution of delay by reading", subtitle = "Bin size = 400ms")

diffs <- raw_rs[c("Reading","Leading","Participant","Item","isFiller","Condition_Q","Condition_GP")]  %>% spread(Reading,Leading)

diffs$diffs <- diffs$Previewed-diffs$Cold
diffDis<- describe(diffs$diffs)
diffsByP<-aggregate(diffs~Participant,data=diffs,FUN=function(x){round(mean(x))})
colnames(diffsByP) <- c("Participant", "Mean difference in delay (ms)")
diffsByP.props <- describe(diffsByP$`Mean difference in delay (ms)`)
```

For cold readings, n = `r table(raw_rs$Reading)["Cold"]` and for previewed, n = `r table(raw_rs$Reading)["Previewed"]`. 

## Difference in delay across paired readings

Overall, each recording pair (n = `r diffDis$n`) has a mean difference in delay (DelDif = previewed delay - cold delay) of `r round(diffDis$mean/1000,1)`s (sd = `r round(diffDis$sd/1000,1)`s), with a minumum of `r round(diffDis$min/1000,1)`s and a max of `r round(diffDis$max/1000,1)`s. The median DelDif is `r round(diffDis$median/1000,1)`s. The distribution DelDif is shown in Figure \@ref(fig:deddif).

```{r deddif, fig.cap="Distribution of DelDif"}
ggplot(na.omit(diffs), aes(diffs)) +
  geom_histogram(binwidth = 500,color="white",fill="black") +
  labs(x="Difference in delay", y="Count",title="Distribution of difference in delay",subtitle="Bin width = 500ms")
```

If we calculate the mean delay difference by participant, we find a mean participant DelDef of `r round(diffsByP.props$mean/1000,1)`s. Each participant's DelDif is &le; `r diffsByP.props$min`ms and &ge; `r round( diffsByP.props$max/1000,1)`s, with a median of `r round(diffsByP.props$median/1000,2)`s. Table \@ref(tab:difsbyp) shows these values.

```{r difsbyp}
kable(
  diffsByP[order(diffsByP[,2]),],
  digits = 0,
  caption="Delay differences by participant",
  booktab=T,
  longtab=T
)
```

The distribution of the participants' DelDifs can be found in Figure \@ref(fig:difhistbyp).

```{r difhistbyp, fig.cap="Mean difference in delay by participant"}
ggplot(diffsByP, aes(`Mean difference in delay (ms)`/1000)) +
  geom_histogram(binwidth = 1,color="white",fill="black") +
  labs(x="Mean difference in delay", y="Count",title="Mean difference in delay by participant", subtitle="Bin size = 1s")
```

# Individual variation in IRT

Individuals vary with regard to the effect of the garden path condition on IRT. For `r table(pmeansByCondition$pattern)["TRUE"]` of `r nrow(pmeansByCondition)`, the increase in IRT for garden paths is greater for interrogatives than it is for declaratives.

```{r pPattern}
kable(
  pmeansByCondition[c(1:5)],
  caption="Mean wIRT (ms) by condition and participant",
  booktab=T,
  longtab=T
)
```

```{r irtzoomLft, fig.cap="Left tail of raw IRT distribution"}
ggplot(subset(irt_data_all,irt<1000), aes(irt)) +
  geom_histogram(binwidth = 50,color="white",fill="#333333") + 
  xlab("Raw IRT") + ylab("Frequency") + 
  ggtitle("Left tail of IRT distribution (ms)",subtitle="IRT < 1s, bin size = 50ms")
```


```{r zoomRgt, fig.cap="Right tail of raw IRT distribution"}
ggplot(subset(irt_data_all,irt>22000), aes(irt/1000)) +
   geom_histogram(binwidth = 1, color="white", fill="#333333") +
  xlab("Raw IRT") + ylab("Frequency") + 
  ggtitle("Right tail of IRT distribution (s)", subtitle="IRT > 22s, bin size = 1s")
```

# Interrogative parsing cost

As previously discussed, interrogatives appear to have a computational processing cost when compared to interrogatives [cf. @lehiste1973phonetic]. The filler sentences in this study were designed so as to provide a diagnostic of the interrogative effect on IRT.

```{r fillers}
```

`r if (knitr::is_html_output()) '# References {-}'`