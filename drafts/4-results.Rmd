---
always_allow_html: yes
---

```{r results, echo=F, warning=F,message=F}
library(ggplot2)
library(ggthemes)
library(extrafont)
library(psych)
library(Cairo)
library(readr)

loadfonts("pdf")
knitr::opts_chunk$set(
  echo=FALSE, 
  warning=FALSE, 
  message=FALSE, 
  fig.height = 3, 
  fig.pos="H"
)

bgcolor <- "#ffffff"
par(bg = bgcolor)

kable <- function(...) {
  knitr::kable(booktab = TRUE,...)
}

### global plot themeing
theme_set(
  theme_tufte(base_size = 12) + 
    theme(
      plot.background = element_rect(fill = bgcolor, color = bgcolor),
      panel.border = element_blank(),
      text = element_text(color="black"),
      axis.text = element_text(color="black")
    )
)

# sentence type order
sto <- function (SentenceType) {
  ifelse(SentenceType %in% c("D Arg", "Q Arg", "D Mod", "Q Mod"),
    return(
      ifelse(
        SentenceType=="Q Arg",
        length(SentenceType),
        ifelse(
          SentenceType=="D Arg",
          length(SentenceType)-1,
          ifelse(
            SentenceType=="Q Mod",
            length(SentenceType)-2,
            length(SentenceType)-3
          )
        )
      )
    ),
    return(1)
  )
}

```
# Results and discussion {#res}

This chapter reports various descriptions and analyses of the recordings obtained, and the relevance of those findings to the research questions motivating this study. The reported results include the effect of Speech Act (declarative/D vs. interrogative/Q) and PP2 Status (modifier/Mod vs. argument/Arg) on the location of prosodic breaks, as well as on time spent by a participant considering a sentence between readings, i.e., the inter-reading time (IRT). In order to evaluate the extent to which participants adhered to the protocol as intended, i.e., began to read immediately for Reading 1 as opposed to producing a considered reading for Reading 2, the delay for which a sentence is displayed before a participant begins to read it is compared for Reading 1 (R1 delay) vs. Reading 2 (R2 delay). The prosodic patterns for participants with especially fast and especially slow R1 delays are presented as a way of investigating the extent to which individual differences might impact those patterns, and as a further exploration of the success of the protocol instructions in producing the intended behavior. A finding on the apparent processing cost of interrogative context when compared to declarative context among the filler sentences is also reported.

## Planned analyses

There are two types of planned analyses presented in this section. Two primary planned analyses: prosodic phrasing and IRT; one ancillary planned analysis: the effect of interrogativity on IRT. 

### Data for analysis {#data}

Data for 32 participants in total were analyzed. Given 4 versions of the experiment and 2 possible orderings there would ideally be 4 participants per version-order combination. Due to the exclusion of some participants discussed in Section \@ref(parti), the actual distribution was as shown in \@ref(tab:vtab).

IRTs below 0.25s (n = 2) and above 25.0s (n = 5) were assumed to be implausible and omitted from the analyses reported below. Experimental data were then Winsorized by participant to bring data below the 2.5% and above the 97.5% threshold to the value at those thresholds, resulting in the distribution as shown in figure \@ref(fig:wIRT) (n = 489).

```{r wIRT,fig.cap="Distribution of IRT."}
irt_data <- read_csv("export/irt_data.csv")
ggplot(irt_data, aes(wirt/1000)) +
  geom_histogram(binwidth = 0.5,color="white",fill="#333333") + 
  labs(subtitle="Bin size = 0.5s",xlab="IRT (seconds)",ylab="Frequency")
ggsave("wIRT.pdf",device=cairo_pdf)
# dev.off()
```

Overall mean for IRT was 6.5s (sd = 3.8). The longest IRT was 22.2s and the shortest was 0.7s. Median IRT was 6.1s.

[^xtra]: The two 5-count cells include 2 additional participants whose data were collected in pursuit of another full set (i.e., towards an expansion to 40 participants) that was not completed due to a lack of participant sign-ups.

```{r vtab, echo=F}
library(kableExtra)
library(readr)
library(dplyr)


mdata <- read_csv("export/prosody_data.csv")

versiontab <- mdata %>% 
  filter(!duplicated(Participant)) %>%
  with(table("Version"=LIST,Order)) %>%
  addmargins() 

rownames(versiontab)[1:4]<-paste("Version", rownames(versiontab))
versiontab[1:4,1:2] %>% kable(
  caption="Number of participants per version-order combination.",
  col.names = c("1", "2"),
  align="c",
  booktab=T
) %>% kable_styling() %>% 
  add_header_above(c(" "=1, "Order"=2)) %>%
  kable_styling(latex_options = c("hold_position"))
```

Some of the expected 3072 recordings (32 participants x 2 readings x 48 items, 16 experimental and 32 filler) could not be used due to intrusive background noise during the recording session. Additionally, data were also excluded from analysis if either of a Reading 1/Reading 2 pair was missing; 9 such incomplete pairs were excluded. Without analyzable data from both members of a pair, it is difficult to determine the extent to which the elicitation protocol was executed as intended (i.e., the extent of preview for Reading 1 vs. Reading 2). The number of recordings per sentence type included in the analyses that follow were distributed as shown in Table \@ref(tab:rvtab); if no data had been omitted, the expected count per sentence type was 256.

```{r rvtab, echo=F}
library(tidyr)
tot<-as.data.frame(xtabs( ~ PP2Status + SpeechAct, data=mdata))
gtot <- tot

totDisp <- gtot %>% as.data.frame() %>%
  spread(SpeechAct,Freq) %>% arrange(desc(PP2Status))


kable(
  totDisp,
  caption="Number of recordings analyzed, as a function of Speech Act and PP2 Status.",
  align="c",
  col.names=c(" ","D","Q")
) %>% kable_styling(latex_options = c("hold_position"))
```

For experimental items, 978 recordings were subjected to prosodic analysis, constituting 95.6% of the utterances elicited. Because IRT is a measure arising from pairs of utterances (Reading 1 and Reading 2) rather than from single recordings, the database for response timing took in 489 data points.

### Prosodic boundary patterns {#results-prosody}

This section will report the prosodic phrasings found in the recordings collected, and the extent to which those patterns are or are not influenced by the design parameters of the study (Speech Act and PP2 Status), as well as which reading (Reading 1 or Reading 2) a given recording represents. These data are reported first descriptively (i.e., in terms of frequency), and then using regression models to calculate the statistical significance of whatever patterns emerge from the data. Finally, a summary of findings and their implications for the hypothesis motivating this study is provided.

```{r byGPr2}
library(scales)
### r2
r2data <- subset(mdata,Reading==2)

pp1tab<-xtabs(PP1 ~ PP2Status + SpeechAct, data=r2data)
objtab<-xtabs(OBJ ~  PP2Status + SpeechAct, data=r2data)
tot<-as.data.frame(xtabs( ~  PP2Status + SpeechAct, data=r2data))


pp1<-as.data.frame(pp1tab)
pp1$tot<-tot$Freq
pp1$pct <- pp1$Freq/pp1$tot
pp1$pretty <- sprintf("%s (%d)",percent(pp1$pct),pp1$Freq)
pp1disp <- pp1 %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

obj<-as.data.frame(objtab)
obj$tot<-tot$Freq
obj$pct <- obj$Freq/obj$tot
obj$pretty <- sprintf("%s (%d)",percent(obj$pct),obj$Freq)
objdisp <- obj %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

pp1obj <- cbind(pp1disp,objdisp[2:3])

r2pp1<-pp1disp
r2obj<-objdisp


### r1
r1data <- subset(mdata,Reading==1)

pp1tab<-xtabs(PP1 ~ PP2Status + SpeechAct, data=r1data)
objtab<-xtabs(OBJ ~  PP2Status + SpeechAct, data=r1data)
tot<-as.data.frame(xtabs( ~  PP2Status + SpeechAct, data=r1data))



pp1<-as.data.frame(pp1tab)
pp1$tot<-tot$Freq
pp1$pct <- pp1$Freq/pp1$tot
pp1$pretty <- sprintf("%s (%d)",percent(pp1$pct),pp1$Freq)
pp1disp <- pp1 %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

obj<-as.data.frame(objtab)
obj$tot<-tot$Freq
obj$pct <- obj$Freq/obj$tot
obj$pretty <- sprintf("%s (%d)",percent(obj$pct),obj$Freq)
objdisp <- obj %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

pp1objr1 <- cbind(pp1disp,objdisp[2:3])

r1pp1<-pp1disp
r1obj<-objdisp

pp1<-cbind(r1pp1,r2pp1[2:3])


obj<-cbind(r1obj,r2obj[2:3])

```

In what follows, the distribution of OBJ boundaries and PP1 boundaries are reported as a function of the four sentence types created by the materials design (D/Q x Mod/Arg), for each of Reading 1 and Reading 2. Then, the patterns of boundaries over the two positions are considered, before moving to statistical analysis. Note that while judgments about the potential boundary after the infinitival construction verb were collected, reports of these were exceptionally rare (occurring in only 8% of recordings), and thus have been set aside. The break locations are indicated with a % symbol in (@breakpos), which was previously provided in Section \@ref(sita) as (@bpos) and is repeated here for convenience.

  (@breakpos)
  \begingroup
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccccccc}
      & & \footnotesize V Break & & \footnotesize OBJ Break & & \footnotesize PP1 Break & \\
      She had & wanted to set & \% & the textbooks & \% & on the top shelf & \% & into the file box. \\
      \cmidrule(r){2-2} \cmidrule(r){4-4} \cmidrule(r){6-6} \cmidrule(r){8-8} 
      & \footnotesize V Region & & \footnotesize OBJ Region & & \footnotesize PP1 Region & & PP2 Region \\
    \end{tabular}
  \endgroup

The occurrence of the OBJ boundary was somewhat less frequent for Reading 1 of Arg sentences (`r (57.7+56.7)/2`%) than for Reading 2 of Arg sentences (`r (73.0+74.2)/2`%).

```{r obj}
names(obj)[2:5] <- c(2:5)
obj <-obj %>%  arrange(desc(PP2Status))
kable(
  obj,
  caption="Percent occurrence of OBJ boundary (frequency of occurrence in parenthesis) as a function of sentence type and Reading.",
  col.names = c("", "D", "Q","D", "Q"),
  align="c"
) %>% 
  add_header_above(c(" "=1,"Reading 1"=2,"Reading 2"=2)) %>% 
  kable_styling(latex_options = c("hold_position"))
```

The PP1 break was almost always present for cases where PP2 was an argument (98.7% occurrence, overall). That boundary was present substantially less often (68.0%) for cases where PP2 could be interpreted as a modifier. Speech Act and reading did not appear to impact the overall distribution of the PP1 break.

```{r pp1}
names(pp1)[2:5] <- c(2:5)
pp1 <-pp1 %>%  arrange(desc(PP2Status))
kable(
  pp1,
  caption="Percent occurrence of PP1 boundary (frequency of occurrence in parenthesis) as a function of sentence type and Reading.",
  col.names = c("", "D", "Q","D", "Q"),
  align="c"
) %>% 
  add_header_above(c(" "=1,"Reading 1"=2,"Reading 2"=2)) %>% 
  kable_styling(latex_options = c("hold_position"))
```

When looking at both breaks together, a sentence could have one of four patterns: both the OBJ and PP1 break present; only OBJ present; only PP1 present; or neither break present. There were only 5 cases where neither was present, and those were omitted in all subsequent reports of prosodic patterns.

```{r r2combobreaksBycondpre}
r2pdata<-droplevels(subset(mdata,Reading==2 & simple2lvl != "NEITHER"))
qdata <- subset(r2pdata,PP2Status=="Arg")
ddata <- subset(r2pdata,PP2Status=="Mod")
qpros<-table(qdata$simple2lvl,qdata$SpeechAct) %>%
  prop.table(margin=2) %>% apply(2,percent)

dpros<-table(ddata$simple2lvl,ddata$SpeechAct) %>%
  prop.table(margin=2) %>% apply(2,percent)

r2combo<-as.data.frame(cbind(dpros,qpros))

write_csv(r2combo,"export/ccombo.csv")

r1pdata<-droplevels(subset(mdata,Reading==1 & simple2lvl != "NEITHER"))
qdata <- subset(r1pdata,PP2Status=="Arg")
ddata <- subset(r1pdata,PP2Status=="Mod")
qpros<-table(qdata$simple2lvl,qdata$SpeechAct) %>%
  prop.table(margin=2) %>% apply(2,percent)


dpros<-table(ddata$simple2lvl,ddata$SpeechAct) %>%
  prop.table(margin=2) %>% apply(2,percent)

r1combo<-as.data.frame(cbind(dpros,qpros))


bothrbothb <- cbind(r1combo,r2combo)
bothrbothb <- bothrbothb %>% as_tibble(.name_repair = "unique")
row.names(bothrbothb) <- c("2Both","1OBJ","3PP1")
bothrbothb<-arrange(bothrbothb,row.names(bothrbothb))
row.names(bothrbothb) <- c("OBJ only","Both","PP1 only")

```

```{r bothbreaks}
kable(
  bothrbothb,
  caption="Percent occurrence of both boundaries as a function of sentence type and Reading.",
  align="c",
  col.names=rep(c("D","Q"),4)
) %>%
  add_header_above(c(" "=1,rep(c("Mod" = 2, "Arg"=2),2))) %>%
  add_header_above(c(" "=1, "Reading 1" = 4, "Reading 2" = 4)) %>%
  column_spec(1,bold=T)%>% kable_styling(latex_options = "hold_position")

```

There was very little difference across readings, with the following generalizations of the data shown in Table \@ref(tab:bothbreaks) (and its companion Figure \@ref(fig:bothbreaks2)) holding for both Reading 1 and Reading 2. For Mod sentences the OBJ-only pattern is relatively frequent (31.1% in declaratives, 31.4% in interrogatives), wheres for Arg sentences there are very few instances with the OBJ-only pattern (0.8% in declaratives, 2.5% in interrogatives). The pattern with both breaks is less common for  Mod (54.1% in declaratives, 43.0% in interrogatives) sentences than for Arg sentences (72.1% in declaratives, 71.7% in interrogatives). The PP1-only pattern occurs at about the same rate in Mod interrogatives (25.6%) as in Arg declaratives (27%) and Arg interrogatives (25.8%), but is noticeably less common for Mod declaratives (14.8%). These proportions are visually represented in Figure \@ref(fig:bothbreaks2).

```{r bothbreaks2, fig.cap="Break pattern as a function of sentence type and Reading.",fig.pos="!H", fig.height=3.5}

mdata$SentenceType <- paste(mdata$SpeechAct,mdata$PP2Status)
adata <- subset(mdata,simple2lvl!="NEITHER")
adata$reading <- paste("Reading", adata$Reading)

ggplot(adata,    
    aes(
      x=reorder(SentenceType, sto(SentenceType)),
      y=..count..,
      fill=reorder(
        simple2lvl,
        ifelse(simple2lvl=="PP1", 3, ifelse(simple2lvl=="OBJ",1,2))
      )
    )) +
  geom_bar(position="fill",color="black",width=0.5) +
  labs(fill="Pattern",x=" ",y=" ") +
  facet_grid(cols = vars(reading)) +
  scale_y_continuous(labels=scales::percent) + 
  scale_fill_brewer(breaks=c("OBJ", "BOTH", "PP1"), palette="Greys")

ggsave("bothbreaks2.pdf",height=3.75,device=cairo_pdf)
# dev.off()
```

The relative strength of the PP1 and OBJ boundaries was also collected. Figure \@ref(fig:bdom) incorporates this information, where PP1-dominance means that the PP1 boundary was reported to be stronger than the OBJ break; OBJ-dominance means the reverse; and “Equal strength” means that neither boundary was reported to be stronger than the other (the 5 instances with no breaks were again omitted).

Considering the data in terms of dominance rather than simple occurrence results in the categorization of most of the cases of both boundaries being present as either PP1-dominant or OBJ-dominant. 

```{r bdom, fig.cap="Percent break dominance occurence as a function of sentence type and Reading.",fig.pos="!H", fig.height=3.5}
library(ggplot2)
library(ggthemes)
library(extrafont)
adata <- subset(mdata,simple2lvl!="NEITHER")
adata$reading <- paste("Reading", adata$Reading)
adata$dom <- ifelse(adata$pdom,"PP1",ifelse(adata$odom,"OBJ","Equal strength"))

ggplot(adata,    
    aes(
      x=reorder(SentenceType, sto(SentenceType)),
      y=..count..,
      fill=reorder(
        dom,
        ifelse(simple2lvl=="OBJ", 1, ifelse(simple2lvl=="PP1",3,2))
      )
    )) +
  geom_bar(position="fill",color="black",width=0.5) +
  labs(fill="Dominant break",x="Sentence Type",y="Percent where dominant") +
  facet_grid(cols = vars(reading)) +
  scale_y_continuous(labels=scales::percent) + 
  scale_fill_brewer(breaks=c("PP1", "Equal strength", "OBJ"), palette="Greys") 

ggsave("bdom.pdf",device=cairo_pdf)
# dev.off()
```

Figure \@ref(fig:bdom) clearly shows a robust effect of PP2 Status (Mod or Arg) on break dominance, and little to no impact of Reading or Speech Act (D or Q). A dominant break after PP1 is frequent for all Arg sentences, both declaratives and questions, and in both Reading 1 and Reading 2. It is less frequent for Mod sentences in both Reading 1 and Reading 2. The higher relative frequency of PP1 break dominance in Arg sentences compared to Mod sentences is expected, since it represents a syntactically motivated prosodic break in the Arg cases. For the Mod sentences, there is no syntactic motivation for the PP1 break, so it makes sense for it to be less frequent and less dominant in Mod sentences. It is noteworthy and puzzling that the patterns do not differ greatly across the two readings, as one would expect difficulty getting the prosody right on the first try for the difficult sentences being tested in the current study. 

A number of mixed effects logistic regression models support the general observations above. Models predicting PP1 break, OBJ break, PP1 break dominance and OBJ break dominance are reported. All models include random intercepts for participant and item, but due to convergence errors, no random slopes for any predictors are included. 

The intercept always represents the Mod sentence type, which is not expected to present any particular difficulty to the reader, since Mod PP2 Status is compatible with what is assumed to be the running parse when it is encountered (i.e., PP1 has been interpreted as the goal argument of the verb, and PP2 does not disrupt that interpretation). For those models where Speech Act is included in the model, the intercept represents the declarative sentence type. In this way, the more complex sentence types are compared to the simplest one represented in the model. If Reading is included in a model, the intercept represents Reading 1.

For each analysis, a reduced model and the full model (i.e., the model containing all predictors of interest), when it converged, are reported. In each case, the reported reduced model is the simplest nested model[^nested] that did not represent a statistically significant worse fit according to likelihood ratio tests (LRTs) than the next most complex model. This model selection process was used for each of the four analyses that follow (OBJ and PP1 boundary occurrence, OBJ and PP1 break dominance). All reported models represent statistically significant improvement over a minimal model, which was a model with only an intercept and random effects (no fixed effects). That comparison is reported for each selected reduced model, as well as the Akaike Information Criterion (AIC[^aic]) value for the selected model and the minimal model. All regression models were run using the lme4 R package (@R-lme4).

[^nested]: A simpler model nested within a more complex model is one that includes a subset of the more complex model's predictors, e.g., OBJ Dominance as a function of PP2 Status is nested within OBJ Dominance as a function of Speech Act $\times$ PP2 Status.

[^aic]: AIC is a representation of the amount of information lost by using a regression model to estimate data points. It is a measure that balances both the goodness of fit of a model and the simplicity of a model, guarding against over fitting and under fitting the data involved.

In the full model predicting OBJ break occurrence, shown in Table \@ref(tab:fullobjmod), only the estimate for D Mod Reading 1 (the intercept) and the effect of PP2 Status show statistical significance.

```{r fullobjmod}
csv <- '
Outcome: OBJ break (Full),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",0.93,0.59,.11
Q,                             0.83,0.74,.26
Arg,                          -1.4,0.71,< .05
Reading 2,                     0.56,0.35,.11
Q:Arg,                        -1.03,1.00,.30
Q:Reading2,                   -0.81,0.48,.09
Arg:Reading2,                  0.29,0.46,.53
Q:Arg:Reading2,                0.97,0.65,.13
'
tab <- read_csv(csv,na=character())
tab %>% kable(
  align="rccc", 
  caption="Mixed effects logistic regression model predicting OBJ break occurrence (Full)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")

```

Table \@ref(tab:objMod) shows a reduced model, predicting the occurrence of an OBJ break with estimates for the coefficients of the fixed effects of Reading 2, PP2 and the interaction between Reading and PP2 Status. The removal of Speech Act as a predictors revealed the Reading $\times$ PP2 Status interaction to become a significant predictor. The model that included PP2 Status and its interaction with Reading provided a statistically significantly better fit than a minimal model (AIC~Minimal~=1068.0, AIC~Reduced~=1031.6, $\chi^2$(3)=42.34, p < .001).

```{r objMod}
csv <- 'Outcome: OBJ break (Reduced),Estimate,Std. Error,p
"Mod, Reading 1 (Intercept)",1.39,0.45,< .01
Reading 2,0.11,0.23,.62
Arg,-1.98,0.5,< .001
Reading2:Arg,0.81,0.32,< .05'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc", 
  caption="Mixed effects logistic regression model predicting OBJ break occurrence (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")

```

The log odds[^logodds] of an OBJ break for Mod Reading 1 is 1.39 (std. error = 0.45, p < .01). The log odds of that break increased in Reading 2 but the increase was not statistically significant. PP2 arguments reduced the log odds of an OBJ break compared to PP2 modifiers by a robust amount, but less so in Reading 2 than in Reading 1.

The OBJ break is expected to occur more often in Mod cases, because that break marks the argument attachment (and therefore a change in branching direction) of PP1.

[^logodds]:  Log odds is, in this case, the natural log of the odds ratio, so the log odds of A is log~e~(P(A)/P(¬A)). A log odds of 1.39 translates to an odds ratio of 4.01:1 (e^1.39^=4.01) and a probability of 80% (4.01/(1+4.01)=0.80).

The full model for predicting PP1 boundary occurrence showed significance only for the intercept and the effect of PP2 Status. 

```{r fullpp1Mod}
csv <- '
Outcome: PP1 break (Full),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",0.68,0.07,< .001
Q,0.01,0.09,.89
Arg,0.31,0.09,< .001
Reading 2,0.00,0.04,.99
Q:Arg,0.00,0.13,0.97
Q:Reading2,-0.01,0.06,.82
Arg:Reading2,0.00,0.06,.99
Q:Arg:Reading2,0.00,0.08,.97
'

tab <- read_csv(csv,na=character())

tab %>% kable(
  align="rrrl",
  caption="Mixed effects logistic regression model predicting PP1 break occurrence (Full)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

The model selection process described above found that the the best model was the one with PP2 Status as the only fixed effect, which was found to be better than a minimal model by LRT (AIC~Minimal~=855.59, AIC~Reduced~=629.60, $\chi^2$(1)=228, p < .001).

```{r pp1Mod}
csv <- '
Outcome: PP1 break (Reduced),Estimate,Std. Error,p
"Mod (Intercept)",0.96,0.3,< .01
Arg,4.12,0.44,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting PP1 break occurrence (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

Sentences with argument PP2s had greatly increased log odds of a PP1 break compared to ones with modifier PP2s. This is again expected, because the PP1 break is indicating the change in branching direction for argument attachment of PP2. That Speech Act is not a relevant predictor is evidence against a prosodic explanation of the motivating intuition for this study; we would expect both a main effect of Speech Act and definitely an interaction between Speech Act and PP2 Status, if the prosody were more (or less) different across the PP2 Status factor for interrogatives than for declaratives.

Models were also run for predicting break dominance. The full model predicting OBJ break dominance is shown in Table \@ref(tab:fodom).

```{r fodom}
csv <- '
Outcome: OBJ-dominance (Full),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",0.50,0.09,< .001
Q,0.03,0.12,0.82
Arg,-0.45,0.12,< .001
Reading 2,0.07,0.05,.22
Q:Arg,-0.03,0.17,.86
Q:Reading2,-0.03,0.07,.68
Arg:Reading2,0.03,0.07,.71
Q:Arg:Reading2,0.00,0.11,.97
'

tab <- read_csv(csv,na=character())

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting OBJ break dominance (Full)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

Table \@ref(tab:odom) reports the best model for predicting OBJ break dominance. The best model was one with fixed effects for reading and PP2 Status. A likelihood ratio test (LRT) found that the selected model represented a statistically significant improvement over a minimal model (AIC~Minimal~=1270.5, AIC~Reduced~=1079.3, $\chi^2$(2)=219.63, p < .001). There was no statistically significant effect of Speech Act on OBJ break dominance.

```{r odom}
csv <- '
Outcome: OBJ-dominance (Reduced),Estimate,Std. Error,p
"Mod, Reading 1 (Intercept)",-0.16,0.32,.62
Reading 2,0.4,0.16,< .05
Arg,-2.32,0.18,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting OBJ break dominance (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

PP1 break dominance and OBJ break dominance are not entirely complementary, because it is possible for both breaks to have equal prominence. As such, models predicting PP1 were also explored. 

The full model predicting PP1 break dominance failed to converge, so only the reduced model is reported.

Table \@ref(tab:pdom) reports the best model for predicting PP1 break dominance. Unlike the model for predicting OBJ break dominance, the best model for predicting PP1 break dominance includes Speech Act as a predictor. The best model is one with fixed effects for reading, Speech Act, and PP2 Status.

```{r pdom}
csv <- '
Outcome: PP1-dominance (Reduced),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",-0.19,0.33,.57
Reading 2,-0.38,0.15,< .05
Q,0.31,0.15,< .05
Arg,2.2,0.17,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting PP1 break dominance (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

This model was better than a minimal model according to LRT (AIC~Minimal~=1290.4, AIC~Reduced~=1078.8, $\chi^2$(3)=217.59, p < .001). PP1 break dominance was much more likely for sentences with argument PP2s than sentences with modifier PP2s, with interrogatives having slightly increased log odds of PP1 break dominance. Log odds of PP1 break dominance were slightly less in Reading 2 than Reading 1. There were no significant interaction terms.

Because reading was a significant predictor for 3 of the 4 models reported, and there are theoretical reasons to believe that Reading 2 is more representative of the natural or intended prosody of the reader, models were also run predicting PP1-dominance and OBJ-dominance for Reading 2 data only. In both cases, the best model had the same structure: fixed effects of Speech Act and PP2 Status, with no interaction term.

```{r r2dom}
csv <- '
,Estimate,Std. Err,p,Estimate,Std. Err,p
D Mod (Intercept),0.66,0.24,< 0.01,-0.97,0.27,< .001
Q,-0.3,0.22,0.16,0.35,0.22,.1
Arg,-2.07,0.24,< 0.001,2.15,0.24,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  col.names=c("(Reading 2 only)","Estimate","Std. Err","p","Estimate","Std. Err","p"),
  align="rccc",  caption="Mixed effects logistic regression models predicting break dominance in Reading 2 (Reduced)."
) %>% 
  add_header_above(c(" "=1, "Outcome: OBJ Dominance" = 3, "Outcome: PP1 Dominance" = 3)) %>%
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

For both OBJ-dominance and PP1-dominance, the main effect of Speech Act is non-significant, but its inclusion marginally improves the fit of each model. Even when limited to only Reading 2 data, Speech Act does not interact with PP2 Status, which is again supportive of a non-prosodic explanation for the motivating intuition. That there is a robust effect of PP2 Status is reassuring evidence that prosody is sensitive to syntax, and that the study's item construction is motivating the intended parse.

### Inter-reading time {#irt}

Inter-reading time is the amount of time after the completion of Reading 1 and before the beginning of phonation of Reading 2. The details of how this was measured and defined can be found in Section \@ref(method-irt). IRT is meant to provide an estimate of how much difficulty the reader has in processing a given sentence. If a reader spends more time studying a sentence prior to reading it aloud a second time, the IRT will be longer, which can be taken as an indicator of processing load on the first reading.

Since IRT is a measure across pairs of recordings (Reading 1/Reading 2), so the number of data analyzed in this section are half as many as those in the prosody analyses.

```{r}
irt_props <- describe(irt_data$irt)
wirt_props <- describe(irt_data$win_irt)
### sd by condition table
sdByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=sd
)

### means by condition table
meansByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=mean
)
names(meansByConditionLong)[3] <- "mean"
### concise means by condition table
meansByCondition <- meansByConditionLong %>% spread(Q,mean) 
names(meansByCondition)[1] <- "Condition"
### add sd and se to meansByConditionLong
meansByConditionLong$sd<-sdByConditionLong$x
meansByConditionLong$se<-sdByConditionLong$x/sqrt(irt_props$n)


### difference in condition means
diffs<-list(Condition="Increase", `D`=diff(meansByCondition$`D`), `Q`= diff(meansByCondition$`Q`))
meansByCondition<-rbind(meansByCondition, diffs)
cdiff <- meansByCondition$`D`[3]-meansByCondition$`Q`[3]

meansByConditionS <- meansByCondition
meansByConditionS$`D` <- meansByConditionS$`D`/1000
meansByConditionS$`Q` <- meansByConditionS$`Q`/1000

meansByConditionS$Condition <- c("Mod", "Arg", "Mod - Arg")
meansByConditionS$`Q - D` <- meansByConditionS$Q - meansByConditionS$D
```

```{r interactionplot,fig.cap="Mean IRT as a function of sentence type.",fig.height=3.5,fig.pos="H"}

meansByConditionLong$Qn <- ifelse(
  meansByConditionLong$Q == "Q", 
  "Interrogative", 
  "Declarative"
)

meansByConditionLong$GPn <- ifelse(
  meansByConditionLong$GP == "+GP", 
  "Arg", 
  "Mod"
)

ggplot(
  meansByConditionLong, 
  aes(mean/1000,x=reorder(GPn, desc(GPn)),y=mean/1000,group=Qn,linetype=Qn)
) + 
  geom_line() +
  geom_point() +
  ylim(5.75,7.25) +
  geom_errorbar(aes(ymin=(mean-se)/1000, ymax=(mean+se)/1000), width=.125) +
  labs(
    x="", 
    y="Mean IRT (ms)",
    linetype="",
    subtitle = "Error bars represent one standard error"
  )
ggsave("interact.pdf",device=cairo_pdf())
# dev.off()
```

Figure \@ref(fig:interactionplot) shows the mean IRT as a function of sentence type. The two slopes are only very slightly divergent. PP2 Status has a statistically significant effect on IRT: sentences with argument PP2s having substantially longer IRTs (6.7s) than those with modifier PP2s (6.3s). The effect of Speech Act was not statistically significant, though interrogatives attracted longer (6.7s) IRTs than declaratives (6.4s), and including Speech Act as a predictor of IRT improved the predictive power of the model. This is discussed in more detail below.

Regression models support the observations above. All models discussed include random intercepts for participant and item. Models with random slopes did not converge, and so random slopes were not included.
	
The full model for predicting IRT included fixed effects of Speech Act and PP2 Status and the interaction between them. This model is shown in table \@ref(tab:hyp).


```{r hyp}
csv <- '
IRT Full Model,Estimate,Std. Error,p
D Mod (Intercept),6.32,0.59,< .001
Q,0.29,0.3,.34
Arg,0.42,0.3,.17
Q:Arg,0.08,0.43,.85
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",  
  caption=
    "Linear mixed effects regression model predicting IRT by sentence type with interaction term (Full)."
) %>% 
  kable_styling(latex_options = "hold_position")
```

Of the nested models (those with subsets of the full model), the best model according to likelihood ratio testing (LRT) was the one with only the fixed effect of PP2 Status, shown in Table \@ref(tab:redirt), i.e., the difference between the reduced and full model was not statistically significant (AIC~Full~=2347.8, AIC~Reduced~=2346.2, $\chi^2$(2)=2.41, p = .3). The improvement of fit of the reduced model over a minimal model with only an intercept and random effects was statistically significant (AIC~Reduced~=2346.2, AIC~Minimal~=2348.7, $\chi^2$(1)=4.51, p < .05), indicating a robust main effect of PP2 Status on IRT.

```{r redirt}
csv <- '
IRT Reduced Model,Estimate,Std. Error,p
Mod (Intercept),6.47,0.58,< .001
Arg,0.46,0.21,< .05
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",  
  caption=
    "Linear mixed effects regression model predicting IRT by sentence type (Reduced)."
) %>% 
  kable_styling(latex_options = "hold_position")
```

These results are not supportive of *Hypothesis 3* formalized in Section \@ref(pred), as statistical significance was expected for the interaction of Speech Act with PP2 Status if interrogative context is indeed ameliorating for PP-attachment garden paths, compared to declarative context for the same construction.

### Discussion of primary planned analyses

Throughout the analysis of prosodic phrasing patterns, PP2 Status was the most robust predictor of OBJ and PP1 boundary occurrence and their relative strengths. The OBJ break was more frequent and more frequently dominant for sentences with a PP2 that was an argument than those with a PP2 that could be a modifier; conversely, the PP1 break was more frequent and more frequently dominant for sentences with a PP2 that was interpretable as a modifier than those with a PP2 that was an argument.

Returning to the predictions discussed in \@ref(pred), recall the contrast made there between linguistically motivated prosodic breaks and other breaks or pauses. The Arg and Mod sentences differ with regard to where the linguistically motivated breaks fall. These expectations are shown in (@breakpatMod2) and (@breakpatArg2) (originally shown in Chapter 1 and repeated here for convenience), where "$\|$" represents a linguistically motivated prosodic break.

\singlespacing

  (@breakpatMod2) **Mod pattern**
  \begingroup
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccc}
      \dots & stick & the letter & $\|$ & in the mailbox &  & of the proper stack \\
      & & \footnotesize Direct object & & \footnotesize PP1 & & \footnotesize vice president. \\
    \end{tabular}
  \endgroup

  (@breakpatArg2) **Arg pattern**
  \begingroup
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccc}
      \dots & stick & the letter & & in the mailbox & $\|$ & onto the proper stack. \\
      & & \footnotesize Direct object & & \footnotesize PP1 & & \footnotesize PP2 \\
    \end{tabular}
  \endgroup

\doublespacing

Consider again *Hypotheses 1* and *2* from Section \@ref(pred), repeated below.

  (@hypb2) *Hypothesis 1* (Prosodic Errors)\linebreak\nopagebreak
  A first reading of a sentence where PP2 is a goal argument (Arg) will be more likely to exhibit less natural prosody (e.g., more hesitation at and within the PP2 region) than either:
    a. Either reading of a sentence where PP2 is a modifier (Mod)
    b. A second reading of a sentence where PP2 is a goal argument (Arg)
    
While the present study did not specifically attempt to assess whether a given reading represents more or less natural prosody for these constructions, given that there is a difference between readings, it seems most likely that Reading 2 is the more natural of the two since it represents a considered reading, rather than a hurried one without as much preview. *Hypotheses 1* is fully supported only on the assumption that this is so. The robust effect of PP2 Status throughout all of the regression models of prosodic phrasing, however, suggests that at least the (a) portion of the both *Hypothesis 1* above and *Hypothesis 2* below are supported, if reading is disregarded. That Reading 2 is a significant predictor of both OBJ-dominance and PP1-dominance supports the (b) portions of these two hypotheses.
    
  (@hypd2) *Hypothesis 2* (Prosody/Syntax Mismatch)\linebreak\nopagebreak
  A first reading of a sentence where PP2 is a goal argument (Arg) will more often be produced with prosodic structure that represents an implausible or ungrammatical parse of the string (i.e., PP2 incorrectly attached as a modifier).
    a. Either reading of a sentence where PP2 is a modifier (Mod)
    b. A second reading of a sentence where PP2 is a goal argument (Arg)

*Hypothesis 2* is fairly well supported: the (a) portion by the significant effect of PP2 Status on OBJ-dominant and PP1-dominant pronunciations, and the (b) portion by the significance of Reading on the same. That said, the results do not represent a perfect correlation. Setting Reading 1 aside, Arg sentences (whether D or Q) were found to have the Arg-optimal pronunciation in almost 80% of recordings (PP1-dominant pronunciations), leaving some 22% of pronunciations as prosody/syntax mismatches (OBJ-dominant). For Mod sentences (whether Q or D), the picture is similar, but with a weaker contrast: prosodic coding identifies the Mod-optimal pronunciation in some 64% of recordings (OBJ-dominant). That leaves nearly one-third of pronunciations as prosody/syntax mismatches (PP1-dominant). Given the assumption that for Reading 2 a participant should already know the structure of the sentence and its interpretation, it is not entirely clear why some Reading 2 pronunciations still do not represent to the expected pattern. 

It could be that for some participants and some sentences the intended interpretation of the sentence is never grasped because the level of complexity was simply too high, or for some other reason. It might also be that while the participant arrived at the intended syntax, it was too difficult to devise or produce the prosodic phrasing that corresponds with that syntax. Finally, it is possible that the prosodic coding was imperfect, given the difficulty of evaluating prosodic phenomena, even for ToBI experts and other highly trained linguists. Most likely, all three of these possibilities are at play, and some portion of the cases of prosody/syntax mismatch can be attributed to each.

Returning to the predictions made in the first chapter, and indeed the intuition that motivated this line of research, it should be noted that *Hypothesis 3* (repeated here for convenience) as originally formalized is not supported by the facts.

  (@hypd) *Hypothesis 3*
  The inter-reading time (IRT) will be longer for Arg sentences that are declarative than for:
    a. Arg sentences that are interrogative
    b. Mod sentence that are interrogative or declarative

The prediction made by this hypothesis is essentially that IRT would show an effect of the interaction between PP2 Status and Speech Act, but no such significance was detected. It is possible that this is because IRT is not the proper measure for processing difficulty, but to make that claim one would have to explain why it nonetheless shows a robust effect of Arg PP2 Status (i.e., garden-path sentences exhibit longer IRTs than non-garden-path sentences). It seems more likely that IRT is a useful measure of processing difficulty, but that for some other reason, the intuition that the declarative instances of the Arg sentences are harder to process than the interrogative versions is not reflected in IRT.

### An ancillary planned analysis, the processing cost of interrogativity {#qslow}

```{r}
library(scales)
library(readr)

irt_fillers <- subset(read_csv("export/all_data.csv"), isFiller)
mean.irtByQ <- aggregate(
  irt_fillers$irt, 
  by=list("Q"=irt_fillers$Condition_Q),
  FUN=mean
)
mean.irtByPP <- aggregate(
  irt_fillers$irt, 
  by=list("+PP"=irt_fillers$Condition_GP),
  FUN=mean
)
mean.irtByQnPP <- aggregate(
  irt_fillers$irt, 
  by=list("Q"=irt_fillers$Condition_Q,"+PP"=irt_fillers$Condition_GP),
  FUN=mean
)
```

It is worth noting that the mean IRT for interrogative versions (6.7s) of the experimental sentences in the reported study was longer than for the declaratives (6.4s). While this finding was not statistically significant, @qp2 found that whole-sentence silent reading times for interrogatives were longer than for declaratives; and @mehler1963some provided a very early report of the processing cost of interrogativity: a so-called kernel sentence, i.e., a simple declarative, was easier to recall verbatim than were a number of sentences that he considered to be syntactic transformations of that kernel sentence (K): negative (N), polar question (Q), passive (P), and combinations thereof: NQ, NP, QP and NPQ. Mehler found that accurate recall was more frequent for K sentences (300/460, `r percent(300/460)`) than for the other sentences types, with interrogatives (210/460, `r percent(210/460)`) being recalled accurately at a lower rate than the two other individual transformations (234/460, `r percent(234/460)` for N; 243/460, `r percent(243/460)` for P).

To further investigate the effect that interrogativity appears to have on processing, the filler sentences in this study were designed in two versions, interrogative (Q) and declarative (D), so as to provide a diagnostic of the interrogative effect on IRT outside the construction of interest. A linear mixed effects regression model predicting IRT for filler items by Speech Act with random intercepts for participant and item found that IRT is increased by 0.4s for interrogatives (std. error = 0.2; p < .05); declaratives had a mean IRT of 6.2s, while interrogatives had a mean IRT of 6.6s. Half of the fillers had a sequence of two PPs at the end of the sentence to mirror the experimental items: a model predicting IRT by the presence of those PPs found minimal effect on IRT ($\beta$=0.01, std. error = 0.22, p = .96). This indicates that the presence of a string of PPs does not independently affect the apparent processing cost of interrogativity.

Interrogative status itself appears to increase the time needed for participants to feel they have satisfactorily studied a sentence in order to read it aloud correctly. This is consistent with the @mehler1963some and @qp2 findings that interrogatives are in some way more complicated or difficult than declaratives, and is an interesting finding, although not clearly related to the primary research question behind this study.

## Exploratory analyses    

This section presents two exploratory analyses that were conducted in an effort to explain some of the surprising findings just reported. 

One looks at protocol adherence, to investigate whether the lack of effect of Reading on prosodic boundary occurrence can be attributed to look-ahead. The results of that analysis suggest that look-ahead was most likely not at play. 

The second exploratory analysis looks at the identity of PP2-heads and finds that *of*-headed PP2s (which are categorized as Mod PP2 Status) corresponded to shorter IRTs than did *from*-headed PP2s (despite *from*-headed PP2s also being categorized as Mod).

### On Reading 1 delay {#r1del}

Reading 1 (R1) delay is the amount of time between the initial display of a sentence and the start of phonation. Participants' median R1 delay ranged from 0.6s to 1.6s with a standard deviation of 0.25s. The distribution of R1 delay was notably different than that of R2 delay, as shown in Figure \@ref(fig:delayComparison), which indicates that participants were adhering to the protocol at least most of the time.

```{r delayComparison,fig.pos="!h",fig.cap="Distributions of R1 delay and R2 delay"}
raw_rs_file <- read_csv("../csvs/merged.csv")
raw_rs <- subset(raw_rs_file,!isFiller & Leading < 15000)
raw_rs$reading <- raw_rs$Reading
raw_rs$Reading <- ifelse(raw_rs$reading == 1, "R1", "R2")
ggplot(raw_rs, aes(Leading/1000, fill = Reading)) +
  geom_histogram(binwidth = 0.5,position="dodge",color="black") +
  scale_fill_manual(values=c("black","white")) +
  ggtitle("Bin size = 0.5s") +
  xlab("R1/R2 Delay in seconds") +
  ylab("Frequency") 

ggsave("deldis.pdf",device=cairo_pdf())
# dev.off()
```

As a way of analyzing the protocol, and the extent to which participants performed as expected, participants were categorized based on their median R1 delay. In what follows, a *fast* median R1 delay was less than or equal to 0.9s, and a *slow* one was longer than 1.05s, resulting in 12 participants per category. Ten participants had R1 delays between those values and were set aside. The calculations for categorizing participants were done over Reading 1 of experimental items (n = 489). Note that while R1 delay category (i.e., *fast* or *slow*) is a property of R1 delay, data for both readings is nonetheless explored within these categories.

```{r facet, fig.cap="Plot of pattern proportions as a function of sentence type.",fig.height=3.5,fig.pos="H"}

hidel <- 1051
lodel <- 901

catdes <- sprintf("FAST median R1 delay < %0.2fs. SLOW median R1 delay > %0.2fs",lodel/1000,hidel/1000)

adata <- read_csv("../drafts/export/prosody_with_r1delcat.csv")
adata$SentenceType <- paste(ifelse(adata$Condition_Q,"Q","D"),ifelse(adata$Condition_GP,"Arg","Mod"))
adata$reading <- paste("Reading", adata$Reading)
pdata <- subset(adata, r1DelCat != "NORMAL")

ggplot(pdata,    
    aes(
      x=reorder(SentenceType,sto(SentenceType)),
      y=..count..,
      fill=reorder(
        simple2lvl,
        ifelse(simple2lvl=="PP1", 1, ifelse(simple2lvl=="OBJ",3,2))
      )
    )) +
  geom_bar(position="fill",color="black",width=0.5) +
  labs(fill="Pattern",x=" ",y=" ",caption="FAST n=12, SLOW n = 12") +
  facet_grid(rows = vars(reading),cols=vars(r1DelCat)) +
  scale_y_continuous(labels=scales::percent) + 
  scale_fill_brewer(breaks=c("PP1", "BOTH", "OBJ"), palette="Greys")

ggsave("facet.pdf",device=cairo_pdf())
# dev.off()
```

```{r rsplit}
adata$both <- adata$simple2lvl=="BOTH"

r1data<-subset(adata,Reading==1)

fast <- subset(r1data,r1DelCat == "FAST")
slow <- subset(r1data,r1DelCat == "SLOW")

fastTabN <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)  
slowTabN <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTab <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)
slowTab <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTabN<-fastTabN[c(1,sto(names(fastTabN)[2:5])+1)]
slowTabN<-slowTabN[c(1,sto(names(slowTabN)[2:5])+1)]
fastTab<-fastTab[c(1,sto(names(fastTab)[2:5])+1)]
slowTab<-slowTab[c(1,sto(names(slowTab)[2:5])+1)]
  
tab<-cbind(fastTab,slowTab[2:5])
ntab<-cbind(fastTabN,slowTabN[2:5])
tab[2:9]  <- mapply(percent,tab[,2:9])
ntab[2:9] <- format(ntab[2:9],digits=0)

tab <- rbind(tab,ntab)

colnames(tab)[1] <- "Pattern"
colnames(tab)[6:9] <- paste0(colnames(tab)[6:9],".slow")
tab <- arrange(tab,Pattern)
tab <- tab %>% select(Pattern,everything())

r1tab<-tab
r2data<-subset(adata,Reading==2)

fast <- subset(r2data,r1DelCat == "FAST")
slow <- subset(r2data,r1DelCat == "SLOW")

fastTabN <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)  
slowTabN <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTabN<-fastTabN[c(1,sto(names(fastTabN)[2:5])+1)]
slowTabN<-slowTabN[c(1,sto(names(slowTabN)[2:5])+1)]

fastTab <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)
slowTab <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTab<-fastTab[c(1,sto(names(fastTab)[2:5])+1)]
slowTab<-slowTab[c(1,sto(names(slowTab)[2:5])+1)]
  
tab<-cbind(fastTab,slowTab[2:5])
ntab<-cbind(fastTabN,slowTabN[2:5])
tab[2:9]  <- mapply(percent,tab[,2:9])
ntab[2:9] <- format(ntab[2:9],digits=0)

tab <- rbind(tab,ntab)

colnames(tab)[1] <- "Pattern"
colnames(tab)[6:9] <- paste0(colnames(tab)[6:9],".slow")
tab <- arrange(tab,Pattern)
tab <- tab %>% select(Pattern,everything())


tab <- rbind(r1tab,tab)
```
```{r r2split, include = F}
tab %>% kable(
  align="c",
  caption="Break pattern by sentence type and R1 delay category.",
  col.names=c(" ", rep(c("D Arg","D Mod", "Q Arg", "Q Mod"),2))
) %>% kable_styling() %>%
  column_spec(5,border_right = T) %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(" " = 1,"FAST (n=12)"=4,"SLOW (n=12)"=4)) %>%
  pack_rows(index = c("Reading 1" = 6, "Reading 2" = 6))
```
```{r r1catTesting}
xdata <- subset(adata,r1DelCat != "NORMAL")
r1df <- ftable(xtabs(~Participant + simple2lvl + r1DelCat, subset(xdata,Reading==1))) %>% as.data.frame
r2df <- ftable(xtabs(~Participant + simple2lvl + r1DelCat, subset(xdata,Reading==2))) %>% as.data.frame
dfs <- subset(r1df %>% spread(simple2lvl,Freq), BOTH + OBJ + PP1 > 0)
dfs <- cbind("r1"=dfs,"r2"=subset(r2df %>% spread(simple2lvl,Freq), BOTH + OBJ + PP1 > 0))
dfs$deltaBOTH <- dfs$r1.BOTH - dfs$r2.BOTH
dfs$deltaBothCat <- ifelse(dfs$deltaBOTH > 0, "Decrease",ifelse(dfs$deltaBOTH < 0,"Increase","No change"))
```

In cases where median R1 delay was short, readers were more likely to produce only one break (PP1 or OBJ) than if R1 delay was longer. A possible explanation is that for recordings in the *slow* category, readers were more prone to hesitation in general, or perhaps, contrary to instructions, were using both the delay time as well as extra time created via hesitation to look ahead. The significant effect of PP2 Status for the *slow* category in Reading 2 could be due to the reader not fully understanding the Arg sentences prior to Reading 2, thus increasing their likelihood of hesitation; i.e., where the fast category represents confident readers, the *slow* category represents less confident readers. As a result of the reader's increased difficulty in Reading 1 for the recordings in the *slow* category, the effect of processing difficulty is not limited to Reading 1 but in fact spills over into Reading 2. The *slow* category represents readers who are never able to fully comprehend the sentence, and thus hesitate more frequently.

Mixed effects logistic regression models found no significant effect of R1 Delay Category for PP1 break dominance or OBJ break dominance. So, ultimately, variation in R1 delay does not seem to be of concern.

### On PP2 heads {#pp2h}

As mentioned in the items description (section \@ref(exps)), half of the items used *of* for the head of PP2 in the Mod cases, while half used *from.* In the Arg condition, half used *into* and half used *onto* to head PP2. An analysis that attempts to predict IRT by the PP2 head (e.g., *into* vs. *from*) found that while *into* and *onto* do not behave differently from each other, *of* and *from* do. Starting from a maximally complex model that included the lexical identity of the matrix verb, the construction verb, the head of PP1, and the head of PP2, as well as Speech Act and PP2 Status, the model that best predicted IRT was one that is essentially the same as the reduced model just reported (i.e., with Speech Act and PP2 Status as predictors), except that it substitutes the lexical identity of the PP2 head for PP2 Status, with *into* and *onto* collapsed into one level used as the reference level (intercept). 

```{r pp2hd}
csv <- '
PP2 head model,Estimate,Std. Error,p
D \\em{into/onto} (Intercept),6.76,0.58,< .001
Q,0.32,0.21,.13
\\em{from},-0.08,0.27,.77
\\em{of},-0.84,0.27,< .01
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",  
  escape=F,
  caption=
    "Linear mixed effects regression model predicting IRT by Speech Act and PP2 head."
) %>% 
  kable_styling(latex_options = "hold_position")
```

Sentences where PP2 was headed by *of* typically had IRTs that were 0.84s faster than sentences where PP2 was headed by *into*/*onto*; when the PP2 head was *from*, IRT was only 0.08s faster than for *into*/*onto*, a difference that is not statistically significant (p > .50).

It is clear from the above that the sentences containing argument PP2s tested here (ones headed by *into*/*onto*) result in longer IRT measures than those containing Mod PP2s ($\beta$ = 0.46, p < .05), but Speech Act and the interaction between the two factors are not a significant predictors of IRT (see Section \@ref(irt)). It then must be said that IRT does not represent a behavioral reflex of the intuition that interrogativity makes difficult to process PP2-attachment ambiguities easier.

What might explain the difference in PP2 heads' effect on IRT? For one, the *of*-headed PPs are marked by *of* as the possessors of the noun in the immediately preceding PP1; e.g., *on the roof rack of the minivan* could be re-cast as *on the minivan’s roof rack*. In contrast, the *from*-headed PP2s are all locatives, and in that sense are similar to cases with *into/onto*.  It is just that the *from* phrases are not the kind of locatives that can function as the goal arguments demanded by the construction verbs used in the study.  That is, even if these were to be used in some circumstances as directional rather than purely locative prepositions, they would function as sources rather than goals, e.g., *he moved the cows from the pasture to the barn*.

There might also be a simpler (and less interesting) explanation: because *of* is only two characters long, whereas *from* and *into*/*onto* are all four characters, participants may have recognized a pattern. For instance, they may have deduced that a two-character PP2 head meant the sentence did not have the difficult properties of some of the sentences with four-character PP2 heads (most notably the *into*/*onto* cases), which would have eliminated some of the needed study time.

## Summary

PP2 Status is a significant predictor of prosodic boundary occurrence, prosodic boundary dominance, and inter-reading time (IRT). This is reassuring with regard to the design of the study, and supportive of the well established finding that syntax can influence prosody. It is also supportive of the claim made here, that IRT is a useful diagnostic of sentence complexity and processing difficulty.

Speech Act was not a significant predictor of prosodic phrasing, with the exception of PP1-dominance. That it was not a significant predictor of boundary occurrence or of OBJ-dominance is not supportive of the hypotheses about its apparent ameliorating effect on difficult to process sentences made prior to this study. That it was a significant predictor in one analysis leaves open the possibility that those hypotheses are not entirely false.

Speech Act was not a significant predictor of IRT for the experimental items, but did appear to be a reliable predictor of longer IRTs in the filler items (which lacked the garden-path properties of the Arg experimental items), relative to otherwise identical declarative sentences.

Ultimately, the apparent ameliorating effect of interrogativity on otherwise difficult to process declarative sentences with PP-attachment ambiguities does not appear to have a behavioral correlate that has been detected in this study. No significant interaction between Speech Act and PP2 Status was found for any of the analyses conducted.

\clearpage