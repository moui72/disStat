---
always_allow_html: yes
---

```{r results, echo=F, warning=F,message=F}
library(ggplot2)
library(ggthemes)
library(extrafont)
library(psych)
library(Cairo)
library(readr)

loadfonts("pdf")
knitr::opts_chunk$set(
  echo=FALSE, 
  warning=FALSE, 
  message=FALSE, 
  fig.height = 3, 
  fig.pos="H"
)

bgcolor <- "#ffffff"
par(bg = bgcolor)

kable <- function(...) {
  knitr::kable(booktab = TRUE,...)
}

### global plot themeing
theme_set(
  theme_tufte(base_size = 12) + 
    theme(
      plot.background = element_rect(fill = bgcolor, color = bgcolor),
      panel.border = element_blank(),
      text = element_text(color="black"),
      axis.text = element_text(color="black")
    )
)

# sentence type order
sto <- function (SentenceType) {
  ifelse(SentenceType %in% c("D Arg", "Q Arg", "D Mod", "Q Mod"),
    return(
      ifelse(
        SentenceType=="Q Arg",
        length(SentenceType),
        ifelse(
          SentenceType=="D Arg",
          length(SentenceType)-1,
          ifelse(
            SentenceType=="Q Mod",
            length(SentenceType)-2,
            length(SentenceType)-3
          )
        )
      )
    ),
    return(1)
  )
}

```
# Results and discussion {#res}

\setlength\parindent{24pt}\setlength{\parskip}{0.0pt plus 1.0pt}

This chapter reports various descriptions and analyses of the recordings obtained, and offers comments on the relevance of those findings to the research questions motivating this study. The reported analyses focus in the first instance on the impact of Speech Act (declarative/D vs. interrogative/Q) and PP2 Status (modifier/Mod vs. argument/Arg) on the location of prosodic phrase boundaries. The effects of those same factors are also examined for the time spent by a participant considering sentence meaning between first and second readings, i.e., the inter-reading time (IRT). 

An ancillary analysis, likewise made available by the materials design, probes the apparent processing costs inherent to interrogative vs. declarative contexts, among the filler sentences, for which temporary ambiguity is not at issue. 

Finally, to inform future research, two exploratory analyses are undertaken. In order to evaluate the extent to which participants adhered to the Double Reading protocol as intended, an analysis took advantage of participant variability in the time taken to reading any sentence. Was Reading 1 always a non-previewed pronunciation, as opposed to Reading 2's considered pronunciation? The prosodic patterns for participants with faster and slower Reading 1 (R1) delays are presented as a way of investigating the extent to which individual differences might impact prosodic phrasing, and as a further exploration of the success of the protocol instructions in producing the intended behavior. An analysis also explored the difference in IRT produced by sentences with a given PP2 head preposition; that analysis found a difference in behavior between the Mod PP2 heads *of* and *from*, but no difference between the Arg heads *into* and *onto*.

## Planned analyses

There are two types of planned analyses presented in this section. Two primary planned analyses: prosodic phrasing and IRT; one ancillary planned analysis: the effect of interrogativity on IRT. 

### Data for analysis {#data}

Data for 32 participants in total were analyzed. Given 4 versions of the experiment and 2 possible orderings there would ideally be 4 participants per version-order combination. Due to the exclusion of some participants discussed in Section \@ref(parti), the actual distribution was as shown in \@ref(tab:vtab).

```{r vtab, echo=F}
library(kableExtra)
library(readr)
library(dplyr)

mdata <- read_csv("export/prosody_data.csv")

mdata$PP2Status <- ifelse(grepl("N",mdata$REC),"Mod","Arg")
mdata$SpeechAct <- ifelse(grepl("Q",mdata$REC),"Q","D")

versiontab <- mdata %>% 
  filter(!duplicated(Participant)) %>%
  with(table("Version"=LIST,Order)) %>%
  addmargins() 

rownames(versiontab)[1:4]<-paste("Version", rownames(versiontab))
versiontab[1:4,1:2] %>% kable(
  caption="Number of participants per version-order combination.",
  col.names = c("1", "2"),
  align="c"
) %>% kable_styling() %>% 
  add_header_above(c(" "=1, "Order"=2)) %>%
  kable_styling(latex_options = c("hold_position"))
```

IRTs below 0.25s (n = 2) and above 25.0s (n = 5) were assumed to be implausible and omitted from the analyses reported below. Experimental data were then Winsorized by participant to bring data below the 2.5% and above the 97.5% threshold to the value at those thresholds, resulting in the distribution as shown in Figure \@ref(fig:wIRT) (n = 489). That figure makes evident the great variability and positive skew in observed IRT values (range 0.7 to 22.2s; 6.5s (sd = 3.8); median 6.1).

```{r wIRT,fig.cap="Distribution of IRT values for experimental sentences."}
irt_data <- read_csv("export/irt_data.csv")
ggplot(irt_data, aes(wirt/1000)) +
  geom_histogram(binwidth = 0.5,color="white",fill="#333333") + 
  labs(subtitle="Bin size = 0.5s",xlab="Inter-reading time (IRT) in seconds.",ylab="Frequency")
ggsave("wIRT.pdf",device=cairo_pdf)
# dev.off()
```

Some of the expected 3072 recordings (32 participants x 2 readings x 48 items, 16 experimental and 32 filler) could not be used due to intrusive background noise during the recording session. Additionally, data were also excluded from analysis if either of a Reading 1/Reading 2 pair was missing; 9 such incomplete pairs were excluded. Without analyzable data from both members of a pair, it is difficult to determine the extent to which the elicitation protocol was executed as intended (i.e., the extent of preview for Reading 1 vs. Reading 2). The number of recordings per sentence type included in the analyses that follow were distributed as shown in Table \@ref(tab:rvtab); if no data had been omitted, the expected count per sentence type was 256.

```{r rvtab, echo=F}
library(tidyr)
library(dplyr)
library(kableExtra)
library(readr)
mdata<-read_csv("export/prosody_data.csv")
tot<-as.data.frame(xtabs( ~ PP2Status + SpeechAct, data=mdata))
gtot <- tot

totDisp <- gtot %>% as.data.frame() %>%
  spread(SpeechAct,Freq) %>% arrange(desc(PP2Status))


kable(
  totDisp,
  caption="Number of recordings analyzed, as a function of Speech Act and PP2 Status.",
  align="c",
  col.names=c(" ","D","Q")
) %>% kable_styling(latex_options = c("hold_position"))
```

For experimental items, 978 recordings were subjected to prosodic analysis, constituting 95.6% of the utterances elicited. Because IRT is a measure arising from pairs of utterances (Reading 1 and Reading 2) rather than from single recordings, the database for response timing took in 489 data points.

### Prosodic boundary patterns {#results-prosody}

This section reports the prosodic phrasings judged to be present in the recordings of experimental items, and the extent to which those patterns are influenced by the design parameters of the study (Speech Act and PP2 Status), as well as which reading (Reading 1 or Reading 2) a given recording represents. These data are reported first descriptively (i.e., in terms of occurrence frequency), and then using regression models to calculate the statistical significance of whatever patterns emerge from the data. Finally, a summary of findings and their implications for the hypothesis motivating this study is provided.

```{r byGPr2}
library(scales)
### r2
r2data <- subset(mdata,Reading==2)

pp1tab<-xtabs(PP1 ~ PP2Status + SpeechAct, data=r2data)
objtab<-xtabs(OBJ ~  PP2Status + SpeechAct, data=r2data)
tot<-as.data.frame(xtabs( ~  PP2Status + SpeechAct, data=r2data))


pp1<-as.data.frame(pp1tab)
pp1$tot<-tot$Freq
pp1$pct <- pp1$Freq/pp1$tot
pp1$pretty <- sprintf("%s (%d)",percent(pp1$pct),pp1$Freq)
pp1disp <- pp1 %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

obj<-as.data.frame(objtab)
obj$tot<-tot$Freq
obj$pct <- obj$Freq/obj$tot
obj$pretty <- sprintf("%s (%d)",percent(obj$pct),obj$Freq)
objdisp <- obj %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

pp1obj <- cbind(pp1disp,objdisp[2:3])

r2pp1<-pp1disp
r2obj<-objdisp


### r1
r1data <- subset(mdata,Reading==1)

pp1tab<-xtabs(PP1 ~ PP2Status + SpeechAct, data=r1data)
objtab<-xtabs(OBJ ~  PP2Status + SpeechAct, data=r1data)
tot<-as.data.frame(xtabs( ~  PP2Status + SpeechAct, data=r1data))



pp1<-as.data.frame(pp1tab)
pp1$tot<-tot$Freq
pp1$pct <- pp1$Freq/pp1$tot
pp1$pretty <- sprintf("%s (%d)",percent(pp1$pct),pp1$Freq)
pp1disp <- pp1 %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

obj<-as.data.frame(objtab)
obj$tot<-tot$Freq
obj$pct <- obj$Freq/obj$tot
obj$pretty <- sprintf("%s (%d)",percent(obj$pct),obj$Freq)
objdisp <- obj %>%
  subset(select=c(
    "PP2Status",
    "SpeechAct",
    "pretty"
  )) %>% 
  spread(SpeechAct,pretty)

pp1objr1 <- cbind(pp1disp,objdisp[2:3])

r1pp1<-pp1disp
r1obj<-objdisp

pp1<-cbind(r1pp1,r2pp1[2:3])


obj<-cbind(r1obj,r2obj[2:3])

```

In what follows, the distribution of OBJ boundaries and PP1 boundaries are first reported, separately for the four sentence types created by the materials design (D/Q x Mod/Arg), for each of Reading 1 and Reading 2. Then, the patterns of boundaries over the two positions are considered, before moving to statistical analysis. 

Note that while judgments about the potential boundary after the infinitival construction verb were collected, reports of these were sufficiently uncommon (occurring in only 8% of recordings) to warrant their treatment as noise, and so were set aside. The break locations are indicated with a % symbol in (@breakpos), which was previously provided in Section \@ref(sita) as (@bpos) and is repeated here for convenience.

  (@breakpos)
  \begingroup
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccccccc}
      & & \footnotesize V Break & & \footnotesize OBJ Break & & \footnotesize PP1 Break & \\
      She had & wanted to set & \% & the textbooks & \% & on the top shelf & \% & into the file box. \\
      \cmidrule(r){2-2} \cmidrule(r){4-4} \cmidrule(r){6-6} \cmidrule(r){8-8} 
      & \footnotesize V Region & & \footnotesize OBJ Region & & \footnotesize PP1 Region & & PP2 Region \\
    \end{tabular}
  \endgroup
  
Examination of prosodic phrasings begins with counts of the rate at which prosodic breaks were judged to be present after each of the OBJ and PP1 sentence regions: Table \@ref(tab:obj) below summarizes these data, for the OBJ breaks:

```{r obj}
names(obj)[2:5] <- c(2:5)
obj <-obj %>%  arrange(desc(PP2Status))
kable(
  obj,
  caption="Percent occurrence of OBJ boundary (frequency of occurrence in parenthesis) as a function of sentence type and Reading.",
  col.names = c("", "D", "Q","D", "Q"),
  align="c"
) %>% 
  add_header_above(c(" "=1,"Reading 1"=2,"Reading 2"=2)) %>% 
  kable_styling(latex_options = c("hold_position"))
```

A boundary following the OBJ region was more likely overall for Mod sentences (77.8% occurrence rate) than for Arg (65.2%), though that contrast was more sharply drawn for Reading 1 of Arg sentences (57.2%) than for Reading 2 (73.6%).

Data of the same kind, now for the rates at which prosodic breaks were judged to be present after the PP1 sentence regions, are summarized in Table \@ref(tab:pp1) below.  Those data allow the following generalization to be drawn: Reversing the pattern for OBJ, a boundary after PP1 was judged to be present substantially less often for Mod sentences (68.0% occurrence rate) than for Arg; for the latter type, the PP1 break was present in virtually all utterances (98.7%).  Moreover, for breaks at this position, there appeared to be no influence whatsoever of other factors in the design (Speech Act, Reading).  

```{r pp1}
names(pp1)[2:5] <- c(2:5)
pp1 <-pp1 %>%  arrange(desc(PP2Status))
kable(
  pp1,
  caption="Percent occurrence of PP1 boundary (frequency of occurrence in parenthesis) as a function of sentence type and Reading.",
  col.names = c("", "D", "Q","D", "Q"),
  align="c"
) %>% 
  add_header_above(c(" "=1,"Reading 1"=2,"Reading 2"=2)) %>% 
  kable_styling(latex_options = c("hold_position"))
```

Looking at both OBJ and PP1 breaks, taken together, it is clear that the total number of breaks judged present for any sentence type within a given reading substantially exceeds the number of recorded utterances.  For example, for declarative sentences with PP2 as modifier (D/Mod) produced as Reading 1, 179 construction-internal prosodic breaks occurred (95 OBJ + 84 PP1), cf. 123 recordings; thus, 45.5% of those utterances carried both breaks.  Considering all four sentences types produced as Readings 1 and Reading 2, two-break prosodic phrasing patterns were judged to have occurred in 43.0% to 72.1% of recordings.  Appendix \@ref(proApp) provides two tables and an accompanying figure spelling out the distribution of OBJ-only, PP1-only, and OBJ+PP1 break patterns.  Five cases where no phrasing break was judged present were omitted in subsequent reports of prosodic patterns.

For the questions pursued in the current study, the analysis that is most germane is one considering break dominance rather than simple break occurrence. Tables \@ref(tab:r1dombreaks), \@ref(tab:r2dombreaks), and Figure \@ref(fig:bdom) below therefore incorporate the informant’s judgments about the relative strength of the PP1 and OBJ prosodic phrasing boundaries. PP1-dominance means that the PP1 boundary was reported to be stronger than the OBJ break boundary (or that only the PP1 boundary occurred); OBJ-dominance means the reverse; and “Equal strength” means that neither boundary was reported to be stronger than the other. 

```{r bdomprep}
library(ggplot2)
library(ggthemes)
library(extrafont)
adata <- subset(mdata,simple2lvl!="NEITHER")
adata$reading <- paste("Reading", adata$Reading)
adata$dom <- ifelse(adata$pdom,"PP1",ifelse(adata$odom,"OBJ","Equal strength"))

r2pdata<-droplevels(subset(adata,Reading==2 & simple2lvl != "NEITHER"))
qdata <- subset(r2pdata,PP2Status=="Arg")
ddata <- subset(r2pdata,PP2Status=="Mod")

qfreq<-table(qdata$dom,qdata$SpeechAct) 
qpct<-qfreq %>%
  prop.table(margin=2) %>% apply(2,percent)

qpros<-paste0(qpct, " (",qfreq,")")
qpros<-cbind(qpros[1:3],qpros[4:6])

dfreq<-table(ddata$dom,ddata$SpeechAct) 
dpct<-dfreq %>%
  prop.table(margin=2) %>% apply(2,percent)

dpros<-paste0(dpct, " (",dfreq,")")
dpros<-cbind(dpros[1:3],dpros[4:6])


r2combo<-as.data.frame(cbind(dpros,qpros))



r1pdata<-droplevels(subset(adata,Reading==1 & simple2lvl != "NEITHER"))
qdata <- subset(r1pdata,PP2Status=="Arg")
ddata <- subset(r1pdata,PP2Status=="Mod")

qfreq<-table(qdata$dom,qdata$SpeechAct) 
qpct<-qfreq %>%
  prop.table(margin=2) %>% apply(2,percent)

qpros<-paste0(qpct, " (",qfreq,")")
qpros<-cbind(qpros[1:3],qpros[4:6])

dfreq<-table(ddata$dom,ddata$SpeechAct) 
dpct<-dfreq %>%
  prop.table(margin=2) %>% apply(2,percent)

dpros<-paste0(dpct, " (",dfreq,")")
dpros<-cbind(dpros[1:3],dpros[4:6])


r1combo<-as.data.frame(cbind(dpros,qpros))


bothrbothb <- cbind(r1combo,r2combo)
bothrbothb <- bothrbothb %>% as_tibble(.name_repair = "unique")
row.names(bothrbothb) <- c("2Both","1OBJ","3PP1")
row.names(r1combo) <- c("2Both","1OBJ","3PP1")
row.names(r2combo) <- c("2Both","1OBJ","3PP1")
r1combo<-arrange(r1combo,row.names(r1combo))
r2combo<-arrange(r2combo,row.names(r2combo))
bothrbothb<-arrange(bothrbothb,row.names(bothrbothb))
row.names(r1combo) <- c("OBJ-dominant","Equal strength","PP1-dominant")
row.names(r2combo) <- c("OBJ-dominant","Equal strength","PP1-dominant")

```
```{r r1dombreaks}
kable(
  r1combo,
  caption="Percent occurrence of boundary dominance as a function of sentence type for Reading 1.",
  align="c",
  col.names=rep(c("D","Q"),2)
) %>%
  add_header_above(c(" " =1, "Mod"=2,"Arg"=2)) %>%
  column_spec(1,bold=T)%>% kable_styling(latex_options = "hold_position")

```
```{r r2dombreaks}
kable(
  r2combo,
  caption="Percent occurrence of boundary dominance as a function of sentence type for Reading 2.",
  align="c",
  col.names=rep(c("D","Q"),2)
) %>%
  add_header_above(c(" " =1, "Mod"=2,"Arg"=2)) %>%
  column_spec(1,bold=T)%>% kable_styling(latex_options = "hold_position")

```

```{r bdom, fig.cap="Percent break dominance occurrence as a function of sentence type and Reading.",fig.pos="!H", fig.height=3.5}
library(ggplot2)
library(ggthemes)
library(extrafont)
adata <- subset(mdata,simple2lvl!="NEITHER")
adata$reading <- paste("Reading", adata$Reading)
adata$dom <- ifelse(adata$pdom,"PP1",ifelse(adata$odom,"OBJ","Equal strength"))

ggplot(adata,    
    aes(
      x=reorder(SentenceType, sto(SentenceType)),
      y=..count..,
      fill=reorder(
        dom,
        ifelse(simple2lvl=="OBJ", 1, ifelse(simple2lvl=="PP1",3,2))
      )
    )) +
  geom_bar(position="fill",color="black",width=0.5) +
  labs(fill="Dominant break",x="Sentence Type",y="Percent where dominant") +
  facet_grid(cols = vars(reading)) +
  scale_y_continuous(labels=scales::percent) + 
  scale_fill_brewer(breaks=c("PP1", "Equal strength", "OBJ"), palette="Greys") 

ggsave("bdom.pdf",device=cairo_pdf)
# dev.off()
```

The data here clearly show a robust effect of PP2 Status (Mod or Arg) on break dominance, and little to no impact of Reading or Speech Act (D or Q). A dominant break after PP1 is frequent for all Arg sentences, both declaratives and questions, and in both Reading 1 and Reading 2. It is less frequent for Mod sentences in both Reading 1 and Reading 2. The higher relative frequency of PP1 break dominance in Arg sentences compared to Mod sentences is expected, since it represents a syntactically motivated prosodic break in the Arg cases. For the Mod sentences, there is no syntactic motivation for the PP1 break, so it makes sense for it to be less frequent and less dominant in Mod sentences. 

It is both noteworthy and puzzling that the patterns do not differ greatly across the two readings, as one would expect difficulty getting the prosody right on the first try for the difficult sentences being tested in the current study. It may be that the disambiguating preposition, *into* came into view while the participant was still pronouncing an earlier portion of the sentence (due to the Eye-Voice Span, see Section \@ref(look-ahead)). Since the disambiguator is a preposition and thus a function word, it can be processed very quickly, and so it's possible that even very limited look-ahead allowed some readers to come to the correct parse on their first reading.

A number of mixed effects logistic regression models support the general observations above. Models predicting PP1 break, OBJ break, OBJ break dominance and PP1 break dominance are reported. All models include random intercepts for participant and item, but due to convergence errors, no random slopes for any predictors are included. 

The intercept always represents the Mod sentence type, which is not expected to present any particular difficulty to the reader, since sentences with PP2 taking the role of a modifier are compatible with what is assumed to be the running parse when PP2 is encountered (i.e., PP1 has been interpreted as the goal argument of the verb, and PP2 does not disrupt that interpretation). For those models where Speech Act is included in the model, the intercept represents the declarative sentence type. In this way, the more complex sentence types are compared to the simplest one represented in the model. If Reading is included in a model, the intercept represents Reading 1.

For each analysis, a reduced model and the full model (i.e., the model containing all predictors of interest), when it converged, are reported. In each case, the reported reduced model is the simplest nested model[^nested] that did not represent a statistically significant worse fit according to likelihood ratio tests (LRTs) than the next most complex model. This model selection process was used for each of the four analyses that follow (OBJ and PP1 boundary occurrence, OBJ and PP1 break dominance). All reported models represent statistically significant improvement over a minimal model, which was a model with only an intercept and random effects (no fixed effects). That comparison is reported for each selected reduced model, as well as the Akaike Information Criterion (AIC[^aic]) value for the selected model and the minimal model. All regression models were run using the lme4 R package (@R-lme4).

[^nested]: A simpler model nested within a more complex model is one that includes a subset of the more complex model's predictors, e.g., OBJ Dominance as a function of PP2 Status is nested within OBJ Dominance as a function of Speech Act $\times$ PP2 Status.

[^aic]: AIC is a representation of the amount of information lost by using a regression model to estimate data points. It is a measure that balances both the goodness of fit of a model and the simplicity of a model, guarding against over fitting and under fitting the data involved.

In the full model predicting OBJ break occurrence, shown in Table \@ref(tab:fullobjmod), only the estimate for D Mod Reading 1 (the intercept) and the effect of PP2 Status show statistical significance.

```{r fullobjmod}
csv <- '
Outcome: OBJ break (Full),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",0.93,0.59,.11
Q,                             0.83,0.74,.26
Arg,                          -1.4,0.71,< .05
Reading 2,                     0.56,0.35,.11
Q:Arg,                        -1.03,1.00,.30
Q:Reading2,                   -0.81,0.48,.09
Arg:Reading2,                  0.29,0.46,.53
Q:Arg:Reading2,                0.97,0.65,.13
'
tab <- read_csv(csv,na=character())
tab %>% kable(
  align="rccc", 
  caption="Mixed effects logistic regression model predicting OBJ break occurrence (Full)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")

```

Table \@ref(tab:objMod) shows a reduced model, predicting the occurrence of an OBJ break with estimates for the coefficients of the fixed effects of Reading 2, PP2 and the interaction between Reading and PP2 Status. The removal of Speech Act as a predictor revealed the Reading $\times$ PP2 Status interaction to become a significant predictor. The model that included PP2 Status and its interaction with Reading provided a statistically significantly better fit than a minimal model (AIC~Minimal~=1068.0, AIC~Reduced~=1031.6, $\chi^2$(3)=42.34, p < .001).

```{r objMod}
csv <- 'Outcome: OBJ break (Reduced),Estimate,Std. Error,p
"Mod, Reading 1 (Intercept)",1.39,0.45,< .01
Reading 2,0.11,0.23,.62
Arg,-1.98,0.5,< .001
Reading2:Arg,0.81,0.32,< .05'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc", 
  caption="Mixed effects logistic regression model predicting OBJ break occurrence (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")

```

The log odds[^logodds] of an OBJ break for Mod Reading 1 is 1.39 (std. error = 0.45, p < .01). The log odds of that break increased modestly in Reading 2 but the increase was not statistically significant. PP2 arguments reduced the log odds of an OBJ break compared to PP2 modifiers, robustly, but less so in Reading 2 than in Reading 1. As noted earlier, the OBJ break is expected to occur more often in Mod cases, because that break marks PP1 attachment as an argument of the verb.

[^logodds]:  Log odds is, in this case, the natural log of the odds ratio, so the log odds of A is log~e~(P(A)/P(¬A)). A log odds of 1.39 translates to an odds ratio of 4.01:1 (e^1.39^=4.01) and a probability of 80% (4.01/(1+4.01)=0.80).

The full model for predicting PP1 boundary occurrence showed significance only for the intercept and the effect of PP2 Status. 

```{r fullpp1Mod}
csv <- '
Outcome: PP1 break (Full),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",0.68,0.07,< .001
Q,0.01,0.09,.89
Arg,0.31,0.09,< .001
Reading 2,0.00,0.04,.99
Q:Arg,0.00,0.13,0.97
Q:Reading2,-0.01,0.06,.82
Arg:Reading2,0.00,0.06,.99
Q:Arg:Reading2,0.00,0.08,.97
'

tab <- read_csv(csv,na=character())

tab %>% kable(
  align="rrrl",
  caption="Mixed effects logistic regression model predicting PP1 break occurrence (Full)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

The model selection process described above found that the the best model was the one with PP2 Status as the only fixed effect, which was found to be better than a minimal model by LRT (AIC~Minimal~=855.59, AIC~Reduced~=629.60, $\chi^2$(1)=228, p < .001).

```{r pp1Mod}
csv <- '
Outcome: PP1 break (Reduced),Estimate,Std. Error,p
"Mod (Intercept)",0.96,0.3,< .01
Arg,4.12,0.44,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting PP1 break occurrence (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

Sentences with argument PP2s had greatly increased log odds of a PP1 break compared to those with modifier PP2s. This is again expected, because the PP1 break is responsive to the syntactic discontinuity required for the argument attachment of PP2. That Speech Act is not a relevant predictor is evidence against a prosodic explanation of the motivating intuition for this study; we would expect both a main effect of Speech Act and definitely an interaction between Speech Act and PP2 Status, if the prosody were more (or less) different across the PP2 Status factor for interrogatives than for declaratives.

Models were also run for predicting break dominance. The full model predicting OBJ break dominance is shown in Table \@ref(tab:fodom).

```{r fodom}
csv <- '
Outcome: OBJ-dominance (Full),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",0.50,0.09,< .001
Q,0.03,0.12,0.82
Arg,-0.45,0.12,< .001
Reading 2,0.07,0.05,.22
Q:Arg,-0.03,0.17,.86
Q:Reading2,-0.03,0.07,.68
Arg:Reading2,0.03,0.07,.71
Q:Arg:Reading2,0.00,0.11,.97
'

tab <- read_csv(csv,na=character())

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting OBJ break dominance (Full)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

Table \@ref(tab:odom) reports the best model for predicting OBJ break dominance. The best model was one with fixed effects for reading and PP2 Status. A likelihood ratio test (LRT) found that the selected model represented a statistically significant improvement over a minimal model (AIC~Minimal~=1270.5, AIC~Reduced~=1079.3, $\chi^2$(2)=219.63, p < .001). There was no statistically significant effect of Speech Act on OBJ break dominance.

```{r odom}
csv <- '
Outcome: OBJ-dominance (Reduced),Estimate,Std. Error,p
"Mod, Reading 1 (Intercept)",-0.16,0.32,.62
Reading 2,0.4,0.16,< .05
Arg,-2.32,0.18,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting OBJ break dominance (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

PP1 break dominance and OBJ break dominance are not entirely complementary, because it is possible for the two breaks to have equal prominence (as happens, say, when prosodic
phrasing marks out parenthetic content). Thus, models predicting PP1 dominance were also explored. 

The full model predicting PP1 break dominance failed to converge, so only the reduced model is reported. Table \@ref(tab:pdom) reports the best model for predicting PP1 break dominance. Unlike the model for predicting OBJ break dominance, the best model for predicting PP1 break dominance includes Speech Act as a predictor, as well as Reading, and PP2 Status.

```{r pdom}
csv <- '
Outcome: PP1-dominance (Reduced),Estimate,Std. Error,p
"D Mod, Reading 1 (Intercept)",-0.19,0.33,.57
Reading 2,-0.38,0.15,< .05
Q,0.31,0.15,< .05
Arg,2.2,0.17,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",
  caption="Mixed effects logistic regression model predicting PP1 break dominance (Reduced)."
) %>% 
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

This model was better than a minimal model according to LRT (AIC~Minimal~=1290.4, AIC~Reduced~=1078.8, $\chi^2$(3)=217.59, p < .001). PP1 break dominance was much more likely for sentences with argument PP2s than sentences with modifier PP2s, with interrogatives having slightly increased log odds of PP1 break dominance. Log odds of PP1 break dominance were slightly less in Reading 2 than Reading 1. There were no significant interaction terms.

Because reading was a significant predictor for 3 of the 4 models reported, and there are reasons to believe that Reading 2 is more representative of the natural or intended prosody of the sentences at issue, models were also run predicting OBJ-dominance and PP1-dominance for Reading 2 data only. In both cases, the best model had the same structure: fixed effects of Speech Act and PP2 Status, with no interaction term.

```{r r2dom}
csv <- '
,Estimate,Std. Err,p,Estimate,Std. Err,p
D Mod (Intercept),0.66,0.24,< .01,-0.97,0.27,< .001
Q,-0.3,0.22,.16,0.35,0.22,.10
Arg,-2.07,0.24,< .001,2.15,0.24,< .001
'

tab <- read_csv(csv)

tab %>% kable(
  col.names=c("(Reading 2 only)","Estimate","Std. Err","p","Estimate","Std. Err","p"),
  align="rccc",  caption="Mixed effects logistic regression models predicting break dominance in Reading 2 (Reduced)."
) %>% 
  add_header_above(c(" "=1, "Outcome: OBJ Dominance" = 3, "Outcome: PP1 Dominance" = 3)) %>%
  row_spec(0,align="c") %>%
  kable_styling(latex_options = "hold_position")
```

For both OBJ-dominance and PP1-dominance, the main effect of Speech Act failed to reach significance, but its inclusion marginally improves the fit of each model. Even when limited to only Reading 2 data, Speech Act does not interact with PP2 Status, which is again supportive of a non-prosodic explanation for the motivating intuition. That there is a robust effect of PP2 Status is reassuring evidence that prosody is indeed sensitive to syntax, and that the study's material design has driven both the intended initial parse and re-analysis for the Arg sentence-types, where PP2 takes the role of goal argument to the construction verb.

### Inter-reading time {#irt}

Inter-reading time[^irtp] is the time elapsing between the completion of Reading 1 and before the start Reading 2 phonation. The details of how this was measured and defined can be found in Section \@ref(method-irt). Possibly, IRT can provide an estimate of how much difficulty the reader has in processing a given sentence. If a reader spends more time studying a sentence prior to reading it aloud a second time, the IRT is longer; it is thus a possible indicator of processing difficulty.

[^irtp]: Since IRT is a measure across pairs of recordings (Reading 1/Reading 2), the number of data analyzed in this section are half as many as those in the prosody analyses.

```{r}
irt_props <- describe(irt_data$irt)
wirt_props <- describe(irt_data$win_irt)
### sd by condition table
sdByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=sd
)

### means by condition table
meansByConditionLong <- aggregate(
  irt_data$win_irt,
  by=list(
    "Q"=irt_data$Q,
    "GP"=irt_data$GP
  ),
  FUN=mean
)
names(meansByConditionLong)[3] <- "mean"
### concise means by condition table
meansByCondition <- meansByConditionLong %>% spread(Q,mean) 
names(meansByCondition)[1] <- "Condition"
### add sd and se to meansByConditionLong
meansByConditionLong$sd<-sdByConditionLong$x
meansByConditionLong$se<-sdByConditionLong$x/sqrt(irt_props$n)


### difference in condition means
diffs<-list(Condition="Increase", `D`=diff(meansByCondition$`D`), `Q`= diff(meansByCondition$`Q`))
meansByCondition<-rbind(meansByCondition, diffs)
cdiff <- meansByCondition$`D`[3]-meansByCondition$`Q`[3]

meansByConditionS <- meansByCondition
meansByConditionS$`D` <- meansByConditionS$`D`/1000
meansByConditionS$`Q` <- meansByConditionS$`Q`/1000

meansByConditionS$Condition <- c("Mod", "Arg", "Mod - Arg")
meansByConditionS$`Q - D` <- meansByConditionS$Q - meansByConditionS$D
```

```{r interactionplot,fig.cap="Mean IRT as a function of sentence type.",fig.height=3.5,fig.pos="H"}

meansByConditionLong$Qn <- ifelse(
  meansByConditionLong$Q == "Q", 
  "Interrogative", 
  "Declarative"
)

meansByConditionLong$GPn <- ifelse(
  meansByConditionLong$GP == "+GP", 
  "Arg", 
  "Mod"
)

ggplot(
  meansByConditionLong, 
  aes(mean/1000,x=reorder(GPn, desc(GPn)),y=mean/1000,group=Qn,linetype=Qn)
) + 
  geom_line() +
  geom_point() +
  ylim(5.75,7.25) +
  geom_errorbar(aes(ymin=(mean-se)/1000, ymax=(mean+se)/1000), width=.125) +
  labs(
    x="", 
    y="Mean IRT (ms)",
    linetype="",
    subtitle = "Error bars represent one standard error"
  )
ggsave("interact.pdf",device=cairo_pdf())
# dev.off()
```

Figure \@ref(fig:interactionplot) graphs mean IRT values as a function of sentence type. The two slopes diverge minimally. PP2 Status has a statistically significant effect on IRT: sentences with argument PP2s having substantially longer IRTs (6.73s) on average than those with modifier PP2s (6.34s). The effect of Speech Act was not statistically significant, though interrogatives attracted longer (6.66s) IRTs than declaratives (6.40s), and including Speech Act as a predictor of IRT improved the predictive power of the model. This is discussed in more detail below.

Regression models support the observations above. All models discussed include random intercepts for participant and item. Models with random slopes did not converge, and so random slopes were not included.
	
The full model for predicting IRT included fixed effects of Speech Act and PP2 Status and their interaction. This model is shown in Table \@ref(tab:hyp).


```{r hyp}
csv <- '
IRT Full Model,Estimate,Std. Error,p
D Mod (Intercept),6.32,0.59,< .001
Q,0.29,0.3,.34
Arg,0.42,0.3,.17
Q:Arg,0.08,0.43,.85
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",  
  caption=
    "Linear mixed effects regression model predicting IRT by sentence type with interaction term (Full)."
) %>% 
  kable_styling(latex_options = "hold_position")
```

Of the nested models (those with subsets of the full model), the best model according to likelihood ratio testing (LRT) was the one with only the fixed effect of PP2 Status, shown in Table \@ref(tab:redirt). The improvement of fit of the reduced model over a minimal model with only an intercept and random effects was statistically significant (AIC~Reduced~=2346.2, AIC~Minimal~=2348.7, $\chi^2$(1)=4.51, p < .05), indicating a robust main effect of PP2 Status on IRT.

```{r redirt}
csv <- '
IRT Reduced Model,Estimate,Std. Error,p
Mod (Intercept),6.47,0.58,< .001
Arg,0.46,0.21,< .05
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",  
  caption=
    "Linear mixed effects regression model predicting IRT by sentence type (Reduced)."
) %>% 
  kable_styling(latex_options = "hold_position")
```

These results are not supportive of *Hypothesis 3* formalized in Section \@ref(pred), as statistical significance was expected for the interaction of Speech Act with PP2 Status under the assumption that interrogative context ameliorates processing difficulty for PP-attachment garden paths, vs. declarative context for the same construction.

### Discussion of planned analyses

Throughout the analysis of prosodic phrasing patterns, PP2 Status was the most robust predictor of OBJ and PP1 boundary occurrence and their relative strengths. The PP1 break was more frequent and more frequently dominant for sentences with a PP2 that was an argument than those with a PP2 that could be a modifier; conversely, the OBJ break was more frequent and more frequently dominant for sentences with a PP2 that was interpretable as a modifier than those with a PP2 that was an argument.

Returning to the predictions discussed in Section \@ref(pred), recall the contrast made there between syntactically motivated prosodic breaks and other breaks or pauses. The Arg and Mod sentences differ with regard to where such breaks are expected to fall. These expectations are shown in (@breakpatMod2) and (@breakpatArg2) (originally formulated in Chapter 1 and repeated here for convenience), where "$\|$" represents a syntactically motivated prosodic break.

\singlespacing

  (@breakpatMod2) **Mod pattern**
  \begingroup
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccc}
      \dots & stick & the letter & $\|$ & in the mailbox &  & of the proper stack \\
      & & \footnotesize Direct object & & \footnotesize PP1 & & \footnotesize vice president. \\
    \end{tabular}
  \endgroup

  (@breakpatArg2) **Arg pattern**
  \begingroup
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccccccc}
      \dots & stick & the letter & & in the mailbox & $\|$ & onto the proper stack. \\
      & & \footnotesize Direct object & & \footnotesize PP1 & & \footnotesize PP2 \\
    \end{tabular}
  \endgroup

\doublespacing

Consider again *Hypotheses 1* and *2* from Section \@ref(pred), repeated below.

  (@hypb2) *Hypothesis 1* (Prosodic Errors)\linebreak\nopagebreak
  A first reading of a sentence where PP2 is a goal argument (Arg) will be more likely to exhibit less natural prosody (e.g., more hesitation at and within the PP2 region) than either:
    a. A first reading of a sentence where PP2 is a modifier (Mod)
    b. A second reading of a sentence where PP2 is a goal argument (Arg)
    
While the present study did not specifically attempt to assess whether a given reading represents more or less natural prosody for these constructions, given that there is a difference between readings, it seems most likely that Reading 2 is the more natural of the two since it represents a considered reading, rather than a hurried one without as much preview. *Hypotheses 1* is fully supported only on the assumption that this is so. The robust effect of PP2 Status throughout all of the regression models of prosodic phrasing, however, suggests that at least the (a) portion of the both *Hypothesis 1* above and *Hypothesis 2* below are supported, if reading is disregarded. That Reading 2 is a significant predictor of both OBJ-dominance and PP1-dominance supports the (b) portions of these two hypotheses.
    
  (@hypd2) *Hypothesis 2* (Prosody/Syntax Mismatch)\linebreak\nopagebreak
  A first reading of a sentence where PP2 is a goal argument (Arg) will more often be produced with prosodic structure that represents an implausible or ungrammatical parse of the string (i.e., PP2 incorrectly attached as a modifier) than either:
    a. A first reading of a sentence where PP2 is a modifier (Mod)
    b. A second reading of a sentence where PP2 is a goal argument (Arg)

*Hypothesis 2* is fairly well supported: the (a) portion by the significant effect of PP2 Status on OBJ-dominant and PP1-dominant pronunciations, and the (b) portion by the significance of Reading on the same. That said, the results do not represent a perfect correlation. Setting Reading 1 aside, Arg sentences (whether D or Q) were found to have the Arg-optimal pronunciation in almost 80% of recordings (PP1-dominant pronunciations), leaving some 22% of pronunciations as prosody/syntax mismatches (OBJ-dominant). For Mod sentences (whether Q or D), the picture is similar, but with a weaker contrast: prosodic coding identifies the Mod-optimal pronunciation in some 64% of recordings (OBJ-dominant). That leaves nearly one-third of pronunciations as prosody/syntax mismatches (PP1-dominant). Given the assumption that for Reading 2 a participant should already know the structure of the sentence and its interpretation, it is not entirely clear why some Reading 2 pronunciations still do not represent to the expected pattern. 

It could be that for some participants and some sentences the intended interpretation of the sentence is never grasped because the level of complexity was simply too high, or for some other reason. It might also be that while the participant arrived at the intended syntax, it was too difficult to devise or produce the prosodic phrasing that corresponds with that syntax. Finally, it is possible that the prosodic coding was imperfect, given the difficulty of evaluating prosodic phenomena, even for ToBI experts and other highly trained linguists. Most likely, all three of these possibilities are at play, and some portion of the cases of prosody/syntax mismatch can be attributed to each.

Returning to the predictions made in the first chapter, and indeed the intuition that motivated this line of research, it should be noted that *Hypothesis 3* (repeated here for convenience) as originally formalized is not supported by the facts.

  (@hypd) *Hypothesis 3*
  The inter-reading time (IRT) will be longer for Arg sentences that are declarative than for:
    a. Arg sentences that are interrogative
    b. Mod sentence that are interrogative or declarative

The prediction made by this hypothesis is essentially that IRT would show an effect of the interaction between PP2 Status and Speech Act, but no such significance was detected. It is possible that this is because IRT is not the proper measure for processing difficulty, but to make that claim one would have to explain why it nonetheless shows a robust effect of Arg PP2 Status (i.e., garden-path sentences exhibit longer IRTs than non-garden-path sentences). It seems more likely that IRT is a useful measure of processing difficulty, but that for some other reason, the intuition that the declarative instances of the Arg sentences are harder to process than the interrogative versions is not reflected in IRT.

### Ancillary analysis: The processing cost of interrogativity {#qslow}

```{r}
library(scales)
library(readr)

irt_fillers <- subset(read_csv("export/all_data.csv"), isFiller)
mean.irtByQ <- aggregate(
  irt_fillers$irt, 
  by=list("Q"=irt_fillers$Condition_Q),
  FUN=mean
)
mean.irtByPP <- aggregate(
  irt_fillers$irt, 
  by=list("+PP"=irt_fillers$Condition_GP),
  FUN=mean
)
mean.irtByQnPP <- aggregate(
  irt_fillers$irt, 
  by=list("Q"=irt_fillers$Condition_Q,"+PP"=irt_fillers$Condition_GP),
  FUN=mean
)
```

It is worth noting that the mean IRT for interrogative versions (6.66s) of the experimental sentences in the reported study was numerically greater than for the declaratives (6.40s). While this finding was not statistically significant, @qp2 similarly found that whole-sentence silent reading times for interrogatives were longer than for declaratives; and @mehler1963some provided a very early report of the processing cost of interrogativity: a so-called kernel sentence, i.e., a simple declarative, was easier to recall verbatim than were a number of sentences that he considered to be syntactic transformations of that kernel sentence (K): negative (N), polar question (Q), passive (P), and combinations thereof: NQ, NP, QP and NPQ. Mehler found that accurate recall was more frequent for K sentences (`r percent(300/460)`) than for the other sentences types, with interrogatives (`r percent(210/460)`) being recalled accurately at a lower rate than the two other individual transformations (`r percent(234/460)` for N; `r percent(243/460)` for P).

To further investigate the effect that interrogativity appears to have on processing, the filler sentences in this study were designed in two versions, interrogative (Q) and declarative (D), so as to provide a diagnostic of the interrogative effect on IRT outside the construction of interest and absent any syntactic ambiguity. A linear mixed effects regression model predicting IRT for filler items by Speech Act with random intercepts for participant and item found that IRT is increased by 0.37s for interrogatives (std. error = 0.2; p < .05); declaratives had a mean IRT of 6.18s, while interrogatives had a mean IRT of 6.55s. Half of the fillers had a sequence of two PPs at the end of the sentence to mirror the experimental items: a model predicting IRT by the presence of those PPs found a minimal effect on IRT ($\beta$=0.01, std. error = 0.22, p > .50). This indicates that the presence of a string of PPs does not independently affect the processing cost engendered by interrogativity.

Interrogative status itself appears to increase the time needed for participants to feel they have satisfactorily studied a sentence in order to read it aloud correctly. This is consistent with the @mehler1963some and @qp2 findings that interrogatives are in some way more complicated or difficult than declaratives, and is an interesting finding, although not clearly related to the primary research question behind this study.

## Exploratory analyses    

This section presents two exploratory analyses that were conducted in pursuit of additional evidence bearing on certain aspects of the unexpected findings just reported. One looks at protocol adherence, to investigate whether the lack of effect of Reading on prosodic boundary occurrence can be attributed to look-ahead. The results of that analysis suggest that look-ahead was most likely not at play. 

The second exploration examines the possible effects of the content PP2-heads and finds that *of*-headed PP2s (categorized as Mod under the PP2 Status factor of the material) afforded shorter IRTs than did *from*-headed PP2s, despite *from*-headed PP2s also being categorized as Mod.

### On Reading 1 delay {#r1del}

Recall that Reading 1 (R1) delay is the time elapsing between the initial display of a sentence and the start of phonation, and that participants' median R1 delay ranged from 0.6s to 1.6s with a standard deviation of 0.25s. The distribution of R1 delay was notably different than that of R2 delay, as shown in Figure \@ref(fig:delayComparison), which of itself indicates that participants were adhering to the protocol at least most of the time.

```{r delayComparison,fig.pos="!h",fig.cap="Distributions of R1 delay and R2 delay"}
library(readr)
library(tidyr)
library(dplyr)
library(scales)

hidel <- 1051
lodel <- 901

adata <- read_csv("../drafts/export/prosody_with_r1delcat.csv")
adata$SentenceType <- paste(ifelse(adata$Condition_Q,"Q","D"),ifelse(adata$Condition_GP,"Arg","Mod"))
adata$reading <- paste("Reading", adata$Reading)
pdata <- subset(adata, r1DelCat != "NORMAL")

adata$both <- adata$simple2lvl=="BOTH"

catdes <- sprintf("FAST median R1 delay < %0.2fs. SLOW median R1 delay > %0.2fs",lodel/1000,hidel/1000)
raw_rs_file <- read_csv("../csvs/merged.csv")
raw_rs <- subset(raw_rs_file,!isFiller & Leading < 15000)
raw_rs$reading <- raw_rs$Reading
raw_rs$Reading <- ifelse(raw_rs$reading == 1, "R1", "R2")
ggplot(raw_rs, aes(Leading/1000, fill = Reading)) +
  geom_histogram(binwidth = 0.5,position="dodge",color="black") +
  scale_fill_manual(values=c("black","white")) +
  ggtitle("Bin size = 0.5s") +
  xlab("R1/R2 Delay in seconds") +
  ylab("Frequency") 

ggsave("deldis.pdf",device=cairo_pdf())
# dev.off()
```

As a way of analyzing the protocol, and the extent to which participants performed as instructed, participants were categorized based on their median R1 delay. In what follows, a *fast* median R1 delay was less than or equal to 0.9s, and a *slow* one was longer than 1.05s, resulting in 12 participants per category. Ten participants had R1 delays falling between those values, and were set aside in this exploration of protocol adherence. The calculations for categorizing participants were done over Reading 1 of experimental items (n = 489). Note that while sub-group categorization (i.e., *fast* vs. *slow*) is strictly a property of R1 delay, data bearing on prosodic phrasing patterns for both readings is nonetheless examine in what follows.

```{r rsplit}

r1data<-subset(adata,Reading==1)

fast <- subset(r1data,r1DelCat == "FAST")
slow <- subset(r1data,r1DelCat == "SLOW")

fastTabN <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)  
slowTabN <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTab <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)
slowTab <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTabN<-fastTabN[c(1,sto(names(fastTabN)[2:5])+1)]
slowTabN<-slowTabN[c(1,sto(names(slowTabN)[2:5])+1)]
fastTab<-fastTab[c(1,sto(names(fastTab)[2:5])+1)]
slowTab<-slowTab[c(1,sto(names(slowTab)[2:5])+1)]
  
tab<-cbind(fastTab,slowTab[2:5])
ntab<-cbind(fastTabN,slowTabN[2:5])
tab[2:9]  <- mapply(percent,tab[,2:9])
ntab[2:9] <- format(ntab[2:9],digits=0)

tab <- rbind(tab,ntab)

colnames(tab)[1] <- "Pattern"
colnames(tab)[6:9] <- paste0(colnames(tab)[6:9],".slow")
tab <- arrange(tab,Pattern)
tab <- tab %>% select(Pattern,everything())

r1tab<-tab
r2data<-subset(adata,Reading==2)

fast <- subset(r2data,r1DelCat == "FAST")
slow <- subset(r2data,r1DelCat == "SLOW")

fastTabN <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)  
slowTabN <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTabN<-fastTabN[c(1,sto(names(fastTabN)[2:5])+1)]
slowTabN<-slowTabN[c(1,sto(names(slowTabN)[2:5])+1)]

fastTab <- xtabs(~simple2lvl + SentenceType, data=fast) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)
slowTab <- xtabs(~simple2lvl + SentenceType, data=slow) %>% 
  prop.table(margin=2) %>% 
  as.data.frame() %>% 
  spread(SentenceType, Freq)

fastTab<-fastTab[c(1,sto(names(fastTab)[2:5])+1)]
slowTab<-slowTab[c(1,sto(names(slowTab)[2:5])+1)]
  
tab<-cbind(fastTab,slowTab[2:5])
ntab<-cbind(fastTabN,slowTabN[2:5])
tab[2:9]  <- mapply(percent,tab[,2:9])
ntab[2:9] <- format(ntab[2:9],digits=0)

tab <- rbind(tab,ntab)

colnames(tab)[1] <- "Pattern"
colnames(tab)[6:9] <- paste0(colnames(tab)[6:9],".slow")
tab <- arrange(tab,Pattern)
tab <- tab %>% select(Pattern,everything())


tab <- rbind(r1tab,tab)
```

```{r r1catTesting}
xdata <- subset(adata,r1DelCat != "NORMAL")
r1df <- ftable(xtabs(~Participant + simple2lvl + r1DelCat, subset(xdata,Reading==1))) %>% as.data.frame
r2df <- ftable(xtabs(~Participant + simple2lvl + r1DelCat, subset(xdata,Reading==2))) %>% as.data.frame
dfs <- subset(r1df %>% spread(simple2lvl,Freq), BOTH + OBJ + PP1 > 0)
dfs <- cbind("r1"=dfs,"r2"=subset(r2df %>% spread(simple2lvl,Freq), BOTH + OBJ + PP1 > 0))
dfs$deltaBOTH <- dfs$r1.BOTH - dfs$r2.BOTH
dfs$deltaBothCat <- ifelse(dfs$deltaBOTH > 0, "Decrease",ifelse(dfs$deltaBOTH < 0,"Increase","No change"))
```

As is evident in illustrative figure \@ref(fig:facet) below (showing break occurrence data), readers were more likely to produce only one break (PP1 or OBJ) when median R1 delay was short, vs. when media R1 delay was slower. 

```{r facet, fig.cap="Plot of pattern proportions as a function of sentence type.",fig.height=3.5,fig.pos="H"}
library(ggplot2)
library(ggthemes)

ggplot(pdata,    
    aes(
      x=reorder(SentenceType,sto(SentenceType)),
      y=..count..,
      fill=reorder(
        simple2lvl,
        ifelse(simple2lvl=="PP1", 1, ifelse(simple2lvl=="OBJ",3,2))
      )
    )) +
  geom_bar(position="fill",color="black",width=0.5) +
  labs(fill="Pattern",x=" ",y=" ",caption="FAST n=12, SLOW n = 12") +
  facet_grid(rows = vars(reading),cols=vars(r1DelCat)) +
  scale_y_continuous(labels=scales::percent) + 
  scale_fill_brewer(breaks=c("PP1", "BOTH", "OBJ"), palette="Greys")

ggsave("facet.pdf",device=cairo_pdf())
# dev.off()
```

A possible explanation is that for recordings in the *slow* category, readers were more prone to hesitation in general, or perhaps, contrary to instructions, were using both the delay time as well as extra time created via hesitation to look ahead. The significant effect of PP2 Status for the *slow* category in Reading 2 could be due to the reader not fully understanding the Arg sentences prior to Reading 2, thus increasing their likelihood of hesitation; i.e., where the fast category represents confident readers, the *slow* category represents less confident readers. As a result of the reader's increased difficulty in Reading 1 for the recordings in the *slow* category, the effect of processing difficulty is not limited to Reading 1 but in fact spills over into Reading 2. The *slow* category represents readers who are never able to fully comprehend the sentence, and thus hesitate more frequently.

Mixed effects logistic regression models found no significant effect of R1 Delay Category for PP1 break dominance or OBJ break dominance. So, ultimately, variation in R1 delay does not seem to be of concern.

### On PP2 heads {#pp2h}

As mentioned in the items description (Section \@ref(exps)), half of the items used *of* for the head of PP2 in the Mod cases, while half used *from.* In the Arg condition, half used *into* and half used *onto* to head PP2. An analysis that attempts to predict IRT by the PP2 head (e.g., *into* vs. *from*) found that while *into* and *onto* do not behave differently from each other, *of* and *from* do. Starting from a maximally complex model that included the lexical identity of the matrix verb, the construction verb, the head of PP1, and the head of PP2, as well as Speech Act and PP2 Status, the model that best predicted IRT was one with Speech Act and PP2 head as predictors, with *into* and *onto* collapsed into one level of the PP2 head factor and used as the reference level (intercept). 

```{r pp2hd}
csv <- '
PP2 head model,Estimate,Std. Error,p
D \\em{into/onto} (Intercept),6.76,0.58,< .001
Q,0.32,0.21,.13
\\em{from},-0.08,0.27,.77
\\em{of},-0.84,0.27,< .01
'

tab <- read_csv(csv)

tab %>% kable(
  align="rccc",  
  escape=F,
  caption=
    "Linear mixed effects regression model predicting IRT by Speech Act and PP2 head."
) %>% 
  kable_styling(latex_options = "hold_position")
```

Sentences where PP2 was headed by *of* typically had IRTs that were 0.84s faster than sentences where PP2 was headed by *into*/*onto*; when the PP2 head was *from*, IRT was only 0.08s faster than for *into*/*onto*, a difference that is not statistically significant (p > .50).

It is clear from the above that the sentences containing argument PP2s tested here (ones headed by *into*/*onto*) result in longer IRT measures than those containing Mod PP2s ($\beta$ = 0.46, p < .05), but Speech Act and the interaction between the two factors are not a significant predictors of IRT (see Section \@ref(irt)). Based on these findings, it does not appear that IRT represents a behavioral reflex of the intuition that the processing difficulty of PP2-attachment ambiguities is lessened in interrogative contexts vs. declarative contexts.

What might explain the difference in PP2 heads' effect on IRT? It seems immediately relevant that the *of*-headed PPs in the experimental materials were all marked by *of* as the possessors of the noun in the immediately preceding PP1; e.g., *on the roof rack of the minivan* could be re-cast as *on the minivan’s roof rack*. In contrast, the *from*-headed PP2s are all locatives, and in that sense are similar to cases with *into/onto*.  It is just that the *from* phrases are not the kind of locatives that can function as the goal arguments demanded by the construction verbs used in the study.  That is, even if these were to be used in some circumstances as directional rather than purely locative prepositions, they would function as sources rather than goals, e.g., *he moved the cows from the pasture to the barn*.

There might also be a simpler (and less interesting) explanation: because *of* is only two characters long, whereas *from* and *into*/*onto* are all four characters, participants may have recognized a pattern. For instance, they may have deduced that a two-character PP2 head meant the sentence did not have the difficult properties of some of the sentences with four-character PP2 heads (most notably the *into*/*onto* cases), which would have eliminated some of the needed study time.

## Summary

PP2 Status is a significant predictor of prosodic boundary occurrence, prosodic boundary dominance, and inter-reading time (IRT). This is reassuring with regard to the design of the study, and supportive of the well established finding that syntax can influence prosody. It is also supportive of the claim made here, that IRT is a useful diagnostic of sentence complexity and processing difficulty.

Speech Act was not a significant predictor of prosodic phrasing, with the exception of PP1-dominance. That it was not a significant predictor of boundary occurrence or of OBJ-dominance is not supportive of the hypotheses about its apparent ameliorating effect on difficult to process sentences made prior to this study. That it was a significant predictor in one analysis leaves open the possibility that those hypotheses are not entirely false.

Speech Act was not a significant predictor of IRT for the experimental items, but did appear to be a reliable predictor of longer IRTs in the filler items (which lacked the garden-path properties of the Arg experimental items), relative to otherwise identical declarative sentences.

Ultimately, the apparent ameliorating effect of interrogativity on otherwise difficult to process declarative sentences with PP-attachment ambiguities does not appear to have a behavioral correlate that has been detected in this study. No significant interaction between Speech Act and PP2 Status was found for any of the analyses conducted.

\clearpage